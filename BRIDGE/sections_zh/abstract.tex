\documentclass[../main_zh.tex]{subfiles}
\begin{document}

\begin{abstract}
  神经架构搜索（NAS）已成为一种自动化设计深度学习模型的关键方法。
  尽管前景广阔，NAS 通常会产生巨大的计算和硬件成本。
  为了缓解这些挑战，可迁移神经架构搜索（TNAS）应运而生，它利用先前的 NAS 结果来增强在新任务上的性能。
  然而，现有方法主要集中在相同神经搜索空间内的知识迁移，忽略了跨域可迁移性的潜力。
  受此启发，我们通过学习通用的神经表示来探索跨异构搜索空间的演化 TNAS。
  具体而言，我们引入了一种新方法，该方法使用一个简单的分词器将神经架构的操作和拓扑信息编码成一个统一的序列。
  然后，该序列由一个变分自编码器处理，其中基于 Transformer 的编码器用于捕获丰富的神经表示，解码器则重建原始序列。
  通过利用这些潜在表示，我们进一步建立了一个作为桥梁的域间映射，从而在不同搜索空间之间实现有效的显式解迁移，以增强演化 NAS 过程。
  为了利用这种能力，我们开发了一种演化顺序迁移优化方法，在种群初始化期间迁移知识，从而提供了灵活性和适应性。
  据我们所知，这项工作是文献中首次探索跨不同空间的演化 TNAS 的尝试。
  此外，我们通过使用不同的架构空间（包括 NAS-Bench-101、NAS-Bench-201 和 DARTS 搜索空间）进行的综合实证研究，证明了我们方法的实用性。
  我们的结果表明，所提出的方法显著增强了 NAS 在不同领域中的适应性和性能。
\end{abstract}

\begin{IEEEkeywords}
  神经架构搜索，迁移学习，表示学习
\end{IEEEkeywords}

\end{document}