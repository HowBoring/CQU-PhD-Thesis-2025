\documentclass[../main_zh.tex]{subfiles}

\graphicspath{{\subfix{../figures}}}

\begin{document}

\section{引言}

\IEEEPARstart{深}{度}学习在各个领域都取得了成功~\cite{DBLP:conf/cvpr/HeZRS16,DBLP:conf/cvpr/RedmonDGF16,DBLP:conf/acl/FosterVUMKFJSWB18}。
然而，开发有效的深度神经网络（DNN）仍然是一个具有挑战性且耗时的过程，需要大量的领域专业知识和实验验证。
这一挑战推动了神经架构搜索（NAS）的发展，这是一种自动化发现针对特定任务和数据集的优化 DNN 架构的技术。

传统上，DNN 的设计严重依赖于手动调优和专家直觉，这两种方法都资源密集，并且可能偏向于设计者的经验。
NAS 引入了一种系统的、数据驱动的方法，显著减少了架构设计过程中的人力劳动和主观性~\cite{DBLP:conf/iclr/ZophL17,DBLP:conf/cvpr/ZophVSL18}。
然而，这种自动化也带来了其自身的挑战，主要是高计算成本~\cite{pham_efficient_2018}。评估众多架构以确定特定任务最有效的架构需要大量的计算资源，这可能成为研究和实际应用中的一个限制因素。
因此，大量工作~\cite{pham_efficient_2018,DBLP:conf/iclr/LiuSY19,DBLP:journals/tnn/DongHFTTO23,DBLP:conf/iclr/BakerGRN18,DBLP:conf/iconip/HouDFQ21} 致力于寻找在特定问题场景中快速搜索高性能神经架构的有效方法。

% 可迁移 NAS (TNAS) 是减轻 NAS 通常相关的巨大资源需求的一种有前途的方法。
% 通过利用在一个领域中识别出的架构模式并将其应用于新的但相关的任务，TNAS 简化了神经架构的优化~\cite{DBLP:journals/pami/LuSGBDB21}。
% 效率的提升来自于在搜索过程中引入偏差，该偏差利用了从先前解决的 NAS 任务中获得的见解和知识。
% 演化 TNAS 通过利用种群演化进一步增强了这一过程，提供了更大的灵活性并有效地减轻了负迁移~\cite{DBLP:conf/cvpr/LiaoJD23,DBLP:journals/tec/ZhouWFLWT24}，取得了显著的进展。

可迁移 NAS (TNAS) 已成为一种有前途的策略，可减轻 NAS 的巨大资源需求。
通过利用在一个领域中发现的架构模式并将其应用于相关任务，TNAS 可以简化神经架构的优化过程~\cite{DBLP:journals/pami/LuSGBDB21}。
这种效率是通过在搜索过程中引入偏差来实现的，该偏差由从先前解决的 NAS 任务中获得的见解提供信息。
演化 TNAS 通过采用演化算法来引入灵活性和减轻负迁移，从而进一步增强了这一概念，取得了显著的进步~\cite{DBLP:conf/cvpr/LiaoJD23,DBLP:journals/tec/ZhouWFLWT24}。
然而，演化 TNAS 的有效性受到搜索空间一致性的限制。
这种限制降低了演化 TNAS 的灵活性，因为搜索空间通常需要根据神经架构设计的特定要求进行定制。
将神经架构知识跨领域迁移给现有方法带来了巨大挑战，因为搜索空间在大小、复杂性和异构性方面可能有很大差异。
例如，一些搜索空间可能只允许在层的数量和类型上有所变化~\cite{DBLP:journals/tnn/SunXZY20}，而另一些搜索空间可能允许在每层的连接性和操作上进行更细粒度的修改~\cite{DBLP:conf/cvpr/ZophVSL18,DBLP:conf/iclr/LiuSY19,DBLP:journals/tnn/DongHFTTO23}。
因此，在一个空间中搜索获得的知识可能不容易迁移到另一个空间，特别是当空间是异构的，即具有不同的结构和操作特性时。
这种局限性阻碍了演化 TNAS 的效率和有效性，因为每个新的搜索空间都需要重新启动搜索过程，从而使以前的 NAS 知识和见解过时。

解决 NAS 中的跨域可迁移性问题对于克服这些障碍至关重要，而跨不同和异构搜索空间迁移 NAS 解决方案的潜力在很大程度上仍未被探索和利用。
一个值得注意的努力是\textit{跨域预测器} (CDP)~\cite{DBLP:conf/nips/0002TLW022}，它利用领域自适应方法来构建跨不同搜索空间的神经性能预测器。然而，CDP 在搜索过程中无法实现直接的知识迁移，并且受到搜索空间之间需要相似性的限制。


% 在本文中，我们介绍了 \OUR{} (\textbf{B}uilding \textbf{R}epresentation for \textbf{I}nter-\textbf{D}omain Hetero\textbf{g}eneous Transferable NAS)，它利用神经表示学习来支持 NAS 中的跨域知识迁移。
% 神经表示学习旨在构建一个潜在空间，该空间编码了全面的神经架构信息。
% 这种表示通过特定于架构的性能指标得到进一步丰富，从而有助于为解决方案迁移建立有效的跨域映射。
% 利用这些跨域迁移能力，我们在 \OUR{} 中设计了一个演化迁移优化 (ETO) 框架，该框架支持可迁移的神经架构搜索过程。
% 因此，\OUR{} 不仅拓宽了 NAS 的适用性，而且还利用了从多个搜索空间中获得的累积知识，从而简化了架构搜索过程并减少了计算开销。
% 为了学习稳健且信息丰富的表示~\cite{DBLP:conf/ijcnn/LukasikFZHK21}，我们设计了一个结构为变分自编码器 (VAE) 的表示学习器。
% 该学习器利用基于 Transformer 的编码器将架构序列映射到潜在空间。
% 然后，一个互补的解码器从潜在向量中重建这些序列，这些潜在向量也用于预测架构性能，从而提供增强学习过程的额外监督信号。
% 通过利用这些潜在表示，\OUR{} 学习了一个域间映射，有效地弥合了异构搜索空间之间的差距，并促进了显式解决方案迁移。
% 这种方法可以有效地利用从探索不同环境中收集到的见解，从而提高新 NAS 工作的效率和效果。
% 在多个规模的搜索空间上进行的实证评估表明，我们的方法明显优于不包含解决方案迁移的基线方法，证实了 ETO 框架在增强 NAS 结果方面的有效性。
在本文中，我们介绍了 \OUR{} (\UB{B}uilding \UB{R}epresentation for \UB{I}nter-\UB{D}omain Hetero\UB{g}eneous \UB{E}volutionary TNAS)，这是一种利用神经表示学习促进演化 NAS 中跨域知识迁移的新颖方法。 \OUR{} 的核心要素包含一个神经表示学习框架，旨在构建一个潜在空间，该空间编码了全面的神经架构信息。这种表示通过特定于架构的性能指标得到进一步丰富，从而能够为解决方案迁移建立有效的跨域映射。
% 为了实现这一点，我们设计了一个结构为变分自编码器 (VAE) 的表示学习器，其中包含一个基于 Transformer 的编码器，用于将架构序列映射到潜在空间。一个互补的解码器从潜在向量中重建这些序列，这些潜在向量同时用于预测架构性能，从而提供增强学习过程的额外监督信号。
为了实现这一点，我们设计了一个基于变分自编码器 (VAE) 的表示学习器，其中包含一个基于 Transformer 的编码器，用于将架构序列编码到潜在表示空间中。
通过利用这些潜在表示，\OUR{} 学习了一个域间映射，有效地弥合了异构搜索空间之间的差距，并促进了显式解决方案迁移。这种方法能够有效地利用从不同神经架构空间中收集到的见解，从而提高新 NAS 工作的效率和效果。
所提出的 \OUR{} 中的另一个关键组件是演化顺序迁移优化 (ESTO)~\cite{DBLP:journals/cim/TanFJ21,DBLP:journals/tec/XueYHZCST22,10342789} 搜索算法，该算法使用学习到的域间映射，支持跨空间的可迁移神经架构搜索过程。这种方法不仅扩展了 NAS 的适用性，而且还利用了来自多个搜索空间的累积知识，简化了架构搜索过程并减少了计算开销。
% 在多个规模的搜索空间上进行的实证评估表明，\OUR{} 明显优于不包含解决方案迁移的基线方法。
在不同规模和复杂性的搜索空间上进行的实证评估表明，\OUR{} 明显优于缺少解决方案迁移机制的基线方法。
这些结果证实了所提出的 \OUR{} 在增强 NAS 结果方面的有效性，强调了我们的方法在推动 NAS 领域发展方面的潜力。
本文的主要贡献可以概括为：
\begin{itemize}
  \item 据我们所知，这项工作提出了第一个用于跨不同异构域的演化 TNAS 的显式解决方案迁移方法，解决了该领域的一个关键空白。

  \item 我们提出了一种用于跨域 TNAS 的神经表示学习方法，该方法有效地在域之间建立了有效的映射。
        我们的方法利用了一种基于 Transformer 和 VAE 架构集成的新颖表示学习模型。
        该模型有效地提取了神经架构的显著特征，并构建了平滑的潜在空间表示。
        此外，我们结合了架构性能指标来丰富这些表示，从而优化潜在空间分布。
        这种优化支持具有性能驱动搜索目标的 TNAS。
        % 全面的消融研究表明，我们提出的方法在多个表示学习指标上取得了具有竞争力的性能，强调了我们方法的有效性。

  \item 我们提出了一种用于跨域演化 TNAS 的 ESTO 方法。
        该方法包含一个统一的跨域编码策略和相应的演化算子。
        结合通过我们的表示学习方法实现的跨域迁移能力，该方法有效地利用了来自不同搜索空间的现有 NAS 解决方案。
        实证评估表明，我们提出的跨域演化 TNAS 方法在多个搜索空间尺度上取得了显著的性能改进。
\end{itemize}

% 本文的其余部分组织如下：
% \S\ref{sec:preliminary} 提供了必要的背景信息和相关工作。
% \S\ref{sec:method} 详细介绍了 \OUR{} 的理论和技术方面。
% 实验设置、结果和相应的分析在 \S\ref{sec:experiment} 中呈现。
% 最后，\S\ref{sec:conclusion} 总结了整个工作。

本文的其余部分结构如下：
第~\ref{sec:preliminary}节提供了必要的背景信息并回顾了相关工作。
第~\ref{sec:method}节介绍了我们提出的方法，即 \OUR{} 的理论基础和技术细节。
第~\ref{sec:experiment}节详细介绍了实验设置，并对我们的发现进行了全面分析。
最后，第~\ref{sec:conclusion}节总结了这项工作的贡献并讨论了潜在的未来方向。

\section{初步}\label{sec:preliminary}

本节介绍了 NAS 的必要背景，并回顾了与我们的工作至关重要的两个相关领域的最新进展：NAS 中的知识迁移和神经架构的表示学习。这些主题为我们提出的方法提供了基础背景，并将我们的贡献置于当前 NAS 研究的格局中。

\subsection{神经架构搜索}

神经架构搜索 (NAS) 已发展成为一种自动化设计深度学习模型的关键技术。
它系统地识别为特定任务量身定制的优化架构，从而减轻了对专家直觉和手动调整的依赖。
形式上，NAS 可以表述为一个双层优化问题。
让 \( \mathcal{A} \) 表示神经架构的搜索空间，\( \mathcal{W} \) 表示与特定架构相关的权重空间。
NAS 的目标是找到一个最优架构 \( a^* \in \mathcal{A} \)，该架构最小化验证损失 \( \mathcal{L}_\mathrm{val} \)，该损失取决于架构 \( a \) 及其对应的最优权重 \( w^*(a) \)：
\begin{equation}
  \begin{aligned}
    a^*    & = \arg\min_{\mathclap{a \in \mathcal{A}}} \: \mathcal{L}_\mathrm{val}(w^*(a), a) \\
    w^*(a) & = \arg\min_{\mathclap{w \in \mathcal{W}}} \: \mathcal{L}_\mathrm{train}(w, a) \\
  \end{aligned}
\end{equation}
其中，给定架构 \( a \) 的最优权重 \( w^*(a) \) 通常通过最小化训练损失 \( \mathcal{L}_\mathrm{train} \) 来获得。

NAS 框架评估无数的架构配置以确定最有效的设计，这传统上需要高昂的计算成本和大量的资源分配。
早期的方法，例如强化学习 (RL)~\cite{DBLP:conf/iclr/ZophL17,DBLP:conf/cvpr/ZophVSL18,Gao2019GraphNASGN,Lyu2021MultiobjectiveRL,Hsu2018MONASMN} 和演化算法 (EA)~\cite{Liu2020ASO,DBLP:conf/aaai/RealAHL19,DBLP:journals/tec/ZhouQGT21,DBLP:conf/iconip/HouDFQ21,DBLP:journals/tnn/DongHFTTO23}，证明了自动化架构搜索的可行性，但通常因其资源密集型而受到批评。
随后，出现了参数共享、一次性模型和权重继承等技术，通过在不同架构评估中重用模型权重来减少计算开销。
最近的进展，例如 DARTS~\cite{DBLP:conf/iclr/LiuSY19} 和 NAO~\cite{DBLP:conf/nips/LuoTQCL18}，利用了基于梯度的优化和潜在空间表示来进一步减少计算开销，使 NAS 在大规模应用中更加实用。

尽管取得了这些进展，NAS 的计算成本仍然是一个重大障碍。研究人员已经探索了许多提高效率的技术，包括使用代理模型来预测候选架构的性能，从而最大限度地减少了详尽评估的需要。

% 最近的进展引入了更复杂的策略，包括结合基于梯度的优化方法。
% 例如，DARTS~\cite{DBLP:conf/iclr/LiuSY19} 利用架构搜索空间的连续松弛来实现梯度下降以进行有效搜索。
% NAO~\cite{DBLP:conf/nips/LuoTQCL18} 为神经架构构建了一个连续的潜在空间，其中应用梯度下降来优化潜在表示，然后将其重建为神经架构。
% 这种方法显著加快了搜索过程，使其对于大规模应用更加可行。
% 尽管取得了这些进展，与 NAS 相关的高计算成本仍然是一个障碍。
% 为了解决这个问题，研究人员探索了各种提高效率的技术，例如使用代理模型来预测候选架构的性能，从而减少了详尽评估的需要。

\subsection{NAS 中的知识迁移}

% 为了提高 NAS 的效率，研究人员越来越多地探索知识迁移作为加速搜索过程的机制。
为了解决 NAS 中固有的计算挑战，研究人员越来越多地研究知识迁移机制，以此作为在保持解决方案质量的同时加快架构搜索过程的一种手段。
最初的努力集中在使用超网等技术进行任务内知识迁移，超网将所有候选架构封装为子图并在它们之间共享参数~\cite{DBLP:conf/cvpr/ZophVSL18,pham_efficient_2018,Huang2023SplitLevelEN}。
通过利用参数共享，NAS 可以用最少的训练来评估架构，从而加快搜索过程。

% 随后的研究将知识迁移扩展到不同的任务，同时保留相同的搜索空间~\cite{DBLP:conf/cvpr/ZophVSL18}。
% 然而，直接在任务之间迁移架构通常需要任务之间高度的相似性，从而限制了广泛的适用性~~
% 一些方法通过量化任务相似性来指导迁移学习来解决这一挑战。
% 例如，Arch-Graph~\cite{DBLP:conf/cvpr/HuangHLCXLL22} 根据任务嵌入预测特定于任务的架构，而 EMT-NAS~\cite{DBLP:conf/cvpr/LiaoJD23} 利用演化多任务框架通过种群演化促进隐式知识迁移。
随着研究的进展，重点扩展到更复杂的迁移目标，例如在不同任务和搜索空间之间进行迁移~\cite{DBLP:conf/cvpr/ZophVSL18}。
研究的一个方向是在使用相同搜索空间的同时在不同任务之间迁移知识。
一种直接的方法涉及将从源任务上的 NAS 获得的解决方案迁移到更复杂的目标任务~\cite{DBLP:conf/cvpr/ZophVSL18,DBLP:conf/iclr/LiuSY19}。
然而，这种直接的架构迁移方法要求源任务和目标任务之间高度相似，限制了其广泛的适用性。
对于具有显著领域偏移的任务，直接重用架构可能会导致次优性能~\cite{DBLP:conf/iclr/LiuSY19}。
为了解决这个问题，一些工作集中在测量和利用任务之间的相关性来指导迁移学习。
Arch-Graph~\cite{DBLP:conf/cvpr/HuangHLCXLL22} 提出了一种方法，该方法根据给定的任务嵌入来预测特定于任务的最优架构。
EMT-NAS~\cite{DBLP:conf/cvpr/LiaoJD23} 通过使用演化个体繁殖来促进跨任务种群的隐式知识迁移，而 MTNAS~\cite{DBLP:journals/tec/ZhouWFLWT24} 通过为 NAS 开发演化多任务框架来扩展这一概念。

虽然以前的研究在 NAS 的知识迁移方面取得了重大进展，但对固定搜索空间的依赖限制了其在不同场景中的适用性。
现实世界任务和计算环境的异构性需要不同的搜索空间，每个搜索空间对应于不同的资源约束和优化目标。
因此，迫切需要为 NAS 开发跨域迁移能力，从而能够利用跨不同架构搜索空间和应用环境的知识。
跨域可迁移性不仅可以增强 NAS 方法的灵活性和通用性，而且还可以通过利用从不同领域获得的知识来潜在地提高搜索效率。
它需要将从一个搜索空间获得的知识映射到另一个搜索空间，可能具有不同的操作或网络拓扑。
为了应对这一挑战，一些研究已经开发出通过对齐搜索空间来迁移知识的方法。
例如，CDP~\cite{DBLP:conf/nips/0002TLW022} 通过采用领域自适应间接促进了不同搜索空间之间的知识迁移，这使得不同搜索空间之间的分布在表示中更接近，从而训练了一个跨域神经架构性能预测器。
\textit{一种用于 NAS 的通用可迁移预测器}~\cite{DBLP:conf/sdm/HanMCRSZ0JN23} 也试图通过用由原始算子组成的计算图来表示任何给定的候选卷积神经网络来迁移预测器。

然而，据我们所知，目前缺乏针对 NAS 中跨领域解决方案显式迁移的全面研究，这将有助于探索不同的搜索空间，从而能够根据各种计算资源约束和部署场景进行高效的神经架构设计。 \DiffAdd{我们的工作通过学习神经架构的表示并集成基于种群的搜索策略，为跨领域的显式解决方案迁移搭建桥梁，从而超越了传统的 TNAS 方法，实现了更灵活、可扩展和有效的迁移。}

\subsection{神经架构的表示学习}

% 为了实现下游任务，例如准确性预测和延迟预测，有效的神经架构表示学习至关重要。
% 为了提高 NAS 的效率和有效性，许多研究都集中在利用表示学习技术来建模神经架构，从而构建这些架构的潜在特征空间——通常称为表示空间。
为了提高 NAS 的效率，许多研究都集中在表示学习上，以对神经架构进行建模，从而构建一个捕获其基本特征的潜在表示空间。
已经开发出性能预测器以在该潜在空间内运行，从而可以有效地估计神经架构的质量。 
% 例如，有几种方法已经开发出直接在该表示空间内运行的性能预测器，以估计神经架构的质量。
早期的预测器利用 LSTM~\cite{DBLP:journals/corr/abs-1712-03351,DBLP:conf/eccv/LiuZNSHLFYHM18,DBLP:conf/nips/LuoTQCL18}、MLP~\cite{DBLP:conf/cvpr/Xu00TJX021,DBLP:conf/aaai/WhiteNS21} 和随机森林~\cite{DBLP:journals/tec/SunWXJYZ20} 等模型来加速架构的评估。
% 然而，由于对架构拓扑的编码不足，这些方法通常表现出有限的预测准确性。
% 例如，NAO~\cite{DBLP:conf/nips/LuoTQCL18} 直接将整个架构编码为扁平向量，这限制了其捕获复杂结构关系的能力。
最近，人们开始关注利用 GCN~\cite{DBLP:conf/cvpr/ChenGCLZWT21,DBLP:conf/aaai/LiGZ20,DBLP:conf/nips/ShiPXLKZ20,DBLP:conf/eccv/WenLCLBK20,DBLP:conf/ijcnn/LukasikFZHK21} 来建模架构的图格式数据（例如邻接矩阵和节点特征）。
神经预测器~\cite{DBLP:conf/eccv/WenLCLBK20} 通过计算两个修改后的 GCN 层的均值来编码架构中的每个节点，这两个 GCN 层分别使用邻接矩阵和邻接矩阵的转置来传播信息。
Transformer~\cite{DBLP:conf/nips/VaswaniSPUJGKP17} 也用作神经架构表示学习器，并在许多近期工作中得到了广泛应用~\cite{DBLP:conf/mm/Guo0HLXY022,DBLP:conf/cvpr/YiZH0023}。
% 自注意力机制允许 Transformer 相互权衡输入序列不同部分的重要性。
Transformer 中的自注意力机制在建模不同架构组件（例如拓扑和算子）之间错综复杂的依赖关系和相互作用方面尤其有利。
% 在神经架构表示学习的背景下，这种机制在捕获各种组件（例如拓扑和算子）之间错综复杂的依赖关系和相互作用方面被证明是有利的。
% 因此，Transformer 在建模图结构数据中固有的细微关系方面表现出色。
NAR-Former~\cite{DBLP:conf/cvpr/YiZH0023} 将神经网络的操作和拓扑信息都编码到一个序列中，利用 Transformer 生成一个简洁的向量表示，从而有助于预测关键指标，例如单个单元架构和完整神经网络的延迟和准确性。
这些研究的发现进一步证明，可以根据其性能在表示空间内有效地对神经架构进行聚类。
% 大多数神经架构表示的构建主要是为了预测给定架构的性能指标~\cite{DBLP:conf/cvpr/YiZH0023,DBLP:conf/cvpr/Xu00TJX021,DBLP:conf/aaai/LiGZ20}。

此外，最近的研究工作，例如 NAO 等~\cite{DBLP:conf/nips/LuoTQCL18,DBLP:conf/nips/YanZAZ020,DBLP:conf/ijcnn/LukasikFZHK21}，已经证明了在表示空间内进行搜索的可行性。
它们通过构建一个连续的表示空间，利用贝叶斯优化 (BO) 和梯度下降 (GD) 等连续优化方法快速进行搜索。
% 从性能的角度来看，先前工作中构建的连续表示空间表现出显着的分布特性，这已被证明有助于更有效的搜索过程。
具体来说，这些连续表示允许更平滑地导航架构空间~\cite{DBLP:conf/ijcnn/LukasikFZHK21}，从而能够更有效地探索和优化神经网络设计。

基于性能预测器的方法和连续搜索方法的成功都凸显了利用表示学习在不同领域之间建立迁移映射的潜力。通过利用这些连续空间的分布特征，我们假设即使在它们的原始公式差异很大的情况下，也可以在不同的架构搜索空间之间形成有意义的联系。
% 上述努力的成功表明，通过表示学习在不同领域之间建立迁移映射的潜力。
% 通过利用这些连续空间的分布特征，我们假设即使在它们的原始公式差异很大的情况下，也可以在不同的架构搜索空间之间建立有意义的联系。
在本研究中，我们引入了一种新颖的方法，即神经架构表示作为跨域 TNAS 的管道，从而能够利用从不同搜索空间中获得的知识。


\ifSubfilesClassLoaded{
  \bibliographystyle{IEEEtran}
  \bibliography{IEEEabrv,\subfix{../references.bib}}}

\end{document}
