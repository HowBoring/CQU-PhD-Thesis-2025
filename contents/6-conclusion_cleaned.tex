\documentclass[../main.tex]{subfiles}

\begin{document}

\mychapter{总结与展望}\label{sec:ch6-summary-and-outlook}

\mysection{全文总结}
\label{sec:ch6-1-overall-summary}

本文聚焦于深度学习领域愈发突出的“规模–效率矛盾”，即模型性能持续提升与构建成本（数据、算力与人力投入）快速攀升之间的张力。为在统一语境下讨论效率问题，本文以“深度模型构建”为参照，将传统以参数优化为中心的视角扩展为覆盖数据与知识准备、模型设计与实现、模型参数实现三个环节的完整构建流程。围绕这一流程，本文采用知识迁移作为研究视角与路径：通过识别、表征与复用已训练模型在构建过程中形成的任务知识、结构表示与模型参数等可迁移信息，力求在既定约束下减少新增数据与计算投入，提升构建效率。
基于对构建过程的拆解，本文分别在上述三个环节独立开展研究，形成以下三项支撑性工作。

在数据与知识准备环节，面向低数据资源（少样本）场景下对标注数据的高度依赖与迁移不稳问题，第3章提出 Prompt-Distiller：融合双教师知识与对比学习的少样本蒸馏方案。方法以两类互补教师提供多视角后验知识，结合响应蒸馏、中间层表征对齐与对比学习目标，并引入不确定性感知的实例级加权，在少样本设定下提升可迁移信息密度与迁移稳定性，从而提高构建效率。

在模型设计与实现环节，针对神经架构搜索（NAS）计算开销高与架构/评估经验难以跨异构搜索空间复用的瓶颈，第4章提出 \textsc{Bridge}：跨空间可迁移的架构表示与搜索加速框架。方法通过学习统一的结构表示并构建空间间“桥接”映射与迁移式性能预测，实现跨空间的快速筛选与早停，显著降低冷启动搜索与评估成本，同时保持所得架构的任务适配性。

在模型参数实现环节，聚焦免/少训练条件下多模型融合的稳定性与性能保持难题，第5章提出 KG-MFTO：知识图谱引导的参数融合方案。方法在模型—任务层面构建显式关系，进行关系引导的选择性继承与冲突修正，在零或极少量训练约束下实现多个预训练模型能力的高效、稳定整合，兼顾平均与最差任务性能，并控制额外开销。

综上，本文在深度模型构建的三个关键环节分别提出以知识迁移为核心的高效构建方法，并通过实验验证其有效性。三项工作彼此独立、针对性明确，共同为“基于知识迁移的深度模型高效构建”提供了可复用的证据与实现路径。

\mysection{局限性分析}
\label{sec:ch6-2-limitations-analysis}

尽管在三个环节分别取得了进展，本文工作仍存在若干局限，有待后续研究进一步完善。

首先，关于第3章的 Prompt-Distiller。其有效性在一定程度上依赖高质量教师的可用性：方法采用双教师（提示微调教师 $\Theta_T$ 与原始预训练教师 $\Theta_{T'}$），前者的获取成本与后者的域适配性都会影响蒸馏效果。作为基于提示学习的方案，性能可能受提示模板设计影响（本文使用公开模板，但最优提示的自动化获取仍具挑战）。同时，探针驱动的对比学习收益与探针训练质量、负样本采样策略有关，需仔细调优。当前验证主要聚焦文本分类场景，其对更复杂的自然语言生成、序列标注或跨模态任务的适用性尚需进一步评估。

其次，关于第4章的 \textsc{Bridge} 框架。学习兼具表达力与可迁移性的通用架构表示本身具有挑战性，当源域与目标域范式差异较大时，表示质量与后续映射有效性可能下降。跨域映射函数 $\mathcal{M}$ 的学习通常需要少量目标域评估样本作为锚点，因此并非完全零成本迁移，其效率提升程度受目标域冷启动样本规模影响。尽管引入演化式顺序迁移以降低负迁移风险，但在源/目标域高度不匹配时，迁移初始种群仍可能偏离最优区域。最后，架构表示学习模块（例如基于 Transformer 的变分编码器）需要一定预训练开销。

再次，关于第5章的 KG-MFTO。在待融合模型数量 $n$ 显著增大时，知识图谱规模与基于图的推断复杂度会同步上升，动态图谱的维护与推断成为潜在瓶颈。方法对用于评估候选融合质量的函数 $F$ 的稳健性较为敏感：若评估存在噪声或偏差，将影响图谱记录与 GAT 预测，进而影响热启动与求解效果。尽管引入了知识引导的演化求解器（热启动 CMA\mbox{-}ES）以提升效率，其效果仍受初始解质量与关系建模准确度制约。当前范式多假设待融合模型处于可对齐的参数空间（如同一基础架构族），对于跨架构融合尚难直接适用。

除方法层面的局限外，本文还存在共性不足：三项工作分别面向数据与知识准备、模型设计与实现、模型参数实现三个环节独立展开，未研究不同环节技术之间可能的相互影响（例如，\textsc{Bridge} 搜索得到的紧凑架构是否更利于后续蒸馏或融合，抑或融合得到的模型是否可作为更强教师）。另一方面，本文主要聚焦构建效率（数据与计算成本），对知识迁移可能带来的公平性、鲁棒性与可解释性变化未作系统评估。迁移亦可能继承源域偏差，如何在效率提升的同时确保可靠性与安全性，值得持续关注。


\mysection{未来展望}
\label{sec:ch6-3-future-work}

本文围绕基于知识迁移的深度模型高效构建，在知识、结构与参数三个层面分别进行了探索并提出相应方法。尽管取得了一定的进展，但正如 6.3 节所分析，当前研究仍存在诸多局限性，同时也揭示了广阔的未来研究空间。立足于本文的工作基础与当前领域的挑战，未来值得进一步探索的方向主要包括以下几个方面：

首先，在知识层迁移方面，可以进一步深化少样本知识蒸馏的机制与应用场景。未来的工作可探索将 Prompt-Distiller 思想扩展至更复杂的任务，如少样本条件下的自然语言生成、序列标注或跨模态学习任务，研究如何在这些场景下有效传递教师模型的结构化知识或生成能力。此外，可以研究更自适应的知识迁移策略，例如自动化地学习最优的提示模板用于蒸馏，或者动态地调整不同知识源（如不同教师、不同层级特征）在蒸馏过程中的权重。探索更鲁棒的对比学习机制以应对教师模型可能存在的噪声或偏差，以及研究如何更深度地融合无监督或自监督信号来进一步减少对标注数据的依赖，也是有价值的方向。

其次，在结构层迁移方面，可以致力于发展更通用、更高效的跨异构架构知识迁移方法。针对 \textsc{Bridge} 框架的局限性，未来研究可探索更强大的神经网络架构表示学习模型，例如能够捕捉更细粒度结构信息或动态计算图特性的表示方法，以构建更具表达力的统一潜在空间。同时，研究更先进的跨域映射技术，如非线性映射、基于最优传输理论的分布对齐方法，或能够量化迁移不确定性的概率映射模型，有望提升异构迁移的精度与鲁棒性。将跨异构迁移思想扩展到多目标 NAS 场景（同时优化精度、延迟、能耗等），以及研究如何在几乎无需目标域标注样本的情况下实现零样本架构迁移，也是极具挑战性但意义重大的方向。此外，将 \textsc{Bridge} 框架应用于更广泛的架构类型（如图神经网络、循环神经网络）的迁移也是值得探索的扩展。

再者，在参数层迁移方面，可以持续优化免训练模型融合的可扩展性、鲁棒性与应用范围。随着预训练模型数量的持续增长，如何将 KG-MFTO 框架扩展以高效融合数十乃至上百个模型是一个重要的工程与算法挑战，可能需要更高效的知识图谱构建与推理机制，以及更可扩展的优化求解器。提升融合过程对性能评估噪声的鲁棒性，研究更精确的模型间协同/冲突关系的量化方法，以及探索融合结果的可解释性，对于增强该范式的可靠性至关重要。此外，突破当前参数融合大多局限于同构架构的限制，研究如何在存在架构差异的模型之间进行有效的参数（或功能）层面的融合，将极大地拓展该技术的应用边界。将 KG-MFTO 与 PEFT 技术（如 LoRA 模块的融合）相结合，探索参数高效调整与免训练融合的协同，也是一个有趣的方向。

最后，超越单个维度的独立研究，探索知识、结构、参数三个层面高效构建技术之间的协同与整合，是未来一个更宏大但也更复杂的方向。尽管本文强调了三项工作的独立性，但未来研究可以探讨它们之间潜在的相互促进作用。例如，由 \textsc{Bridge} 高效搜索出的紧凑架构，是否天然更适合通过 Prompt-Distiller 进行知识蒸馏？通过 KG-MFTO 融合得到的强力模型，是否能作为更好的教师模型或架构搜索的性能评估器？反之，知识蒸馏或参数融合技术能否用于压缩或优化 \textsc{Bridge} 搜索得到的架构？理解和利用这些跨维度交互，可能催生出更全局、更系统的高效模型构建新范式。此外，将模型构建的效率考量从单纯的数据、计算成本，扩展到更全面的可持续性指标（如碳排放、生命周期成本），并将公平性、鲁棒性、隐私保护等社会伦理因素纳入高效构建的优化目标中，也将是未来研究不可或缺的重要组成部分，推动“绿色 AI”与“可信 AI”的发展。

综上所述，深度模型的高效构建是一个充满挑战但也机遇无限的研究领域。本文的工作仅是其中的初步探索，期望能为未来的研究提供一些有益的启示，共同推动人工智能技术向着更高效、更普惠、更负责任的方向前进。

\end{document}