\documentclass[../main.tex]{subfiles}

\begin{document}

\mychapter{总结与展望}\label{sec:ch6-summary-and-outlook}

\mysection{全文总结}
\label{sec:ch6-1-overall-summary}

本文聚焦于深度学习领域愈发突出的“规模–效率矛盾”，即模型性能持续提升与构建成本（数据、算力与人力投入）快速攀升之间的张力。为在统一语境下讨论效率问题，本文以“深度模型构建”为参照，将传统以参数优化为中心的视角扩展为覆盖模型设计与实现、数据与知识准备、模型参数实现三个核心环节的完整构建流程。

基于这一分析框架，本文的核心论点是：“高效构建”所面临的前沿瓶颈，是在这三个环节中动态演进的。因此，本文采用“知识迁移”作为贯穿始终的研究视角与方法论，沿着深度学习发展的历史脉络，对这一“演进的瓶颈”展开了递进式的研究，并形成了以下三项支撑性工作：

首先，在深度学习发展早期，性能突破的关键在于模型架构的设计与实现。针对此阶段NAS计算开销高、架构经验难以跨异构搜索空间复用的核心瓶颈，第 \ref{sec:ch4-evolutionary-transfer-nas-heterogeneous-spaces} 章提出了 \textsc{Bridge} 框架。该方法通过学习统一的架构表示并构建空间“桥接”映射与迁移式性能预测，实现了跨空间的快速筛选与早停，显著降低了架构设计的冷启动搜索与评估成本。

其次，随着Transformer等统一的架构范式成为主流，研究瓶颈从“架构发现”转移至数据与知识的准备环节。针对此阶段在少样本场景下，知识蒸馏存在的不稳定与过拟合问题，第 \ref{sec:ch3-dual-contrastive-distillation-for-few-shot-prompts} 章提出了 Prompt-Distiller 方案。该方法通过融合双教师知识与对比学习，在少样本设定下提升了可迁移信息密度与迁移稳定性，从而提高了数据与知识的利用效率。

再次，LLM 阶段，研究瓶颈进一步转移至模型参数的实现环节。针对免重复训练条件下多模型融合的参数冲突与性能退化难题，第 \ref{sec:ch5-knowledge-guided-multi-form-optimization-for-llm-merging} 章提出了 KG-MFTO 框架。该方法通过构建动态知识图谱，将复杂的融合问题分解为多形式优化任务，并利用知识引导的演化求解器，在零或极少量训练约束下实现了多个预训练模型能力的高效、稳定整合。

综上所述，本文在“深度模型构建”的理论框架下，沿着“架构 $\rightarrow$ 数据 $\rightarrow$ 参数”这一瓶颈演进的脉络，逐一探索和应对了知识迁移在三个核心环节中的前沿挑战。三项共同为“基于知识迁移的深度模型高效构建”这一核心命题提供了系统性的解决方案与实现路径。

\mysection{局限性分析}
\label{sec:ch6-2-limitations-analysis}

尽管在三个环节分别取得了进展，本文工作仍存在若干局限，有待后续研究进一步完善。

首先，关于第 \ref{sec:ch4-evolutionary-transfer-nas-heterogeneous-spaces} 章的 \textsc{Bridge} 框架。学习一种同时具备表达力与可迁移性的通用架构表示具有挑战性。当源域与目标域范式差异较大时，表示质量与后续映射有效性可能下降。跨域映射函数 $\mathcal{M}$ 的学习通常需要少量目标域评估样本作为锚点，因此并非完全零成本迁移，其效率提升程度受目标域冷启动样本规模影响。尽管引入 ESTO 以降低负迁移风险，但在源/目标域高度不匹配时，迁移初始种群仍可能偏离最优区域。最后，架构表示学习模块（例如基于 Transformer 的变分编码器）需要一定预训练开销。

其次，关于第 \ref{sec:ch3-dual-contrastive-distillation-for-few-shot-prompts} 章的 Prompt-Distiller。其有效性在一定程度上依赖高质量教师的可用性：方法采用双教师（提示微调教师 $\Theta_T$ 与原始预训练教师 $\Theta_{T'}$），前者的获取成本与后者的域适配性都会影响蒸馏效果。作为基于提示学习的方案，性能可能受提示模板设计影响（本文使用公开模板，但最优提示的自动化获取仍具挑战）。同时，探针驱动的对比学习收益与探针训练质量、负样本采样策略有关，需仔细调优。当前验证主要聚焦文本分类场景，其对更复杂的自然语言生成、序列标注或跨模态任务的适用性尚需进一步评估。

再次，关于第 \ref{sec:ch5-knowledge-guided-multi-form-optimization-for-llm-merging} 章的 KG-MFTO。在待融合模型数量 $n$ 显著增大时，知识图谱规模与基于图的推断复杂度会同步上升，动态图谱的维护与推断成为潜在瓶颈。方法对用于评估候选融合质量的函数 $F$ 的稳健性较为敏感：若评估存在噪声或偏差，将影响图谱记录与 GAT 预测，进而影响热启动与求解效果。尽管引入了知识引导的演化求解器以提升效率，其效果仍受初始解质量与关系建模准确度制约。当前范式多假设待融合模型处于可对齐的参数空间（如同一基础架构族），对于跨架构的融合尚难直接适用。

\mysection{未来展望}
\label{sec:ch6-3-future-work}

本文围绕基于知识迁移的深度模型高效构建，在架构、数据与知识、参数三个层面分别进行了探索并提出相应方法。尽管取得了一定的进展，但正如 \ref{sec:ch6-2-limitations-analysis} 节所分析，当前研究仍存在诸多局限性，同时也揭示了广阔的未来研究空间。立足于本文的工作基础与当前领域的挑战，未来值得进一步探索的方向主要包括以下几个方面：

在架构迁移方面，可以致力于发展更通用、更高效的跨异构架构知识迁移方法。针对 \textsc{Bridge} 框架的局限性，未来研究可探索更强大的神经网络架构表示学习模型，例如能够捕捉更细粒度架构信息或动态计算图特性的表示方法，以构建更具表达力的统一潜在空间。同时，研究更先进的跨域映射技术，如非线性映射、基于最优传输理论的分布对齐方法，或能够量化迁移不确定性的概率映射模型，有望提升异构迁移的精度与鲁棒性。将跨异构迁移思想扩展到多目标 NAS 场景（同时优化精度、延迟、能耗等），以及研究如何在几乎无需目标域标注样本的情况下实现零样本架构迁移，也是极具挑战性但意义重大的方向。此外，将 \textsc{Bridge} 框架应用于更广泛的架构类型（如图神经网络、循环神经网络）的迁移也是值得探索的扩展。

在任务知识迁移方面，可以进一步深化少样本知识蒸馏的机制与应用场景。未来的工作可探索将 Prompt-Distiller 思想扩展至更复杂的任务，如少样本条件下的自然语言生成、序列标注或跨模态学习任务，研究如何在这些场景下有效传递教师模型的结构化知识或生成能力。此外，可以研究更自适应的知识迁移策略，例如自动化地学习最优的提示模板用于蒸馏，或者动态地调整不同知识源（如不同教师、不同层级特征）在蒸馏过程中的权重。探索更鲁棒的对比学习机制以应对教师模型可能存在的噪声或偏差，以及研究如何更深度地融合无监督或自监督信号来进一步减少对标注数据的依赖，也是有价值的方向。

在参数迁移方面，可以持续优化免重复训练模型融合的可扩展性、鲁棒性与应用范围。随着预训练模型数量的持续增长，如何将 KG-MFTO 框架扩展以高效融合数十乃至上百个模型是一个重要的工程与算法挑战，可能需要更高效的知识图谱构建与推理机制，以及更可扩展的优化求解器。提升融合过程对性能评估噪声的鲁棒性，研究更精确的模型间协同/冲突关系的量化方法，以及探索融合结果的可解释性，对于增强该范式的可靠性至关重要。此外，突破当前参数融合大多局限于同构架构的限制，研究如何在存在架构差异的模型之间进行有效的参数（或功能）层面的融合，有望拓展该技术的应用边界。将 KG-MFTO 与 PEFT 技术（如 LoRA 模块的融合）相结合，探索参数高效调整与免重复训练融合的协同，也是一个有意义的方向。

最后，超越单个维度的独立研究，探索架构、数据与知识、参数三个层面高效构建技术之间的协同与整合，是未来一个更宏大但也更复杂的方向。本文的三项工作是作为针对不同演进阶段核心瓶颈的独立解决方案而提出的，未来研究可以探讨它们之间潜在的相互促进作用。例如，由 \textsc{Bridge} 高效搜索出的紧凑架构，是否天然更适合通过 Prompt-Distiller 进行知识蒸馏？通过 KG-MFTO 融合得到的强力模型，是否能作为更好的教师模型或架构搜索的性能评估器？反之，知识蒸馏或参数融合技术能否用于压缩或优化 \textsc{Bridge} 搜索得到的架构？理解和利用这些跨维度交互，可能催生出更全局、更系统的高效模型构建新范式。此外，将模型构建的效率考量从单纯的数据、计算成本，扩展到更全面的可持续性指标，并将公平性、鲁棒性、隐私保护等社会伦理因素纳入高效构建的优化目标中，也将是未来研究不可或缺的重要组成部分，推动“绿色 AI”与“可信 AI”的发展。

% 综上所述，深度模型的高效构建是一个充满挑战但也机遇无限的研究领域。本文的工作仅是其中的初步探索，期望能为未来的研究提供一些有益的启示，共同推动人工智能技术向着更高效、更普惠、更负责任的方向前进。

\end{document}
