\documentclass[../main.tex]{subfiles}
\graphicspath{{../figures/}}

\begin{document}

\chapter[\hspace{0pt}结构层迁移：跨异构搜索空间的进化迁移 NAS]{{\heiti\zihao{3}\hspace{0pt}结构层迁移：跨异构搜索空间的进化迁移 NAS}}
\label{ch:chapter4}

\section[\hspace{-2pt}引言]{\heiti\zihao{-3}\hspace{-8pt}引言}
\label{sec:chapter4_introduction}

深度神经网络模型的构建过程可以抽象为"三阶段模型构建机理"，即从知识获取到结构承载再到参数实现的递进过程。相应地，第一章提出了"多层次知识迁移框架"，分别在知识层、结构层和参数层实现模型构建知识的迁移。这一框架旨在通过各层次知识的复用，提高深度模型构建的效率。在知识层，借助预训练模型进行知识蒸馏可以减少对大规模标注数据的依赖；在结构层，通过不同任务间共享和迁移架构设计经验可加速神经架构搜索；在参数层，将已训练模型参数融合再利用能够实现模型的快速组装。上述三层迁移相互衔接，构成了一条高效的模型构建途径。本章研究紧扣这一总体思路，聚焦于结构层的架构迁移机制。作为对第一章所述多层次迁移框架的自然延续，本章将在结构层面探讨如何跨任务迁移神经网络架构，以支撑后续新模型的构建与优化。

具体来说，本章提出了一种基于表征学习的跨空间进化式可迁移神经架构搜索方法，命名为BRIDGE框架。该方法旨在解决大规模神经架构搜索在算力受限场景下的高开销问题，以及不同任务之间架构设计经验难以复用的瓶颈。BRIDGE框架的核心思想是构建统一的神经架构表示空间，将不同搜索空间中的网络结构映射到共享的潜在表示，以"桥接"源域与目标域之间的差异。为此，我们采用表征学习技术，将神经网络的拓扑结构和算子信息编码为向量化的表示。在该表示中，不同操作集合和拓扑形式的架构都能投射到统一的坐标系下进行度量和比较。这一统一表征机制使异构搜索空间之间的知识迁移成为可能，例如将卷积神经网络的架构设计经验迁移用于Transformer架构的搜索，或将小规模数据集上获得的优良架构迁移到大规模数据场景中继续优化。基于此共享表示空间，本章进一步设计了进化式的序贯迁移优化策略，即进化序贯迁移优化（Evolutionary Sequential Transfer Optimization, ESTO）。在该策略中，我们以源任务中表现优异的架构为初始种群，在目标任务的搜索空间中应用遗传算法进行迭代进化。进化过程中融合了源任务架构的"DNA"（例如有效的模块配置和连接模式），从而大幅减少了在目标空间中盲目探索的范围。通过引入源域知识的良好初始个体，ESTO策略显著提升了NAS在目标任务上的搜索效率。

上述跨空间架构迁移与进化优化的方法有效缓解了传统NAS在异构场景下的低效问题。在不牺牲模型性能的前提下，我们的迁移策略将搜索成本降低到了可接受范围。已有研究表明，迁移NAS方法可将搜索开销从数百GPU天减少到个位数GPU天。在本章的实验中，我们的方法在多个目标任务上的搜索成本平均降低约50\%，而得到的模型精度可媲美甚至超过从零开始独立搜索的结果。这表明，通过结构层的知识迁移，能够以远低于传统方法的算力代价找到高性能的神经网络架构，实现效率与效果的双赢。

当然，实现跨空间架构迁移也伴随诸多挑战。例如，如何在不同搜索空间之间建立有效的架构对应关系，如何衡量并迁移架构各组成部分对性能的贡献，以及如何避免不恰当迁移带来的负面影响，都是需要深入研究的问题。本章将针对这些关键挑战展开讨论，并提出相应的解决方案。在理论上，结构层的知识迁移为异构任务下的自动模型设计提供了一条可行途径；在实践上，它弥补了传统NAS在跨域应用时的局限性。综上，本章的研究在整个论文主线上具有承上启下的支撑作用：既承接了第一章多层次迁移框架中结构层迁移的理念，又为后续章节的方法构建和实验验证奠定了理论基础。

\section[\hspace{-2pt}问题背景与研究动机]{\heiti\zihao{-3}\hspace{-8pt}问题背景与研究动机}
\label{sec:chapter4_background}

\subsection{神经架构搜索的高开销现状}

随着深度学习模型规模和复杂度的不断提高，设计高性能神经网络架构变得愈发困难且耗时。传统上，模型架构主要依赖人工经验进行设计和调整，这种手工尝试方式不仅费时费力，还容易受到设计者主观偏好的影响。为降低人工参与并探索人类直觉之外的架构方案，神经架构搜索（Neural Architecture Search, NAS）应运而生，提供了一种自动化发现最优网络结构的范式。NAS通过在给定的搜索空间内枚举和评估大量候选架构，旨在找到适合特定任务和数据集的最优模型。形式上，NAS可以被定义为一个双层优化问题：外层优化选择架构，内层优化为该架构训练最优的权重参数。通常表示为：

\begin{equation}
	a^* = \arg\min_{a \in \mathcal{A}} L_{\text{val}}\big(w^*(a), a\big), \quad \text{其中 } w^*(a) = \arg\min_{w \in \mathcal{W}} L_{\text{train}}(w, a),
\end{equation}

其中$\mathcal{A}$表示给定的架构搜索空间，$\mathcal{W}$是对应权重参数的空间，$L_{\text{train}}$和$L_{\text{val}}$分别为训练集和验证集上的损失函数。这个定义表明，NAS需要针对每一种候选架构$a$在内层完成一次模型训练（以获得$w^*(a)$），然后再根据验证性能评估架构优劣。这种双层优化的特点导致NAS过程计算代价极其高昂：需要训练评估数以百计乃至数万计的不同架构，耗费巨大的计算资源。早期的NAS方法（例如基于强化学习的方法或进化算法的方法）在一些任务上成功探索出了优于人工设计的架构，但往往伴随着数千GPU天的计算开销，引发了对其实际可行性的质疑。

为缓解NAS的资源消耗问题，大量研究致力于提高搜索效率和降低计算成本。一方面，权重共享的一次性NAS方法通过在一个超网络中训练共享权重，使不同候选架构无需从头训练即可直接评估，从而将搜索成本从数千GPU天降至几GPU天乃至更低。典型例子如Pham等人提出的ENAS方法，通过让所有子网络共享权重，大幅减少了独立训练每个架构的冗余开销。另一方面，可微分NAS通过将架构搜索过程松弛为连续优化问题，使得可以采用梯度下降直接优化架构参数，从而显著加快了搜索收敛。Liu等人提出的DARTS方法便是其中代表，它将离散的架构选择参数化为连续变量并进行迭代优化，在几轮迭代内即可得到高性能架构。此外，基于性能预测模型的方法通过训练一个代理模型来预估架构的性能，利用预测值指导搜索而减少真实评估次数，也取得了显著的加速效果。例如Baker等人的工作利用历史架构性能数据训练了一个性能预测网络，在搜索时用预测值快速淘汰劣质架构，仅对少量候选进行真实评估，从而降低了整体开销。再者，进化算法在NAS中的高效实现（如级联进化、分阶段进化等）也得到了深入研究。总的来看，通过上述诸多策略，NAS的搜索成本相较早期方案已有数量级的下降。然而，即便有了这些改进，对于每一个新任务或新数据集，仍往往需要重新进行一次完整的NAS过程。这意味着，当任务频繁变化时（例如在工业应用中需要针对不同场景不断定制模型），累积的计算代价依然十分可观。这一现状激发了研究者寻找可迁移的NAS方案，以复用先前搜索过程中获得的知识，进一步提升新任务的搜索效率。

\subsection{可迁移NAS的提出与局限}

可迁移NAS（Transferable NAS, TNAS）是为降低NAS重复开销而发展出的新策略。其基本思想是在新任务的架构搜索中借鉴先前已完成任务中学到的架构模式或搜索经验，从而引入一种有指导的偏置，加速优化过程。通过将一种任务中发现的优秀子结构、拓扑模式或其它架构知识迁移到相关的目标任务上，TNAS可以在搜索初期就获得较高的性能起点，减少无效探索。Lu等人在研究中率先提出了神经架构迁移（Neural Architecture Transfer）的方法，证明了利用源任务的最优架构作为起点来微调，可在目标任务上迅速达到优异性能。这一想法也在早期NAS实践中得到验证：例如Zoph等人于2018年通过在小数据集CIFAR-10上搜索出最优卷积单元结构，并将其迁移用于ImageNet大规模图像分类任务，取得了当时最先进的结果。这些工作表明，神经架构在一定程度上具有可迁移性，跨任务共享架构设计经验是切实可行的。

值得注意的是，简单的架构直接迁移有时会遭遇负迁移，即源任务中有效的架构在目标任务上反而表现不佳。这可能是由于任务间差异导致某些架构模式不再适用。为提高迁移NAS的稳健性，研究者引入了进化算法等更灵活的优化手段，形成了进化式可迁移NAS的方向。通过遗传算法等群体搜索方法，可以在迁移过程中对架构进行适应性的微调，逐步演化出适合目标任务的新架构，从而降低生硬迁移带来的性能下降。近期的两个代表性工作是EMT-NAS和MTNAS。Peng Liao等人提出的EMT-NAS使用多任务进化优化框架，在多个相关任务间同时进行NAS以共享架构知识。与传统多任务学习不同，EMT-NAS为每个任务维护独立的网络架构和权重，但通过在进化种群中交换优秀架构，实现了架构层面的知识共享。实验结果显示，EMT-NAS在同时针对CIFAR-10/100和MedMNIST系列数据集进行架构搜索时，相比逐任务独立搜索，总耗时在CIFAR任务上减少了约8\%，在医学影像任务上最多减少了40\%，而性能仍能达到单任务优化的竞争水平。这一成果证明，在相关任务之间迁移架构设计经验，确实能够有效节省总的搜索成本。

另一项工作是Zhou等人提出的MTNAS，这是一种面向多任务的卷积神经网络架构进化搜索框架。MTNAS通过在进化算法中引入多任务适应机制，使不同任务的架构搜索可以共享部分进化过程，从而进一步提高了跨任务的搜索效率。据报道，MTNAS在若干视觉任务上的实验取得了比单任务NAS更优的搜索效率和精度平衡，其在保持较高准确率的同时，将搜索时间进一步压缩，在某些情况下甚至优于EMT-NAS的性能。总的来看，EMT-NAS和MTNAS等方法表明，通过巧妙地将进化优化与知识迁移相结合，NAS可以在多任务情境下取得比传统单任务反复搜索更高的效率。进化式TNAS通过群体智能的方式缓解了架构直接迁移的局限，在一定程度上实现了跨任务的架构经验复用。

然而，需要强调的是，上述可迁移NAS方法大多假设源任务和目标任务共享相同或相似的架构搜索空间。换言之，它们通常预设所有任务的候选架构具有统一的表示形式和操作集合（例如都采用同构的卷积细胞结构），以便直接在不同任务间交换架构个体。事实上，无论是EMT-NAS还是MTNAS，其实验设置均局限在卷积神经网络这一单一架构范式下：任务之间虽有数据集的差异，但网络结构搜索的基本单元和规则是一致的。这种假设限制了可迁移NAS的适用范围。在许多实际场景中，不同任务可能需要截然不同类型的网络结构。例如，图像分类任务偏好卷积网络架构，而自然语言处理任务可能依赖于Transformer或循环神经网络架构；又或者，即使都是视觉任务，小型数据集适合浅层网络，大型数据集则需要更深更复杂的架构。这些情况下，源任务和目标任务的搜索空间在操作算子、层级拓扑等方面存在显著差异。传统的迁移NAS方法面对这种异构搜索空间时往往力不从心：由于架构表示不兼容，源任务中搜得的架构无法直接应用于目标任务，不同空间间的知识共享变得困难。为此，有必要发展新的方法，突破搜索空间不一致带来的迁移障碍，在更加广泛的跨域场景下实现NAS的知识复用。

\subsection{跨空间架构迁移的挑战与动机}

针对上述问题，"跨空间"的神经架构迁移与搜索（即跨异构搜索空间的NAS）成为一个重要但尚未充分研究的课题。跨空间TNAS希望在源和目标任务架构形式不同的情况下，实现知识的迁移与重用，从而进一步提高NAS的泛化能力和效率。这一新范式具有重大的现实意义：如果能够打破搜索空间的藩篱，将不同领域累积的架构设计经验融会贯通，那么NAS有望真正发展成为一种普适的自动化模型设计工具。然而，要达成这一目标，需要克服一系列独特的挑战：

\textbf{架构表示差异}：不同任务/搜索空间的神经网络架构在表示形式上可能截然不同。例如，一个搜索空间可能包含卷积层、池化层等算子，而另一个则由自注意力机制等Transformer算子构成。传统NAS通常针对单一空间定义架构编码方式，当空间改变时，原有的架构编码和操作符集合不再适用。因此，跨空间迁移首先需要解决如何设计统一的架构表示方法，能够对异构空间的架构进行统一编码与比较。这一挑战要求引入一种通用的表示学习方案，将不同空间的架构映射到共同的表征中，为知识迁移搭建桥梁。

\textbf{性能度量差异}：即使我们能够建立共同的架构表示，不同任务对架构优劣的评价标准也可能不同。简单而言，源任务中的高性能架构在目标任务上未必仍然高性能。跨空间迁移NAS需要解决性能指标跨域预测的问题，即如何根据源空间架构的性能推断其在目标空间的表现。已有工作尝试训练跨域性能预测模型，如Liu等人提出的跨域预测器，利用领域自适应技术训练了可跨空间泛化的神经网络性能预测模型。CDP证明了在缺乏直接可比的架构情况下，通过引入中间对齐技术也能部分预测不同空间架构的性能。然而，CDP仅能提供性能评估上的指导，无法实现架构的直接迁移，并且要求源域与目标域的搜索空间具备一定相似性。因此，仍需要更通用的方法来衡量架构在不同任务上的性能相关性，以便有效选择和转换候选架构。

\textbf{迁移策略与负迁移}：在异构空间之间迁移架构，还必须慎重设计迁移策略以避免负迁移。由于源域架构在目标域可能并不直接适用，若生硬地将其应用于目标域，可能导致性能欠佳。需要一种机制能够在迁移过程中对架构进行调整，使其更贴合目标需求。进化算法在这方面展现出独特优势：通过种群初始化和变异交叉操作，可以在保留源架构优秀基因的同时，引入多样性使架构逐步适应新空间。因此，我们有动机将进化式优化融入跨空间NAS，让算法自行"进化"出适应目标空间的架构。这实际上是将进化迁移优化（Evolutionary Transfer Optimization, ETO）的思想引入NAS领域：在进化计算中，ETO关注在一个序列相关的优化任务中复用先前任务的优良解来加速后续任务求解。Tan等人的研究将其称为进化计算的新前沿。近期Xue等人提出的序贯迁移优化方法更是提供了在目标函数不同的异质问题间进行解迁移的框架。这些工作的启发在于，通过顺序地将解从源任务传递到目标任务，并配合适当的调整策略，可以在保证适应度的前提下加速收敛。因此，在NAS的背景下，我们可以借鉴ESTO的思想：将源任务的优秀架构解作为种子，引入目标任务的进化搜索中，使其在适应过程中逐步优化。这一策略有望同时利用源域知识和进化算法的探索能力，避免盲目搜索，从而达到更高的效率。

综上所述，跨空间可迁移NAS面临的核心问题在于：如何建立统一的架构表征来消除搜索空间差异，以及如何设计迁移与搜索算法来充分利用源域知识而不引入负面影响。这些挑战构成了本章研究的动机。一方面，我们希望通过引入表征学习，得到一个跨域通用的架构潜在表示空间，使得不同空间的架构可以直接比较其表示的相似性乃至性能潜力。另一方面，我们希望结合进化搜索策略，在目标域中有效地利用源域的优秀架构作为初始解，通过进化操作快速接近最优解。最终的目标是，在保证目标任务模型性能的前提下，最大程度减少由于搜索空间异构带来的重复劳动，实现"一次搜索，多域受益"的效果。

\section[\hspace{-2pt}相关研究综述]{\heiti\zihao{-3}\hspace{-8pt}相关研究综述}
\label{sec:chapter4_related_work}

围绕上述背景与动机，本章的工作涉及多个研究领域的交叉融合，包括传统NAS方法、可迁移NAS方法，以及面向异构架构空间的跨域NAS探索。为突出本章方法的创新性，下面将对这些相关研究进行综述和评述。

\subsection{传统NAS方法}

传统NAS（单任务架构搜索）作为AutoML的重要分支，近年来经历了快速发展。根据搜索策略的不同，主要的方法类别包括基于强化学习的NAS、基于进化算法的NAS、基于梯度优化的NAS以及基于代理模型的NAS等。

\textbf{强化学习方法}：Zoph和Le在2017年的开创性工作中，将架构的生成过程视为一个序列决策任务，采用强化学习的策略网络来逐步构造神经网络架构。该方法利用RNN控制器输出架构描述的序列，每生成一个候选架构就训练并评估其性能，并将性能作为奖励信号反馈给控制器加以优化。经过大量采样迭代，控制器逐渐学会产生高性能的架构。Zoph等人的算法成功发现了优于人类设计的卷积架构（如NASNet），但其缺点是需要训练成千上万个模型来获得策略网络的有效更新，导致数千GPU天的巨大计算成本。

\textbf{进化算法方法}：Real等人在2018年将遗传算法引入NAS，通过模拟自然进化过程来搜索架构。具体而言，该方法首先随机生成一个初始架构种群，评估每个架构在验证集上的适应度（例如准确率）；然后基于适应度选择部分架构作为"父代"，通过交叉和变异产生"子代"架构，再对新架构进行评估并加入种群；如此迭代，在不断的选择和变异中逐步进化出性能更优的架构。Real等人提出的Regularized Evolution算法在CIFAR-10上找到了当时最优的卷积架构AmoebaNet，证明了进化方法的有效性。进化NAS的优点在于其固有的并行性和多样性维护机制，但早期同样存在评估次数庞大、收敛较慢的问题。

\textbf{可微分方法}：可微分架构搜索通过将离散的架构选择问题放入一个连续的优化框架中求解，以降低搜索难度。DARTS是该方向的代表算法。它将每个候选操作赋予一个连续的权重，将架构视为所有操作的加权和，从而使架构选择参数可以通过梯度下降来优化。经过在训练集和验证集上的交替优化，最终得到的操作权重分布指示了最优架构的组成。可微分NAS显著减少了候选架构的评估训练次数（只需训练一次超网络），使搜索时间从以往的数百GPU天降至几GPU天以内。然而DARTS也存在架构权重优化不稳定、容易陷入局部最优等问题，后续研究提出了诸如正则化、架构参数退火等改进来提升其鲁棒性。

\textbf{代理模型方法}：此类方法关注利用少量真实评估结果训练一个性能预测模型，以在庞大的搜索空间中快速筛选出潜在高性能的架构。典型工作如Baker等人的MetaQNN和后来Liu等人的BANANAS等。BANANAS使用贝叶斯优化结合图神经网络作为架构性能预测器，在每轮迭代中利用预测值选择下一批评估的架构，从而用尽可能少的样本完成搜索。代理模型方法的优势在于大幅降低了对昂贵训练评估的依赖，但预测模型的训练本身也需要积累一定量的数据，且预测的偏差可能影响搜索结果的可靠性。

除了上述几类主要策略外，还有其他值得一提的进展。例如，一些研究将多目标优化引入NAS，兼顾准确率、参数量、推理速度等指标；另一些工作探索强化学习和进化算法的结合，以兼具二者优点。此外，针对不同领域（如图像、文本、图结构数据）的NAS变种也层出不穷。综上，传统NAS领域已经相当活跃，发展出丰富的方法谱系。然而，这些方法几乎清一色地针对单任务场景，对于如何在新任务中重复利用已有的架构设计成果考虑较少。当需要针对多个任务反复进行NAS时，传统方法的累积成本和时间仍然巨大。这正是可迁移NAS出现的初衷：在传统NAS取得初步成功的基础上，更进一步，利用知识迁移来减少重复劳动、提升自动化建模的整体效率。

\subsection{可迁移NAS方法}

可迁移NAS致力于解决多任务情境下NAS的高成本瓶颈，通过在新任务的搜索中引入来自已解决任务的先验知识，实现"一次搜索，多次复用"。这一思想与迁移学习在模型训练中的理念类似：不同之处在于，TNAS迁移的是"架构知识"而非"模型权重"。正如前文所述，Zoph等人的工作揭示了架构可迁移性的可行性，即在小规模数据集上找到的优秀架构单元可以拓展应用到大规模任务中。此后，学者们开始系统地研究如何将NAS的成果从源任务迁移到目标任务，以减少每次搜索的重复开销。

Lu等人在2021年提出的"神经架构迁移"（NAT）方法是早期对TNAS概念的拓展。他们将源任务中性能突出的架构作为蓝图，通过少量的微调和结构调整，使其适应目标任务需求，从而显著减少了目标任务的搜索时间。这一工作为TNAS提供了初步范式：即利用源任务架构作为初始点，缩小目标任务的搜索范围。然而，固定使用单一源架构也存在风险，如遇到任务差异较大时可能出现负迁移。

为提高迁移的灵活性，近年来引入了进化计算的思想，形成了进化式可迁移NAS。正如本章研究所借鉴的，进化算法适合解决具有多个解候选和逐步改进特征的问题，非常契合迁移NAS中需要对源架构进行适应性调整的需求。Peng Liao等人在CVPR 2023上提出的EMT-NAS可视为进化式TNAS的代表。EMT-NAS通过让多个任务的NAS过程同时进行、彼此之间共享架构候选来实现迁移。具体而言，在进化优化中，EMT-NAS的种群个体由多个任务各自的架构组成，不同任务之间周期性地交换高适应度的架构，以共同促进收敛。同时，每个任务仍训练自己架构的专属权重，以避免参数共享带来的负迁移。这种设计确保了架构层面的知识可以相互借鉴，而参数层面互不干扰，从而兼顾了迁移效率和性能稳定性。EMT-NAS在跨数据集的分类任务上验证了其有效性，体现出相对于独立NAS的明显加速。类似地，MTNAS框架采用了多任务遗传算法，针对每个任务维护一个演化子种群，同时定义共享的变异交叉操作来在任务间传递架构信息。这一方法将多任务优化与架构搜索融合，达到同时优化多个任务且各任务互相促进的效果。需要指出的是，这类方法通常假定多个任务在相同类型的搜索空间内进行（例如都为卷积神经网络架构）。因此，其迁移主要体现在任务之间而非空间之间。换句话说，EMT-NAS和MTNAS更多地解决的是如何在多个数据集/任务间共享架构经验，而非如何在不同架构范式间迁移。尽管如此，它们的成功证明了架构知识跨任务复用的巨大潜力：当任务相关时，适当的迁移策略能够实实在在地减少总搜索开销，提高每个任务的性能。

除多任务并行的情形外，还有一些研究关注顺序迁移NAS，即在完成一个源任务NAS后，将所得经验用于后续目标任务。例如，Zela等人在一项研究中提出在源任务上训练一个架构性能预测模型，然后将其应用于目标任务，以辅助评估目标空间的架构。他们的方法利用图卷积神经网络提取架构特征，证明了即使任务不同，该预测模型在一定程度上仍具有判别架构优劣的能力，从而减少了目标任务需要评估的架构数量。类似地，部分研究训练元学习的架构评价函数，希望"一次学习，多任务适用"。然而，这些方法往往要求源任务和目标任务的架构搜索空间相同，或者任务间具有较高的相似度，否则预测模型难以泛化。

综上，可迁移NAS已经展示出在同构空间下的显著优势。当面对系列相关任务时，TNAS方法能够充分利用已有成果，加快新任务的模型设计。然而，当任务所需的模型类型发生改变时，现有方法的适用性就大打折扣。这正引出了下一节讨论的重点：跨空间的NAS方法，即如何打破架构空间不一致的限制，实现真正通用的架构迁移。

\subsection{跨空间NAS探索}

相较于已有的可迁移NAS聚焦于同构搜索空间，跨空间NAS所面对的是源任务和目标任务在架构类型上的根本差异。这一领域目前尚处于起步阶段，相关工作非常有限。Yu Liu等人在NeurIPS 2022提出的跨域预测器可以说是少数直接针对异构架构空间的尝试之一。CDP并未试图显式迁移架构本身，而是采用了一种迂回思路：训练一个在源、目标空间间通用的性能预测模型，以此桥接不同空间架构的性能评估。具体而言，CDP利用领域自适应技术，将源空间和目标空间的架构表示投射到一个共享的特征空间，并在该空间中训练一个性能预测器，使其能够对来自两种空间的架构进行评估。通过这种方式，CDP期望实现"跨空间的性能度量"，从而在目标空间NAS时，借助源空间的丰富性能数据来提高评估效率。然而，CDP的方法也有明显局限：首先，它仍然依赖于源、目标空间在表示上的相似性，只有当两空间的架构特征能够被映射到一个可比较的共同域时，预测器才有用武之地。如果空间差异过大（例如CNN与Transformer架构差异显著），这种映射将变得困难。其次，CDP无法直接产生迁移架构的方案——它只提供了一个性能估计工具，最终目标空间的架构仍需通过常规NAS或其他方法搜索得到，并没有真正减少搜索空间的规模或直接引入源空间的优秀架构。因此，CDP更多是作为NAS的辅助手段，而非独立的跨空间迁移解决方案。

除CDP之外，几乎没有文献直接探讨在异构架构之间如何迁移设计经验。这突出说明了跨空间NAS的研究空白。同时，这一空白也意味着巨大的机遇：如果能找到有效的方法将不同网络范式下的架构相关知识联系起来，无疑将大幅拓展NAS技术的适用边界。为此，本章工作将引入神经架构表征学习这一有前景的工具。最近的一些研究表明，通过学习一个适当的低维表示，可以在连续空间中嵌入和操作架构，从而提高搜索效率。例如，Lukasik等人提出了平滑变分图嵌入方法，将神经网络架构表示为图，并训练变分自编码器将其嵌入到向量空间，在此空间中可以更高效地进行贝叶斯优化或梯度搜索。又如，Yan等人在NIPS 2020研究了无监督的架构表示学习，证明了良好的表示有助于预测架构性能并加速NAS。这些成果启示我们：如果能够学习到一种统一的架构向量表示，不同结构类型的架构就可能在该表示空间中被直接比较和关联，从而为跨空间迁移创造条件。

综上，跨空间NAS仍是亟待开拓的领域。现有初步尝试（如CDP）虽然提供了一定思路，但远未解决实质问题。在这一背景下，我们提出的BRIDGE方法将率先探索异构域进化NAS的架构直接迁移。与以往工作不同，我们的方法通过构建共享的架构表示空间和明确的跨域映射，实现了架构解的显式迁移，开创性地解决了异构搜索空间下知识转移的难题。下一节将形式化地定义本章所讨论的问题，并给出我们方法涉及的关键概念和变量的严格定义。

\section{问题定义与建模边界}
\label{sec:chapter4_problem_formulation}

\subsection{神经架构及搜索空间的形式化定义}

\textbf{神经架构}：在NAS中，我们用$a$表示一个神经网络架构。常见地，$a$可被描述为一个有向无环图$a=(V, E)$，其中节点集合$V$表示网络中的各层（或算子操作），边集合$E$表示层与层之间的数据流连接。通常，图中包括若干输入节点和输出节点，用于表示网络的输入和输出接口；中间节点则各自绑定一个从预定义\textbf{操作集合}$\mathcal{O}$中选取的算子，例如卷积、池化、全连接、激活函数等。每条有向边$(u \rightarrow v) \in E$表示从节点$u$输出的张量馈送到节点$v$，从而确定了网络的拓扑结构和信息流向。在不同的NAS框架中，架构还可以有其他等价表示方式，例如操作序列、配置列表等，但以上DAG定义提供了一个通用的抽象。

\textbf{架构搜索空间}：记$\mathcal{A}$为所有允许架构的集合，即搜索空间。通常$\mathcal{A}$会通过若干先验约束来限定架构的形式，以确保搜索的可行性和有效性。例如，在细粒度的单元搜索中，可能固定每个细胞包含一定数量的节点且拓扑为DAG，或者在宏架构搜索中限定网络的最大层数、每层的类型候选等。搜索空间也内隐地决定了操作集合$\mathcal{O}$和拓扑规则。例如，经典的NAS-Bench-101搜索空间规定了一个含有7个节点（含两个输入节点和一个输出节点）的有向无环图，以及包含卷积3×3、卷积1×1、最大池化3×3三种中间操作的操作集合；而DARTS搜索空间采用的是细胞结构，每个细胞有4个中间节点，每个节点从前两节点接收输入，操作集合则包括卷积、池化和跳连等8种算子。可以认为，搜索空间$\mathcal{A}$定义了架构的"设计语法"，任何不符合该语法的架构都不在搜索范围内。

在\textbf{跨空间NAS}情景下，我们需要面对两个彼此不同的搜索空间。令$\mathcal{A}^{(S)}$表示\textbf{源域}的架构搜索空间，$\mathcal{A}^{(T)}$表示\textbf{目标域}的架构搜索空间（其中$S$和$T$分别代表源任务和目标任务）。两空间可能存在显著差异，例如$\mathcal{A}^{(S)}$基于卷积神经网络结构，而$\mathcal{A}^{(T)}$基于Transformer结构；或者两者的操作集合$\mathcal{O}^{(S)}$与$\mathcal{O}^{(T)}$完全不同，节点和连接方式的限制也不同。这种情况下，源空间中的架构$a^{(S)} \in \mathcal{A}^{(S)}$通常\textbf{无法直接映射}为目标空间中的某个架构$a^{(T)} \in \mathcal{A}^{(T)}$，因为二者的"架构语言"不同。这正是跨空间NAS相比传统TNAS所面临的主要障碍：缺乏一个直接将$a^{(S)}$转换为$a^{(T)}$的映射。

为解决上述障碍，我们引入\textbf{架构表示学习}的思路。我们期望存在一个函数$\Phi$，能够将任意架构$a$映射到一个\textbf{连续向量表示}（latent representation）$\mathbf{z} = \Phi(a)$。理想情况下，$\Phi$应该具备如下性质：(1) \textbf{信息完备性}：从$\mathbf{z}$能够重构回原始架构$a$（即$\Phi$是近乎无损的编码）；(2) \textbf{空间统一性}：源空间和目标空间的架构经过$\Phi$映射后落入\textbf{同一个向量空间}$\mathcal{Z}$，使得不同来源的架构可以直接进行比较和运算；(3) \textbf{性能相关性}：表示向量$\mathbf{z}$应当捕捉架构与性能之间的关联，即距离相近的表示对应的架构性能也应相近，便于在表示空间中进行优化。为了满足这些要求，我们可以采用\textbf{变分自编码器}等深度表征学习技术来学习$\Phi$。具体做法将在下一章详细介绍，这里提前给出抽象定义：假设我们训练得到一个编码器$E$和解码器$D$，其中$E: \mathcal{A} \to \mathcal{Z}$，$D: \mathcal{Z} \to \mathcal{A}$。对于任意架构$a$，都有$D(E(a)) \approx a$（解码器能够重构架构），且$E$在源域和目标域架构上是共同训练的或风格一致的，从而保证$E(a^{(S)})$和$E(a^{(T)})$位于同一表示空间$\mathcal{Z}$中。

\subsection{跨空间知识迁移的定义}

有了上述统一架构表示，我们进一步定义\textbf{跨空间知识迁移}的过程。在本章中，知识迁移主要体现在\textbf{架构解的迁移}，即利用源域的优良架构来指导目标域的搜索。形式化地，设$a^{(S)}_* \in \mathcal{A}^{(S)}$表示源域上一组表现优异的架构解（可以是单个架构，也可以是若干架构组成的集合，通常由源域NAS得到的最优或次优架构构成）。我们的目标是设计一种迁移策略$\mathcal{T}$，将$a^{(S)}_*$映射为目标域空间中的一个候选架构或架构集合$\mathcal{T}(a^{(S)}_*) \subset \mathcal{A}^{(T)}$，使得这些候选在目标域上也能取得较好的性能表现。换言之，$\mathcal{T}$实现了源空间架构解向目标空间解的\textbf{映射/转移}。借助前述架构表示，我们可以将$\mathcal{T}$拆解为两个步骤：首先将源架构编码到表示空间，$\mathbf{z}^{(S)} = E(a^{(S)}_*)$；然后在表示空间中施加一个跨域变换$\mathbf{z}^{(T)} = M(\mathbf{z}^{(S)})$，最后将变换后的表示解码回目标架构，$\hat{a}^{(T)} = D(\mathbf{z}^{(T)})$。这样得到的$\hat{a}^{(T)}$即为与源架构$a^{(S)}_*$"对应"的目标架构。

上述过程的核心在于\textbf{跨域映射函数}$M: \mathcal{Z} \to \mathcal{Z}$的学习。$M$的作用是\textbf{桥接源域与目标域的表示空间差异}，使得源域的架构表示经过$M$后，能够落到与某个目标架构表示相符合的位置。一种直观的思路是利用源域和目标域中架构的性能信息来学习$M$：例如，我们可以将源域的一些架构按照性能高低排序，并在目标域中找到性能对应排名的架构，将二者的表示向量作为锚点进行映射学习。又或者，可以最小化$M(\mathbf{z}^{(S)})$与一些性能等价的$\mathbf{z}^{(T)}$之间的差异来训练$M$。本章的方法采用了显式的映射学习策略，具体细节将在后续章节给出。这里我们只需要知道，通过学习到的$M$，我们能够实现\textbf{显式的解迁移}：即直接将源域优秀解映射为目标域解供进一步评估和优化。这种方案区别于仅利用源域训练一个性能预测器的做法——我们不是间接地指导搜索，而是\textbf{直接提供了一个可供选择的架构解}。正因如此，我们的方法被称为显式解迁移。

值得一提的是，在知识迁移的过程中，我们还会结合\textbf{架构性能预测}的信息来辅助映射的建立。例如，我们可以训练一个\textbf{相对性能排序模型}（Ranker），输入架构的表示$\mathbf{z}$，输出一个相对性能评分，用于比较不同架构的优劣。这样的预测模型在源域和目标域上分别训练或微调后，可以帮助我们判定$M$映射所得的候选架构是否可能是高性能的，从而筛选出最有前景的迁移架构进入下一步的搜索过程。这种结合性能表征的表示优化，使得跨域映射更加可靠。

\subsection{进化式序贯搜索的建模}

有了可以在目标空间中生成迁移架构候选的方法，接下来需要定义\textbf{如何在目标域高效搜索}。本章采用了\textbf{进化算法}作为目标域NAS的基本求解器，并融入了上述迁移策略，形成\textbf{进化序贯迁移优化}的框架。所谓"序贯"，指的是我们将源任务和目标任务的优化过程串联起来：源任务首先独立完成NAS获得优秀架构，然后这些架构通过迁移成为目标任务优化的起点。这一流程契合前面提到的进化迁移优化思想。

具体来说，在目标域NAS的进化算法中，我们对初始种群$P(0)$的构造进行了\textbf{迁移增强}：$P(0)$的一部分个体来自于源域迁移而来的架构解（即$\{\hat{a}^{(T)}_i\}$集合，其中每个$\hat{a}^{(T)}_i = \mathcal{T}(a^{(S)*}_i)$对应一个源域优秀架构），其余个体则随机生成以保证多样性。通过这种方式，目标域的进化搜索一开始就拥有了一批质量较高的候选架构。随后，进化算法按照经典流程迭代：在第$g$代($g\ge1$)，种群$P(g)$从上一代$P(g-1)$中选择适应度靠前的个体作为父本，通过\textbf{交叉}和\textbf{变异}操作产生子代架构，并用目标任务的评价函数$f^{(T)}(\cdot)$评估每个新架构的性能，将评估结果作为适应度。然后根据适应度选择出新一代的存活个体。这个过程不断重复，直到达到预定的代数或性能收敛条件。由于初始种群已注入了源域的"优良基因"，整个进化过程将明显加速并朝着更优解收敛。我们的实验观察到，在搜索早期，迁移解的引入使得种群的最佳适应度远高于纯随机初始化的情况，显著提升了算法的起点；而在随后的迭代中，这些高质量个体通过交叉变异不断拓展和探索了目标空间的邻域，加快了发现全局最优架构的进程。

形式化地，可以将目标域的进化迁移NAS视作在传统进化算法$EA$中集成了一个解迁移算子$\mathcal{T}$。对于目标域优化问题$\min_{a \in \mathcal{A}^{(T)}} L_{\text{val}}(w^*(a), a)$，经典遗传算法解决此问题可表示为$EA(\mathcal{A}^{(T)}, f^{(T)})$，其中$f^{(T)}$为适应度评估（例如验证误差的相反数）。现在，我们引入源域解集合$A^*_S=\{a^{(S)*}_1,\dots,a^{(S)*}_k\}$，则迁移增强的进化NAS过程可表示为：

\begin{equation}
	P(0) = \mathcal{T}(A^*_S) \cup P_{\text{rand}},
\end{equation}

\begin{equation}
	P(g) = EA_step\big(P(g-1), f^{(T)}\big), \quad g=1,2,\dots,G,
\end{equation}

其中$\mathcal{T}(A^*_S) = \{\mathcal{T}(a^{(S)*}_i): a^{(S)*}_i \in A^*_S\}$是迁移到目标域的架构集合，$P_{\text{rand}}$表示随机采样的若干目标域架构个体，$EA_step$表示遗传算法包含的选择、交叉、变异及生存筛选操作。在$g=0$代，我们通过迁移提供了一批高质量个体；在每一代的进化步骤中，种群依据适应度进化。在这里$\mathcal{T}$起到了将源域知识嵌入初始解的作用，从数学优化角度看，这等价于对解空间的初始分布施加了一种先验偏置，使搜索不再从均匀随机分布出发，而是从一个更靠近全局最优的分布开始。

需要说明的是，在我们的框架中，源域架构知识的引入不限于初始种群阶段。更一般地，可以在进化过程中动态地利用源域知识。例如，可设想一种自适应策略：当目标种群陷入停滞时，引入新的迁移架构以激发多样性；或者根据当前种群的分布，选择性地从源域调取与之互补的架构注入。这些扩展虽然并未在本章的主要方法中实现，但它们体现了进化迁移优化的灵活性和潜力。

\subsection{模型边界与假设}

在给出形式化定义之后，我们有必要明确本章研究问题的\textbf{边界和假设}，以厘清方法适用的范围。首先，我们假设源任务和目标任务虽在架构形式上存在差异，但在\textbf{任务目标}上具有一定相关性。例如，它们可能都是图像识别任务（只是在数据规模或细粒程度上不同），或者都是监督学习任务（评价指标相似）。这样假设的目的是确保源域的架构经验对目标域具有潜在参考价值。如果源任务是图像分类而目标任务是语音识别，任务本质差异过大，则纯粹的架构迁移可能效果有限，需要结合其他知识迁移（超出了本章范围）。

其次，我们默认源域已经经过充分的NAS优化，获得了一批高性能架构以及对应的评估结果。这一假设在很多场景下是合理的：源域可以看作已经完成的历史任务，其NAS搜索日志或最优架构是现成可用的。本章的方法并不涉及源域NAS的具体过程，而是关注如何\textbf{利用}其结果。在实践中，源域知识也可能来自公开基准（如NAS-Bench系列）或文献报道的优秀架构，这些都满足我们的假设前提。

再次，我们在表示学习中假设源域和目标域的架构\textbf{都可以被编码到统一长度或维度}的向量表示。这需要在设计架构编码方案时，考虑不同空间架构表示的对齐。本章所采用的序列化编码方式通过引入"占位符"等技术，实现了对可变节点数量架构的固定长度表示。同时，为兼容不同操作集合，我们为源域和目标域分别建立操作符词表，并通过嵌入向量使不同域的操作在表示空间中可比较。这些技术细节保证了$E(a^{(S)})$和$E(a^{(T)})$输出同维度的$\mathbf{z}$向量，使表示空间$\mathcal{Z}$对跨域架构是统一的。读者需要注意的是，尽管维度统一，但这并不意味着源域和目标域架构的分布天然就对齐。正如前述，我们仍需学习映射$M$来调整两者在$\mathcal{Z}$中的相对位置关系。

最后，我们明确本章所侧重的是\textbf{架构层面的迁移}，而非参数或数据的迁移。我们并不直接迁移源域模型的权重到目标模型中，也不假设目标域可以访问源域的数据。所有迁移发生在"架构设计知识"这一层面，即哪些拓扑结构和算子组合是有效的。这一区别使我们的研究聚焦于NAS算法本身的改进，而无需涉及训练过程的变化（例如微调等）。这样做的好处是方法的通用性更强：对于任意给定的源域架构及目标数据集，我们的方法都可以尝试应用，而不需要源、目标任务在数据分布或标签空间上有联系。

综上，本章定义了跨空间进化可迁移NAS问题，即：给定源域搜索空间$\mathcal{A}^{(S)}$及其优秀架构集合$A^*_S$，以及目标域搜索空间$\mathcal{A}^{(T)}$，我们要寻找一种方法，能利用$A^*_S$所包含的结构知识，显著提升在$\mathcal{A}^{(T)}$上找到高性能架构的效率和效果。为此，我们引入统一的架构表示$\Phi$和跨域映射$M$，以及融合解迁移的进化算法，共同构成BRIDGE框架的理论基石。在下一章中，我们将基于这些定义展开具体的方法论阐述，包括架构表示学习模型的设计、跨域映射的训练策略以及进化迁移NAS算法的实现细节。可以预见，良好的理论定义将有助于指导方法的实现，并为实验结果的解释提供依据。通过本节的形式化建模，我们已经为跨空间架构迁移问题建立了清晰的问题边界和目标函数，这为后续章节深入探讨解决方案奠定了必要的理论基础。

\section[\hspace{-2pt}跨空间进化可迁移 NAS 框架]{\heiti\zihao{-3}\hspace{-8pt}跨空间进化可迁移 NAS 框架}

\begin{figure}[htbp]
	\centering
	\caption{
		\textsc{Bridge} 框架的整体概览。
		整个流程可分为三个关键阶段：
		(1) 通过有监督与无监督表示学习构建神经架构的紧凑且语义丰富的隐式表示；
		(2) 在源域与目标域的表示空间之间学习一个结构保持的映射函数，以对齐不同搜索空间下的架构语义；
		(3) 利用所学表示模型与跨域映射，将源域中历史搜索获得的高性能架构解高效迁移至目标域，作为进化搜索的高质量初始种群，从而显著加速目标域的神经架构搜索过程。
	}\label{fig:overview}
\end{figure}
\end{document}
