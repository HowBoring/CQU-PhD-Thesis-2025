\cqusetup{
	%		注意	
	%	* 1. \cqusetup{}中不能出现全空的行，如果需要全空行请在行首注释
	%	* 2. 不需要的配置信息可以放心地坐视不理、留空、删除或注释（都不会有影响）
	%	*
	%	
	% ===================
	%	论文的中英文题目
	% ===================
	ctitle = {基于知识迁移的深度模型高效构建方法研究},
	etitle = {Research on Efficient Construction Methods for Deep Learning Models Based on Knowledge Transfer},
	% ===================
	% 作者部分的信息
	% \secretize{}为盲审标记点，在打开盲审开关时内容会自动被替换为*输出，盲审开关默认关闭
	% ===================
	cauthor = \secretize{侯博宇},	% 你的姓名，以下每项都以英文逗号结束
	eauthor = \secretize{Boyu~Hou},	% 姓名拼音，~代表不会断行的空格
	studentid = \secretize{20211401002},	% 仅本科生，学号
	csupervisor = \secretize{冯~~~亮~~~~~教授},	% 导师的姓名
	esupervisor = \secretize{{Prof.~Liang Feng}},	% 导师的姓名拼音
	cassistsupervisor = \secretize{}, % 本科生可选，助理指导教师姓名，不用时请留空为{}
	cextrasupervisor = \secretize{}, % 本科生可选，校外指导教师姓名，不用时请留空为{}
	eassistsupervisor = \secretize{}, % 本科生可选，助理指导教师或/和校外指导教师姓名拼音，不用时请留空为{}
	cpsupervisor = \secretize{}, % 仅专硕，兼职导师姓名
	epsupervisor = \secretize{},	% 仅专硕，兼职导师姓名拼音
	cclass = \secretize{\rmfamily{2025}\heiti{年}\rmfamily{12}\heiti{月}},	% 博士生和学硕填学科门类，学硕填学科类型
	edgree = {},	% 专硕填Professional Degree，其他按实情填写
	% % 提示：如果内容太长，可以用\zihao{}命令控制字号，作用范围：{}内
	cmajor = {工~~~学},	% 专硕不需填，填写专业名称
	emajor = {Computer Science and Technology}, % % 专硕不需填，填写专业英文名称
	cmajora = {计算机科学与技术},	% 专硕不需填，填写专业名称
	cmajorb = {深度学习},
	% cmajorc = \secretize{},
	% cmajord = 2024年6月,
	% ===================
	% 底部的学院名称和日期
	% ===================
	cdepartment = 计算机学院,	%学院名称
	edepartment = College of Computer Science, %学院英文名称
	% ===================
	% 封面的日期可以自动生成（注释掉时），也可以解除注释手动指定，例如：二〇一六年五月
	% ===================
	mycdate = {2025年12月},
	myedate = {December 2025},
}% End of \cqusetup
% ===================
%
% 论文的摘要
%
% ===================
\begin{cabstract}	% 中文摘要
	作为推动新一轮科技革命的关键技术力量，深度学习在模型架构、数据规模和计算资源的共同驱动下取得显著进展。但由此引发的训练与构建成本快速攀升，使模型规模和构建效率的矛盾成为深度学习发展和应用的一项重要制约因素。为实现深度模型的高效构建，本文引入“深度模型构建”概念，将以往以参数优化为中心的模型训练视角扩展为覆盖模型设计与实现、数据与知识获取、模型参数实现三个核心环节的完整构建流程。基于这一视角，现有深度模型构建的效率瓶颈对应地体现为模型架构设计与评估成本、数据标注与整理成本，以及参数实现的计算和能耗开销等。而在现有的以效率为导向的深度模型构建方法中，知识迁移作为一种通过对已有的信息进行提取和复用的途径，具有广泛适配性，能够有效提升深度模型的构建效率。本文即是以知识迁移作为核心的指导思想与方法论，对深度模型高效构建这一命题进行深入的研究。

	本文在上述三环节框架的基础上，进一步结合深度学习的发展脉络进行分析。在不同的技术发展阶段，深度模型构建所面临的核心瓶颈与研究焦点，在上述三个环节中呈现出动态演进的侧重。对于早期深度模型如卷积神经网络（Convolutional Neural Network，CNN），性能的突破在很大程度上依赖于架构工程的创新，模型架构的设计与评估效率是核心瓶颈；对于Transformer等统一且强大的架构范式，数据与知识的利用效率是研究的焦点，深度模型高效构建的瓶颈转移至数据与知识的利用效率上；而对于大语言模型（Large Language Model，LLM），模型参数的高效调整以实现任务能力的复用成为主要挑战，深度模型高效构建的瓶颈再次转移至对模型参数进行高效的调整以实现任务能力的高效复用。
	本文的研究工作即是围绕这一演进脉络，针对不同阶段涌现出的关键效率障碍展开，主要研究内容与创新点如下：

	（1）针对神经架构搜索（NAS）的架构经验难以在异构的搜索空间之间迁移复用的问题，本文提出了基于表征学习的跨空间可迁移神经架构搜索。本文首先提出了统一的模型架构编码与混合监督的神经架构表征学习方法，以构建平滑而易于迁移的连续表征空间。基于该表征嵌入，本文进一步设计了跨域表征映射机制，实现了跨空间的模型架构迁移能力。基于该迁移能力，本文设计了演化顺序迁移优化的神经架构搜索算法来缓解负迁移问题。最后，本文在CNN的多个规模的异构搜索空间之间进行了充分验证。实验结果表明，所提方法在多个异构搜索空间上均实现了显著的架构设计效率和模型性能提升。

	（2）基于 Transformer 的预训练模型在下游应用中面临的标注数据稀缺、知识蒸馏不稳定等问题，本文提出了基于双教师提示对比学习的少样本知识蒸馏。本文首先通过提示学习增强蒸馏过程中的知识利用效率。在此基础上，本文引入两类互补教师模型以丰富学习信号：一类是提供通用语义知识的预训练教师模型；另一类是提供任务特定知识的提示微调教师模型。此外，为了进一步提升对少样本数据的蒸馏效率，本文提出中间层表示的对比学习，帮助学生模型高效地与教师模型进行知识的对齐。最后，本文以基于Transformer的双向编码器（Bidirectional Encoder Representations from Transformers，BERT）预训练语言模型为载体，在8个广泛使用的自然语言处理测试基准上进行了充分验证。所提方法在少样本条件下相较于现有蒸馏和微调方法的显著优势。

	（3）针对如何高效组合多个已有专家LLM的能力，同时避免在免重复训练模型融合中参数冲突与性能退化的问题，本文提出了知识图谱引导的多形式优化模型融合。本文提出构建一个动态演化的知识图谱，用图结构显示编码模型和任务间的复杂关系信息。其次，本文引入多形式迁移优化思想，通过课程规划器自适应地提出子融合问题，并利用知识引导的演化求解器高效求解。对子融合问题的求解将不断填充和更新知识图谱，并利用更新后的信息进一步求解更复杂子融合问题，进入探索和利用的闭环学习过程，直至完成整体融合求解。本文基于七个不同专长的LLM，在多个不同能力方向的测试基准任务上进行了充分验证，所提方法相对于现有的模型融合方法实现了更高的跨任务能力，以及更少的性能衰减。

\end{cabstract}
% 中文关键词，请使用英文逗号分隔：
\ckeywords{深度神经网络;高效人工智能;迁移学习;大语言模型}

\begin{eabstract}	% 英文摘要
	As a key technological force driving the new round of scientific and technological revolution, deep learning has achieved remarkable progress through the synergistic advancement of model architectures, data scale, and computational resources. However, the resulting rapid escalation of training and construction costs has made the contradiction between model scale and construction efficiency a major obstacle to the development and application of deep learning. To achieve efficient construction of deep models, this paper introduces the concept of ``deep model construction,'' expanding the traditional parameter optimization-centered training perspective to encompass a complete construction process covering three core stages: model design and implementation, data and knowledge acquisition, and model parameter realization. From this perspective, the efficiency bottlenecks in existing deep model construction correspondingly manifest as the costs of model architecture design and evaluation, data annotation and organization, and the computational and energy consumption overhead of parameter realization. Among existing efficiency-oriented deep model construction methods, knowledge transfer, as an approach that extracts and reuses existing information, possesses broad adaptability and can effectively improve the construction efficiency of deep models. This paper conducts in-depth research on the proposition of efficient deep model construction, using knowledge transfer as the core guiding principle and methodology.

	Building upon the aforementioned three-stage framework, this paper further analyzes the development trajectory of deep learning. At different stages of technological development, the core bottlenecks and research focuses of deep model construction exhibit dynamically evolving emphases across these three stages. The research work in this paper revolves around this evolutionary trajectory, targeting the key efficiency barriers emerging at different stages. The main research content and innovations are as follows:

	(1) In the early stages of deep learning development, the performance breakthroughs of deep models dominated by Convolutional Neural Networks (CNNs) largely depended on innovations in architectural engineering, where optimization of architectural topology could significantly enhance the representational capacity of deep models. At this stage, one of the core bottlenecks in efficient deep model construction was reflected in the design and evaluation efficiency of model architectures. Addressing the problem that architectural experience from Neural Architecture Search (NAS) is difficult to transfer and reuse across heterogeneous search spaces, this paper proposes cross-space transferable neural architecture search based on representation learning. First, this paper proposes a unified model architecture encoding and hybrid-supervised neural architecture representation learning method to construct a smooth and easily transferable continuous representation space. Based on this representation embedding, this paper further designs a cross-domain representation mapping mechanism, achieving cross-space model architecture transfer capability. Leveraging this transfer capability, this paper designs an evolutionary progressive transfer optimization neural architecture search algorithm to mitigate negative transfer issues. Finally, this paper conducts comprehensive validation across multiple heterogeneous search spaces of different scales for CNNs. Experimental results demonstrate that the proposed method achieves significant improvements in both architectural design efficiency and model performance across multiple heterogeneous search spaces.

	(2) As relatively unified and powerful architectural paradigms such as Transformers became mainstream, research focus gradually shifted from searching architectural topologies to efficiently utilizing the representational capacity of models. In this context, the bottleneck of efficient deep model construction transferred to the utilization efficiency of data and knowledge. Addressing the problems of scarce annotated data and unstable knowledge distillation faced by pretrained models in downstream applications, this paper proposes dual-teacher prompt-based contrastive learning for few-shot knowledge distillation. First, this paper enhances knowledge utilization efficiency in the distillation process through prompt learning. On this basis, this paper introduces two types of complementary teacher models to enrich learning signals: one type is pretrained teacher models providing general semantic knowledge; the other type is prompt-tuned teacher models providing task-specific knowledge. Furthermore, to further improve distillation efficiency on few-shot data, this paper proposes contrastive learning of intermediate layer representations to help student models efficiently align knowledge with teacher models. Finally, this paper conducts comprehensive validation on 8 widely-used natural language processing benchmarks using Bidirectional Encoder Representations from Transformers (BERT) pretrained language models as the carrier. The proposed method demonstrates significant advantages over existing distillation and fine-tuning methods under few-shot conditions.

	(3) Entering the Large Language Model (LLM) era, with both architecture and data reaching enormous scales, research began focusing on how to guide large models to work in specific downstream task domains. At this point, the bottleneck of efficient deep model construction shifted again to efficiently adjusting model parameters to achieve efficient reuse of task capabilities. Addressing how to efficiently combine the capabilities of multiple existing expert LLMs while avoiding parameter conflicts and performance degradation in training-free model merging, this paper proposes knowledge graph-guided multiform optimization for model merging. This paper proposes constructing a dynamically evolving knowledge graph that explicitly encodes complex relational information between models and tasks using graph structures. Second, this paper introduces the multiform transfer optimization concept, adaptively proposing merging subproblems through a curriculum planner and efficiently solving them using a knowledge-guided evolutionary solver. Solutions to sub-merging problems continuously populate and update the knowledge graph, and the updated information is further utilized to solve more complex sub-merging problems, entering a closed-loop learning process of exploration and exploitation until the complete merging problem is solved. This paper conducts comprehensive validation on multiple benchmark tasks across different capability dimensions based on seven LLMs with different expertise. The proposed method achieves higher cross-task capabilities and lower performance degradation compared to existing model merging methods.
\end{eabstract}
% 英文关键词，请使用英文逗号分隔，关键词内可以空格：
\ekeywords{Deep Neural Networks; Efficient Artificial Intelligence; Transfer Learning; Large Language Model}

% 封面和摘要配置完成