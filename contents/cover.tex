\cqusetup{
	%	************	注意	************
	%	* 1. \cqusetup{}中不能出现全空的行，如果需要全空行请在行首注释
	%	* 2. 不需要的配置信息可以放心地坐视不理、留空、删除或注释（都不会有影响）
	%	*
	%	********************************
	% ===================
	%	论文的中英文题目
	% ===================
	ctitle = {基于知识迁移的深度模型高效构建研究},
	etitle = {Efficient Deep Model Construction via Knowledge Transfer},
	% ===================
	% 作者部分的信息
	% \secretize{}为盲审标记点，在打开盲审开关时内容会自动被替换为***输出，盲审开关默认关闭
	% ===================
	cauthor = \secretize{侯博宇},	% 你的姓名，以下每项都以英文逗号结束
	eauthor = \secretize{Boyu~Hou},	% 姓名拼音，~代表不会断行的空格
	studentid = \secretize{20211401002},	% 仅本科生，学号
	csupervisor = \secretize{冯~~~亮~~~~~教授},	% 导师的姓名
	esupervisor = \secretize{{Prof.~Liang Feng}},	% 导师的姓名拼音
	cassistsupervisor = \secretize{}, % 本科生可选，助理指导教师姓名，不用时请留空为{}
	cextrasupervisor = \secretize{}, % 本科生可选，校外指导教师姓名，不用时请留空为{}
	eassistsupervisor = \secretize{}, % 本科生可选，助理指导教师或/和校外指导教师姓名拼音，不用时请留空为{}
	cpsupervisor = \secretize{}, % 仅专硕，兼职导师姓名
	epsupervisor = \secretize{},	% 仅专硕，兼职导师姓名拼音
	cclass = \secretize{\rmfamily{2025}\heiti{年}\rmfamily{12}\heiti{月}},	% 博士生和学硕填学科门类，学硕填学科类型
	edgree = {},	% 专硕填Professional Degree，其他按实情填写
	% % 提示：如果内容太长，可以用\zihao{}命令控制字号，作用范围：{}内
	cmajor = {工~~~~学},	% 专硕不需填，填写专业名称
	emajor = {Computer Science and Technology}, % % 专硕不需填，填写专业英文名称
	cmajora = {计算机科学与技术},	% 专硕不需填，填写专业名称
	cmajorb = {深度学习},
	% cmajorc = \secretize{},
	% cmajord = 2024年6月,
	% ===================
	% 底部的学院名称和日期
	% ===================
	cdepartment = 计算机学院,	%学院名称
	edepartment = College of Computer Science, %学院英文名称
	% ===================
	% 封面的日期可以自动生成（注释掉时），也可以解除注释手动指定，例如：二〇一六年五月
	% ===================
	mycdate = {2025年12月},
	myedate = {December 2025},
}% End of \cqusetup
% ===================
%
% 论文的摘要
%
% ===================
\begin{cabstract}	% 中文摘要
	深度学习在众多领域取得了显著成就，但其模型性能的提升往往伴随着数据、算力与人力成本的急剧增长，形成了严峻的“规模-效率”矛盾，制约了技术的广泛应用与可持续发展。为应对这一挑战，本文以“知识迁移”作为核心指导思想，旨在探索通过系统性地复用与传递已有知识资产以提升深度模型构建效率的方法论。基于对模型构建过程的分析，研究从知识获取、结构承载与参数实现三个关键环节出发，分别探讨了知识迁移思想在不同维度上的具体应用与实践。

	在知识层面，针对少样本场景下模型对标注数据高度依赖且知识蒸馏稳定性差的问题，本文提出了 Prompt-Distiller 方法。该方法通过融合双教师知识与对比学习策略，实现了低资源条件下从大型提示学习模型到小型学生模型的鲁棒知识迁移，显著提升了数据匮乏环境下的模型构建效率与性能 。

	在结构层面，为解决神经架构搜索 (Neural Architecture Search, NAS) 成本高昂且架构经验难以跨越异构搜索空间复用的瓶颈，本文构建了 \textsc{Bridge} 框架。该框架通过统一的神经架构表示学习与跨域映射机制，结合进化序贯迁移优化（ESTO）策略，首次实现了跨异构搜索空间的架构知识有效迁移，大幅降低了自动化模型设计的开销 。

	在参数层面，聚焦于参数融合中存在的稳定性与性能保持难题，本文提出了 KG-MFTO 范式。该方法利用知识图谱显式建模模型关系，并通过课程规划与知识引导的进化求解器，在零训练条件下实现了多个大型语言模型（LLM）参数的高效、稳定融合，为模型能力的即时、低成本集成提供了新的解决方案 。

	本文通过 Prompt-Distiller、\textsc{Bridge} 和 KG-MFTO 这三项分别聚焦于知识、结构、参数层面的独立研究工作，为深度模型的高效构建提供了具体的技术手段，验证了知识迁移思想在应对不同效率瓶颈方面的有效性与潜力，为推动深度学习向更高效、可持续的方向发展贡献了有益的探索。
\end{cabstract}
% 中文关键词，请使用英文逗号分隔：
\ckeywords{深度学习；高效人工智能；知识迁移；小样本学习；知识蒸馏；神经架构搜索；模型融合}

\begin{eabstract}	% 英文摘要
	Deep learning has achieved remarkable success across numerous domains, yet the enhancement of model performance is often accompanied by a dramatic increase in data, computational, and human costs. This has led to a critical "scale-efficiency" contradiction, hindering the technology's widespread application and sustainable development. To address this challenge, this dissertation adopts "Knowledge Transfer" as the core guiding principle, aiming to explore methodologies for improving the efficiency of deep model construction by systematically reusing and transferring existing knowledge assets. Based on an analysis of the model construction process, this research investigates the specific applications and practices of the knowledge transfer concept within three key dimensions: knowledge acquisition, structure bearing, and parameter implementation, exploring each independently.

	At the knowledge layer, addressing the high dependency on labeled data and the instability of knowledge distillation in few-shot scenarios, this dissertation proposes the Prompt-Distiller method. By integrating dual-teacher knowledge and a contrastive learning strategy, Prompt-Distiller achieves robust knowledge transfer from large prompt-based learning models to small student models under low-resource conditions, significantly enhancing model construction efficiency and performance in data-scarce environments .

	At the structure layer, to tackle the high computational cost of Neural Architecture Search (NAS) and the difficulty of reusing architectural experience across heterogeneous search spaces, the \textsc{Bridge} framework is constructed. Through unified neural architecture representation learning, cross-domain mapping mechanisms, and an Evolutionary Sequential Transfer Optimization (ESTO) strategy, \textsc{Bridge} enables effective architectural knowledge transfer across heterogeneous search spaces for the first time, substantially reducing the overhead of automated model design .

	At the parameter layer, focusing on the stability and performance retention challenges in parameter merging, the KG-MFTO paradigm is proposed. This method utilizes a knowledge graph to explicitly model inter-model relationships and employs curriculum planning along with a knowledge-guided evolutionary solver to achieve efficient and stable merging of parameters from multiple Large Language Models (LLMs) under zero-training conditions, offering a novel solution for instant, low-cost capability integration .

	Through these three independent research efforts—Prompt-Distiller, \textsc{Bridge}, and KG-MFTO—focused respectively on the knowledge, structure, and parameter layers, this dissertation provides concrete technical approaches for efficient deep model construction. It validates the effectiveness and potential of the knowledge transfer principle in addressing diverse efficiency bottlenecks, contributing valuable explorations toward advancing deep learning in a more efficient and sustainable direction.
\end{eabstract}
% 英文关键词，请使用英文逗号分隔，关键词内可以空格：
\ekeywords{Deep Learning; Efficient Artificial Intelligence; Knowledge Transfer; Few-Shot Learning; Knowledge Distillation; Neural Architecture Search; Model Merging}

% 封面和摘要配置完成