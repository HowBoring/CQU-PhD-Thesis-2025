\cqusetup{
	%	************	注意	************
	%	* 1. \cqusetup{}中不能出现全空的行，如果需要全空行请在行首注释
	%	* 2. 不需要的配置信息可以放心地坐视不理、留空、删除或注释（都不会有影响）
	%	*
	%	********************************
	% ===================
	%	论文的中英文题目
	% ===================
	ctitle = {基于知识迁移的深度模型高效构建方法研究},
	etitle = {Research on Efficient Construction Methods for Deep Learning Models Based on Knowledge Transfer},
	% ===================
	% 作者部分的信息
	% \secretize{}为盲审标记点，在打开盲审开关时内容会自动被替换为***输出，盲审开关默认关闭
	% ===================
	cauthor = \secretize{侯博宇},	% 你的姓名，以下每项都以英文逗号结束
	eauthor = \secretize{Boyu~Hou},	% 姓名拼音，~代表不会断行的空格
	studentid = \secretize{20211401002},	% 仅本科生，学号
	csupervisor = \secretize{冯~~~亮~~~~~教授},	% 导师的姓名
	esupervisor = \secretize{{Prof.~Liang Feng}},	% 导师的姓名拼音
	cassistsupervisor = \secretize{}, % 本科生可选，助理指导教师姓名，不用时请留空为{}
	cextrasupervisor = \secretize{}, % 本科生可选，校外指导教师姓名，不用时请留空为{}
	eassistsupervisor = \secretize{}, % 本科生可选，助理指导教师或/和校外指导教师姓名拼音，不用时请留空为{}
	cpsupervisor = \secretize{}, % 仅专硕，兼职导师姓名
	epsupervisor = \secretize{},	% 仅专硕，兼职导师姓名拼音
	cclass = \secretize{\rmfamily{2025}\heiti{年}\rmfamily{12}\heiti{月}},	% 博士生和学硕填学科门类，学硕填学科类型
	edgree = {},	% 专硕填Professional Degree，其他按实情填写
	% % 提示：如果内容太长，可以用\zihao{}命令控制字号，作用范围：{}内
	cmajor = {工~~~学},	% 专硕不需填，填写专业名称
	emajor = {Computer Science and Technology}, % % 专硕不需填，填写专业英文名称
	cmajora = {计算机科学与技术},	% 专硕不需填，填写专业名称
	cmajorb = {深度学习},
	% cmajorc = \secretize{},
	% cmajord = 2024年6月,
	% ===================
	% 底部的学院名称和日期
	% ===================
	cdepartment = 计算机学院,	%学院名称
	edepartment = College of Computer Science, %学院英文名称
	% ===================
	% 封面的日期可以自动生成（注释掉时），也可以解除注释手动指定，例如：二〇一六年五月
	% ===================
	mycdate = {2025年12月},
	myedate = {December 2025},
}% End of \cqusetup
% ===================
%
% 论文的摘要
%
% ===================
\begin{cabstract}	% 中文摘要
	作为推动新一轮科技革命的关键技术力量，深度学习在数据规模、模型架构与计算资源的共同驱动下取得显著进展，但由此引发的训练与构建成本快速攀升，使“规模–效率矛盾”成为制约进一步发展的关键瓶颈。为清晰刻画效率瓶颈，本文引入“深度模型构建（Deep Model Construction）”概念，将以往以参数优化为中心的模型训练视角扩展为覆盖数据与知识准备、模型设计与实现、模型参数实现三个核心环节的完整构建流程；构建效率对应地体现为数据标注与整理成本、架构选择与评估成本，以及参数实现与优化的计算和能耗开销等。
	现有方法在三个环节形成了较为丰富的技术谱系：在数据与知识准备方面，少样本、半监督与知识蒸馏等方法降低了对大规模标注的依赖；在模型设计与实现方面，轻量化算子、剪枝量化与神经架构搜索（Neural Architecture Search，NAS）提高了结构与实现的性价比；在参数实现方面，预训练–微调与参数高效微调（Parameter-Efficient Fine-Tuning，PEFT）、以及免训练的模型融合提升了复用效率。然而，性能优先、效率滞后的评测取向仍然常见；同时，面向效率的做法在三个环节各自存在核心问题，例如：少样本情境下的知识迁移稳定性不足，异构搜索空间之间的架构经验难以复用，免训练模型融合中的参数冲突与任务性能退化仍未得到有效解决。
	针对上述问题，本文采用知识迁移作为研究视角，围绕深度模型构建的三个关键环节分别开展研究，目标是提升数据效率、搜索效率与参数实现效率。主要研究内容与创新点如下：

	（1）数据与知识准备环节：基于双教师对比学习的少样本知识蒸馏。对于少样本场景下知识蒸馏不稳定、易过拟合的问题，本文充分挖掘可利用的外部与隐含信息，通过多视角教师知识与对比式正则约束共同丰富学生模型的学习信号，从而提升其在少样本条件下的表征判别性与迁移稳定性。不同于传统仅依赖单一教师输出概率分布的蒸馏方式，本文从多层次与多视角出发，引入两类互补教师模型：一方面，使用规模较大的预训练教师模型，提供全局语义层面的通用知识；另一方面，采用经少样本提示微调后的教师模型，使其在目标任务上具备更强的领域适应性。二者的结合构成一种多视角后验知识体系，使学生模型既能继承通用表征，又能获得任务特定的细粒度知识。在多个测试基准上的广泛实验表明，所提方法在少样本条件下显著优于现有蒸馏与微调方法，验证了多教师协同与对比增强的知识迁移策略在提升数据效率方面的有效性。

	（2）架构设计环节：基于表征学习的跨空间可迁移神经架构搜索。针对异构搜索空间间难以复用架构和设计经验、冷启动成本高的问题，本文首先提出了统一的模型构架编码，以适应异构的架构搜索空间。其次，提出了混合监督的神经架构的表征学习方法，将神经架构嵌入到平滑的连续表征空间。基于该嵌入，构建空间之间的“桥接”映射与迁移式性能预测，实现跨空间的快速筛选与早停。该方案在保持所得架构竞争力的同时，显著降低搜索与评估开销，提升搜索效率与经验复用度。实验结果表明，所提方法在多个异构搜索空间上均实现了优异的搜索效率与架构性能，验证了跨空间知识迁移在提升神经架构搜索效率方面的有效性。此外，还提供了对迁移机制与表征空间的深入分析，揭示了架构表征学习与跨空间迁移的内在联系。

	（3）模型参数实现环节：知识图谱引导的多形式优化模型融合。面向多模型融合的参数冲突与任务性能退化问题，本文提出构建一个动态演化的知识图谱，用图结构显式编码模型–模型和模型–任务间的关系信息，并通过图注意力网络（Graph Attention Networks, GAT）将模型和任务嵌入到连续表征空间。其次，通过一个课程规划器基于多形式迁移优化（Multi-Form Transfer Optimization，MFTO）的思想自适应地提出子问题（即形式）。提出的子问题将交由一个知识引导的演化求解器，融合领域知识以热启动 CMA-ES 演化算法，用来自知识图谱的先验指导搜索。通过这三者的配合，本文实现了优化过程中的持续知识积累与迁移：每当求解一个子问题后，系统都会将所得的模型–任务适应性结果反馈回知识图谱，更新图结构与节点表征，从而为后续子问题的求解提供更丰富的先验信息。实验结果表明，所提方法在多个大规模语言模型（Large Language Models, LLM）融合任务上均实现了优异的融合效率与任务性能，验证了知识图谱引导的多形式优化在提升模型参数实现效率方面的有效性。此外，还提供了对知识图谱结构与演化求解器行为的深入分析，揭示了知识引导优化与模型融合性能之间的内在联系。
\end{cabstract}
% 中文关键词，请使用英文逗号分隔：
\ckeywords{深度神经网络;高效人工智能;迁移学习;大语言模型}

\begin{eabstract}	% 英文摘要
	As a key driving force in the new wave of technological revolution, deep learning has made remarkable progress under the joint momentum of large-scale data, sophisticated model architectures, and powerful computational resources. However, the rapidly increasing cost of model training and construction has led to an escalating ``scale-efficiency contradiction,'' which has become a critical bottleneck for further advancement. To clearly characterize this bottleneck, this thesis introduces the concept of Deep Model Construction, extending the traditional training perspective centered on parameter optimization into a complete construction pipeline encompassing three core stages: data and knowledge preparation, model design and implementation, and parameter realization. Accordingly, construction efficiency reflects the costs of data annotation and organization, architecture selection and evaluation, as well as the computational and energy overheads of parameter realization and optimization.

	Existing studies have developed a rich technical spectrum across these three stages: in data and knowledge preparation, few-shot learning, semi-supervised learning, and knowledge distillation reduce the dependence on large-scale annotations; in model design and implementation, lightweight operators, pruning and quantization, and neural architecture search (NAS) improve structural and computational efficiency; in parameter realization, pre-training and fine-tuning, parameter-efficient fine-tuning (PEFT), and training-free model merging enhance reuse efficiency. Nevertheless, performance-centric evaluation remains prevalent, and efficiency-oriented approaches in each stage face core challenges: unstable knowledge transfer under few-shot settings, limited reusability of architectural experience across heterogeneous search spaces, and persistent parameter conflicts and task degradation in training-free model fusion.

	To address these issues, this thesis adopts knowledge transfer as the overarching research perspective and conducts systematic studies across the three critical stages of deep model construction, aiming to improve data efficiency, search efficiency, and parameter realization efficiency. The main research contents and contributions are as follows:

	(1) Data and Knowledge Preparation: Few-shot knowledge distillation via dual-teacher contrastive learning. To mitigate the instability and overfitting issues of knowledge distillation under few-shot conditions, this thesis fully exploits external and implicit information by jointly employing multi-view teacher knowledge and contrastive regularization to enrich the student model’s learning signals. Unlike traditional single-teacher distillation relying solely on output distributions, this work introduces two complementary teachers from different perspectives: \circled{1} a large pre-trained teacher providing general semantic knowledge, and \circled{2} a task-adapted teacher fine-tuned via few-shot prompting to capture task-specific representations. Their combination forms a multi-view posterior knowledge system, enabling the student to inherit both general and task-specific knowledge. Extensive experiments on multiple benchmarks demonstrate that the proposed method significantly outperforms existing distillation and fine-tuning baselines under few-shot settings, validating the effectiveness of multi-teacher collaboration and contrastive enhancement in improving data efficiency.

	(2) Architecture Design: Cross-space transferable neural architecture search via representation learning. To overcome the difficulty of reusing design experience across heterogeneous search spaces and the high cost of cold-start searches, this thesis first proposes a unified model encoding scheme applicable to diverse search spaces. Then, a hybrid-supervised architecture representation learning method is developed to embed neural architectures into a smooth continuous space. Based on this embedding, a ``bridge'' mapping and transfer-based performance prediction mechanism are established to enable cross-space rapid selection and early stopping. The proposed framework maintains competitive architecture quality while substantially reducing search and evaluation costs, thereby improving both search efficiency and experience transferability. Experimental results show consistent advantages across heterogeneous search spaces, confirming the effectiveness of cross-space knowledge transfer in enhancing NAS efficiency. In-depth analysis further reveals the intrinsic connection between architecture representation learning and cross-space transfer mechanisms.

	(3) Parameter Realization: Knowledge-graph-guided multi-form transfer optimization for model merging. To tackle parameter conflicts and task degradation in multi-model fusion, this thesis constructs a dynamically evolving knowledge graph that explicitly encodes model–model and model–task relationships. Graph Attention Networks (GATs) are used to embed models and tasks into a continuous representation space. A curriculum planner, inspired by Multi-Form Transfer Optimization (MFTO), adaptively decomposes the fusion problem into sub-problems (forms), which are then solved by a knowledge-guided evolutionary solver. The solver integrates domain knowledge to warm-start the CMA-ES evolutionary process, with priors derived from the knowledge graph guiding the optimization. Through the interplay of these components, the system achieves continual knowledge accumulation and transfer during optimization: after solving each sub-problem, model–task adaptation results are fed back into the knowledge graph to update its structure and embeddings, enriching priors for subsequent problems. Experimental results on multiple large language model (LLM) merging tasks demonstrate superior fusion efficiency and task performance, validating the effectiveness of knowledge-graph-guided multi-form optimization for improving parameter realization efficiency. Further analyses elucidate the intrinsic connection between knowledge-guided optimization dynamics and model fusion performance.
\end{eabstract}
% 英文关键词，请使用英文逗号分隔，关键词内可以空格：
\ekeywords{Deep Neural Networks; Efficient Artificial Intelligence; Transfer Learning; Large Language Models}

% 封面和摘要配置完成