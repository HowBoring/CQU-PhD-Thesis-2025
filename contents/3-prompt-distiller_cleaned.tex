\documentclass[../main.tex]{subfiles}
\graphicspath{{../figures/}}
\usepackage[linesnumbered,ruled,vlined,resetcount,algochapter]{algorithm2e}

\begin{document}

\mychapter{基于双教师对比学习的少样本知识蒸馏方法研究}
\label{sec:ch3-dual-contrastive-distillation-for-few-shot-prompts}

本章聚焦于深度模型数据于知识获取环节的高效构建挑战。针对少样本场景下，知识蒸馏存在的迁移不稳定与过拟合问题，提出 Prompt-Distiller 方法，通过双教师协同蒸馏与基于对比学习的中间层知识对齐，有效提升提示微调模型在少样本条件下的蒸馏效果与泛化能力。该方法为后续章节的小模型高效构建提供了坚实的数据与知识基础支撑。本章首先介绍了知识蒸馏在少样本场景下面临的挑战，随后详细阐述了 Prompt-Distiller 方法的设计思路与实现细节，最后通过大量实验验证了该方法的有效性及实用价值。

\mysection{引言}
\label{sec:ch3-1-introduction}

深度学习模型的成功在很大程度上依赖于大量高质量的标注数据。通过对海量样本的学习，模型能够提取出稳定且具有泛化能力的特征表示。然而，在诸如医疗影像、工业质检、法律文本处理等现实应用中，标注数据往往稀缺且获取成本高昂。这一问题限制了深度模型在小数据环境下的性能与稳定性。为缓解数据匮乏带来的困难，知识蒸馏作为一种模型间知识迁移方法被提出。其核心思想是通过教师模型向学生模型传递知识，使学生模型在学习过程中能够借助教师的输出分布、特征表示或结构关系，获得更丰富的监督信号，从而提升其在有限数据条件下的泛化能力。

现有的知识蒸馏方法大体可以分为响应层蒸馏、特征层蒸馏以及关系蒸馏等类型。传统的响应层蒸馏由 Hinton 等人提出，学生通过最小化与教师输出概率分布之间的 Kullback-Leibler 散度，学习到教师模型的类别相似性信息\cite{distillingknowledgeneural_hinton_2015}。这种基于软标签的蒸馏简单而有效，但其传递的知识主要集中在最终输出层，忽略了教师模型在中间层次所蕴含的表征结构。为此，后续研究提出了特征层蒸馏方法，通过对齐中间层特征或注意力图（如 FitNets、Attention Transfer 等）来增强学生模型的表征能力\cite{patientknowledgedistillation_sun_2019}。此外，关系蒸馏（Relational KD）进一步从样本间相对关系的角度出发，保持教师与学生在特征空间中的几何一致性，使学生继承教师在样本间相似性结构上的知识\cite{relationalknowledgedistillation_park_2019}。在数据缺乏的情境下，这种结构信息比单一的类别分布更具稳定性。
尽管知识蒸馏在缓解数据不足方面表现出一定优势，但在少样本场景下仍面临诸多挑战。首先，教师偏差问题尤为突出，当教师模型本身对目标分布存在偏差时，学生容易被误导并放大错误，从而失去泛化能力\cite{knowledgedistillationsurvey_gou_2021}。其次，在样本数量极少的情况下，学生模型往往仅能学习到教师输出的表层知识，导致表征空间塌缩，类间区分性不足。此外，少样本蒸馏对超参数（如蒸馏温度与权重系数）异常敏感，训练过程不稳定\cite{knowledgedistillationsurvey_gou_2021}。数据扩增与生成技术虽能一定程度缓解数据匮乏，但若增广策略不当或生成样本质量欠佳，反而可能引入噪声。更进一步，在跨域与长尾分布任务中，教师知识的可迁移性与少样本类别的学习能力之间常存在矛盾，如何在知识传递与任务适应性之间取得平衡仍是难点。

为应对少样本场景下知识蒸馏所面临的监督信号不足与迁移不稳定问题，本章提出一种多教师协同与对比增强的知识蒸馏框架。核心思想在于充分挖掘可利用的外部与隐含信息，通过多视角教师知识与对比式正则约束共同丰富学生模型的学习信号，从而提升其在少样本条件下的表征判别性与迁移稳定性。不同于传统仅依赖单一教师输出概率分布的蒸馏方式，本章从多层次与多视角出发，引入两类互补教师模型：一方面，使用规模较大的预训练教师模型，提供全局语义层面的通用知识；另一方面，采用经少样本提示微调（prompt-tuning）后的教师模型，使其在目标任务上具备更强的领域适应性。二者的结合构成一种多视角后验知识体系，使学生模型既能继承通用表征，又能获得任务特定的细粒度知识。

在宏观层面，本章首先利用未标注数据建立预训练教师与学生模型之间的语义对齐机制。通过无标签样本的伪监督蒸馏，学生模型能够在不依赖额外标注的条件下逐步接近教师的语义空间。与此同时，通过提示微调后的教师模型可进一步挖掘潜藏于大规模预训练权重中的任务相关信息，这种信息经由蒸馏过程被有效转移至学生模型，实现“知识再激活”的迁移路径。该设计使得学生模型在有限样本条件下能够充分利用预训练教师的潜在知识储备，从而增强其泛化能力。
在微观实现层面，本章将表征对齐与对比学习相结合，构建层次化蒸馏信号体系。在传统基于输出响应（response-based）的蒸馏损失之外，引入中间层表征对齐，以保证学生在特征空间中的结构一致性。进一步地，在对齐信号的基础上，以教师模型的表征为锚点，构造基于 InfoNCE 的对比学习目标，通过最小化同一语义样本在教师与学生表征间的距离，并最大化不同语义样本间的差异，从而在学生的表示空间中同时强化类内聚合与类间分离。这种“蒸馏–对比”融合机制既继承了教师模型的高层语义知识，又通过对比约束保持了学生特征的判别性与稳定性，有效缓解了少样本蒸馏中常见的表征塌缩与过拟合问题。

综上所述，本章旨在针对少样本场景下知识蒸馏的监督信号稀缺与迁移不稳定问题，探索如何通过多源知识融合与结构化约束实现高效稳健的知识传递。提出一种多教师协同与对比增强的知识蒸馏框架，在有限数据条件下充分挖掘可利用的隐含信息与无标签样本语义结构，以提升学生模型的表征能力与泛化性能。本章的主要贡献概括如下：
\begin{enumerate}[label={（\arabic*）},labelindent=2\ccwd,leftmargin=!]
	\item 提出从多教师互补视角构建少样本蒸馏机制，联合使用大规模预训练教师模型与经提示微调后的任务特定教师模型，实现从通用语义到特定语义的多层次知识迁移。该设计有效丰富了学生模型的监督来源，显著提升了知识蒸馏在数据受限场景下的可迁移性与稳定性。
	\item 提出一种融合表征对齐与对比学习的蒸馏方法。在传统响应层蒸馏信号的基础上，引入中间层表征对齐与基于 InfoNCE 的对比目标，通过以教师表征为锚点的特征约束，强化学生模型的类内聚合与类间分离，从而防止表征塌缩并提升特征判别性。
	\item 设计结合无标签语义对齐与提示学习的蒸馏流程，使学生模型能够在缺乏人工标注的条件下，通过教师知识与自监督对比约束实现语义空间的高效对齐。该机制充分释放了预训练教师的潜在知识，提升了单位数据利用效率。
	\item 在多个少样本任务上，所提出方法在准确率上均显著优于主流知识蒸馏基线，验证了所提出框架在数据匮乏场景下的有效性与普适性。
\end{enumerate}

总体而言，本章研究为少样本知识蒸馏提供了一种可行的多教师协同与对比增强路径，也为在低资源学习条件下实现更稳健的知识迁移提供了新的思路与理论参考。

% 本章余下部分组织如下：首先在第 \ref{sec:ch3-2-problem-definition} 节明确本章所解决的问题及相关场景设定，给出形式化定义；第 \ref{sec:ch3-3-prompt-distiller-framework} 节详细阐述 Prompt-Distiller 算法的理论框架与模型设计，包括双教师蒸馏和基于对比学习的中间层知识转移机制，并以公式推导方式给出各部分损失函数及整体优化目标；随后在第 \ref{sec:ch3-4-prompt-distiller-implementation} 节给出算法的实现细节与伪代码描述；第 \ref{sec:ch3-5-experiments-and-analysis} 节设计广泛的对比实验与消融分析，验证本章方法在多个少样本任务上的有效性；最后第 \ref{sec:ch3-6-chapter-summary} 节对本章工作进行总结，并说明本章研究结果对于后续章节系统实现的支撑作用。通过本章的研究，将为下一章的小模型部署系统实现提供坚实的理论和方法基础。

\mysection{问题定义}
\label{sec:ch3-2-problem-definition}
本章探讨少样本提示学习模型的知识蒸馏问题。场景设定如下：假定存在一个规模庞大的预训练语言模型作为教师模型 $\Theta_T$，其经过少量标注数据的提示微调后能够在目标任务上取得较高性能；同时希望训练一个参数规模更小的学生模型 $\Theta_S$，使其在该任务上的性能接近教师模型但推理效率更高。为了限定问题范围，考虑典型的 $N$-分类任务（例如情感分类、自然语言推理等）。少样本设定下，训练集中每一类别仅有 $K$ 个标注样本，整个训练集记为 $X={(x_i, y_i)}$，其中 $x_i$ 表示第 $i$ 个输入文本，$y_i \in \mathcal{Y}$ 为其对应的类别标签，$\mathcal{Y}$ 为标签集合且 $|\mathcal{Y}| = N$。因此，标注训练集规模仅为 $N \times K$。通常 $K$ 很小，例如 $K=16$ 或 $32$（典型 few-shot 设置）。此外，假定还可获取一定量的无标注语料 $\tilde{X}={x_j'}$，其规模约为标注集的 $n$ 倍（即 $|\tilde{X}| = n\cdot |X|$），这些语料与任务领域大致相关但未经过人工标注。无标注数据可用于辅助蒸馏训练，为学生模型提供额外的自监督信号。需要强调的是，本章方法并不依赖 $\tilde{X}$ 的具体内容或来源，它可以来自于任务相关的公开数据或在线爬取的文本。

在模型训练流程上，教师模型$\Theta_T$首先通过提示微调在少样本集$X$上进行训练，使其适应目标任务。本章采用 PET 方法\cite{exploitingclozequestions_schick_2021} 作为提示微调手段，即利用若干人工设计的提示模板将原始输入$x$转换为带有 [MASK] 占位符的提示输入，然后引导模型在遮蔽位置预测一个预先定义的标签词 $l(y)$ 作为分类结果。例如，对于情感分类任务，标签集合 $\mathcal{Y}$ 以及可选用的标签词集合可能为：
\begin{gather}
	\mathcal{Y}=\{\text{正面}, \text{负面}\} \\
	l(\text{正面})=\text{“great”},\quad l(\text{负面})=\text{“terrible”}
\end{gather}
通过在样本 $x_i$ 对应的提示模板中填入 [MASK]，让 PLM 预测该位置上的输出词语，即可完成分类。用 $s_{\Theta}(l(y)\mid x_i)$ 表示模型 $\Theta$ 在输入 $x_i$（经提示包装后）条件下，输出词为标签词 $l(y)$ 的打分。教师模型 $\Theta_T$ 在提示微调完成后，其在任务上的预测概率定义如下：对于任意输入文本 $x_i$ 和类别 $y\in\mathcal{Y}$，教师模型预测 $x_i$ 属于类别 $y$ 的概率为：
\begin{equation}
	\label{eq:teacher-prob}
	p_{T}(y \mid x_i) = \frac{\exp\big(s_{\Theta_T}(l(y)\mid x_i)\big)}{\displaystyle\sum_{y' \in \mathcal{Y}} \exp\big(s_{\Theta_T}(l(y')\mid x_i)\big)} .
\end{equation}
其中$\exp(\cdot)$表示指数函数，分母部分遍历$\mathcal{Y}$中所有可能类别对应的标签词。\eqref{eq:teacher-prob} 与 Softmax 函数形式一致，表示教师模型$\Theta_T$在看到输入$x_i$时，输出属于类别$y$的相对概率。与传统微调的分类头不同，这里的概率分布是通过MLM头在词汇表上针对标签词计算得到的。利用这样的概率输出，可以进一步定义教师模型在$x_i$上的交叉熵损失或其他指标。本章后续将主要利用$p_T(y\mid x_i)$作为教师模型输出的软标签指导学生模型的训练。

为衡量学生模型的性能，需要定义学生模型 $\Theta_S$ 的预测输出形式。类似地，在提示模板下，学生模型预测 $x_i$ 属于类别 $y$ 的概率可表示为：
\begin{equation}
	\label{eq:student-prob}
	p_{S}(y \mid x_i) = \frac{\exp\big(s_{\Theta_S}(l(y)\mid x_i)\big)}{\displaystyle\sum_{y' \in \mathcal{Y}} \exp\big(s_{\Theta_S}(l(y')\mid x_i)\big)} .
\end{equation}

有了上述定义，进一步刻画知识蒸馏的目标：希望通过训练使得对于任意输入 $x_i$，学生模型的输出分布 $p_S(\cdot \mid x_i)$ 尽可能贴近教师模型的输出分布 $p_T(\cdot \mid x_i)$，同时学生模型也正确地拟合样本的真实标签。在少样本条件下，实现这一目标需要克服教师—学生容量差距大、训练样本少导致的过拟合等难题。

\mysection{Prompt-Distiller算法框架}
\label{sec:ch3-3-prompt-distiller-framework}
针对上述问题定义，本节提出Prompt-Distiller算法的总体框架和原理。如图\ref{fig:PromptDistillerFramework}所示，Prompt-Distiller包含两个核心模块：其一是来自双教师模型的监督知识蒸馏，即学生模型同时从提示微调教师$\Theta_T$和原始预训练教师$\Theta_{T'}$两方面获取知识信号；其二是基于探针的对比学习式中间层知识转移，即通过比较教师与学生模型中间表示对下游任务的输出差异，促进学生学习教师模型更深层次的判别能力。下面分别对这两个模块展开论述。

\begin{figure}[t!]
	\centering
	\includegraphics[width=0.95\textwidth]{Prompt-Distiller/framework.pdf}
	\bicaption[Prompt-Distiller框架与提示增强数据示例]{(a) Prompt-Distiller框架示意图：学生模型在接受提示微调教师模型（左侧）和原始预训练教师模型（右侧）联合指导的过程中，一方面通过输出层预测分布的蒸馏进行学习，另一方面利用探针分类器对比中间层表示差异实现对比学习；(b) 提示增强数据实例展示，其中下划线部分标识了提示模板中的输入与标签词片段。示例展示了如何通过精心设计的提示模板将输入与标签嵌入到上下文中，以增强模型的学习效果。}[Illustration of Prompt-Distiller Framework and Prompt-Augmented Data]{(a) Schematic illustration of the Prompt-Distiller framework: the student model is trained with joint guidance from both a prompt-tuned teacher (left side) and the original pretrained teacher (right side), involving output-level distillation of prediction distributions as well as contrastive learning by comparing intermediate-layer representations through probe classifiers; (b) Example of prompt-augmented data, where underlined segments denote input and label tokens within the prompt template, showcasing how inputs and labels are embedded into the context via carefully designed prompt templates to enhance learning.}
	\label{fig:PromptDistillerFramework}
\end{figure}

\mysubsection{双教师知识蒸馏策略}
\label{sec:ch3-3-1-dual-teacher-distillation-strategy}

\mysubsubsection{从提示微调教师获取任务特定知识}

提示微调后的教师模型$\Theta_T$在少样本任务上性能较高，反映了与任务密切相关的知识和模式。为了使学生模型学习这一任务特定知识，首先考虑最常用的基于输出层概率分布对齐的蒸馏方法。具体来说，对于每一个标注训练样本$(x_i, y_i) \in X$，令$\vec{y}_i \in {0,1}^N$表示$y_i$对应的独热标签向量，维度为$N$（即正确类别位置为1，其余为0）。学生模型在$x_i$上的预测分布由 \eqref{eq:student-prob}给出，表示为向量形式$p_S(y\mid x_i)$。将该分布与$\vec{y}_i$计算交叉熵可得学生模型的监督分类损失：
% \begin{equation}
% 	\label{eq:mlm-loss}
% 	\mathcal{L}_{\text{MLM}}(X) = -\frac{1}{|X|} \sum_{(x_i, y_i)\in X} \sum_{y \in \mathcal{Y}} \vec{y}_i(y)\log p_S(y \mid x_i) .
% \end{equation}
\begin{equation}
  \label{eq:mlm-loss}
  \mathcal{L}_{\text{MLM}}(X)
  = -\frac{1}{|X|}
  \sum_{\substack{(x_i, y_i) \\ \in X}}
  \sum_{y \in \mathcal{Y}}
  \vec{y}_i(y)\log p_S(y \mid x_i) .
\end{equation}
其中$\vec{y}_i(y)$表示独热向量在类别$y$对应位置的取值。需要注意的是，这里的MLM指代模型基于遮掩语言模型头进行预测输出。\eqref{eq:mlm-loss}即为常规的交叉熵分类损失，保证学生模型能正确拟合少量训练样本的真实标签。

在此基础上，引入提示微调教师模型$\Theta_T$输出的软标签信息，作为蒸馏学习的补充监督信号。教师模型$\Theta_T$对于样本$x_i$输出的概率分布已由\eqref{eq:teacher-prob}定义为$p_T(y\mid x_i)$，它代表了教师模型对各类别的信心。虽然$x_i$只有一个真实标签，但$p_T(y\mid x_i)$刻画了各类别之间的相对分布，蕴含了教师模型对错误类别的认知和区分能力，即所谓暗知识。蒸馏的思想是希望学生模型也能掌握这种精细的区分模式，而不仅仅是输出正确类别。因此，可以让学生模型的输出$p_S(y\mid x_i)$去拟合教师模型的输出分布$p_T(y\mid x_i)$。这可以通过计算两个概率分布之间的交叉熵来实现，即定义有监督蒸馏损失为：
\begin{equation}
	\label{eq:kd-loss-supervised}
	\mathcal{L}_{\text{KD}}(X) = -\frac{1}{|X|}\sum_{\substack{(x_i, y_i) \\ \in X}} \sum_{y \in \mathcal{Y}} p_T(y \mid x_i) \log p_S(y \mid x_i) .
\end{equation}

上述损失会促使学生模型的输出尽量接近教师模型，在标注样本上保持与教师一致的行为。需要说明的是，有时引入温度系数$\alpha$可使教师输出分布更平滑，从而更好地指导学生。做法是将教师输出的Softmax温度调高，使其概率更接近于均匀分布，然后再用于蒸馏。公式上相当于将$p_T(y\mid x_i)$取$\text{Softmax}(z/\alpha)$，其中$z$为未归一化的logit向量，$\alpha > 1$时产生平滑作用。在本章实验中，将根据验证集表现选取适当的温度系数，但公式推导中暂不显式表示$\alpha$。

\mysubsubsection{从预训练教师获取通用知识}

虽然提示微调教师提供了宝贵的任务特定知识，但由于少样本学习的限制，其本身可能存在一定的过拟合风险：模型可能记忆了少量训练样本的特点而未能学到一般性规律。特别地，如果$K$极小，教师模型输出的分布$p_T(y\mid x)$在训练样本上可能过于自信（趋近于独热），从而为学生提供的暗知识有限。此外，学生模型参数容量远小于教师模型，很难仅凭少量样本就捕获到教师模型蕴含的大量背景知识。因此，考虑利用教师模型在未经过微调时所具有的通用预训练知识来辅助蒸馏。直观上，原始预训练的教师模型$\Theta_{T'}$在大规模语料上已经学习到广泛的语言表达和常识知识，尽管未针对当前任务，但其对语言现象的理解对学生模型仍然是极为有价值的参考。

具体做法是：让学生模型在无标注数据$\tilde{X}$上去拟合预训练教师$\Theta_{T'}$的输出分布。这类似于蒸馏预训练知识，因为$\Theta_{T'}$的输出反映了模型的预训练记忆，如词汇共现、句法语义等信息。对于每一个无标注样本$x_j' \in \tilde{X}$，通过教师模型$\Theta_{T'}$计算其输出分布$p_{T'}(y\mid x_j')$，其中$l(y)$仍沿用下游任务定义的标签词（此时$\Theta_{T'}$实际上是在零样本条件下对任务进行推断）。虽然$x_j'$没有人工标签，但$\Theta_{T'}$可以给出其属于各类别的一个猜测概率分布。希望学生模型也能给出类似的分布，从而习得$\Theta_{T'}$在预训练中积累的知识。因此，定义无监督蒸馏损失为：
\begin{equation}
	\label{eq:kd-loss-unsupervised}
	\mathcal{L}_{\text{KD}}(\tilde{X}) = -\frac{1}{|\tilde{X}|}\sum_{x_j' \in \tilde{X}} \sum_{y \in \mathcal{Y}} p_{T'}(y \mid x_j') \log p_S(y \mid x_j') .
\end{equation}

该损失通过无监督数据让学生模仿预训练教师的行为，不需要任何人工标注。尤其在$|\tilde{X}|$远大于$|X|$的情况下，学生模型能够接触更多样的语言现象，缓解了仅凭少数样本学习导致的过拟合。同时，由于$\tilde{X}$覆盖了多样的内容，学生模型通过$\mathcal{L}_{\text{KD}}(\tilde{X})$可学习到更通用的特征表示和对输入的鲁棒理解，这将有助于其提升对未见样本的泛化性能。需要注意的是，在计算$p_{T'}(y \mid x_j')$时，$\Theta_{T'}$仍然采用与$\Theta_T$相同的提示模板和标签词来进行预测，尽管它未经微调，其输出可以被视为对下游任务的零样本预测。这种利用预训练模型进行辅助监督的思想，与自蒸馏或“born-again network”等概念有一定相似之处，但区别在于这里$\Theta_{T'}$并非学生模型的历史版本，而是教师模型本身在未调优时的形态。两种教师模型提供的互补知识可以从不同侧面提升学生模型。

\mysubsection{基于对比学习的中间层知识转移}
\label{sec:ch3-3-2-contrastive-middle-layer-transfer}
上述双教师蒸馏策略主要在输出层利用概率分布提供指导。事实上，深度模型内部各层的中间表示也包含丰富的知识，是知识蒸馏的重要来源。例如，教师模型隐藏层中的特征向量承载了对输入文本在不同抽象级别上的表示，直接对齐这些表示可以让学生模型受益。然而，在该场景下，教师为大型模型而学生为小型模型，二者的表示空间和容量相差悬殊。如果强行最小化对应层隐藏向量之间的欧氏距离，可能会由于小模型的表示能力不足而适得其反，甚至拖累训练过程。为此，未选择直接对齐隐藏层向量，而是采用一种探针方法来间接比较教师和学生模型的中间层知识。在训练开始前，冻结教师模型$\Theta_T$和学生模型$\Theta_S$当前的参数（学生模型此时尚未训练），为它们各自的每一层隐藏状态训练一个轻量的MLM分类器，即探针，来预测下游任务的标签。这类似于在不同层上探测模型对任务的判别能力。具体而言，对于教师模型的每一个Transformer编码层$j$（$j=1,2,\dots,N_T-1$，不含最后输出层），训练一个探针分类器$P_T^{(j)}$，它接收该层输出的隐藏表示（通常指[MASK]位置对应的向量）作为输入，输出一个$N$维向量，经过Softmax转化为对$N$个类别的概率分布。训练时，使用少样本集$X$，将探针$P_T^{(j)}$的输出概率与真实标签$\vec{y}_i$计算交叉熵，调整探针参数使其能够利用第$j$层隐藏状态来预测正确类别\footnote{为了保证探针训练的可靠性，可对$X$进行$k$折交叉验证训练探针，以免过拟合。由于$X$极小，此处探针训练只作为蒸馏的辅助步骤，不会引入明显的过拟合。实际应用中也可使用少量额外标注数据训练探针，此处从简处理。}。经过此过程，可以得到教师模型从第1层到第$N_T-1$层的一组探针${P_T^{(1)}, P_T^{(2)}, \dots, P_T^{(N_T-1)}}$。同理，为学生模型$\Theta_S$的每一编码层$k$（$k=1,2,\dots,N_S-1$）训练探针$P_S^{(k)}$。由于学生模型层数与教师不同，不强调层与层的严格对应关系，而是利用所有探针的综合判别能力来设计中间层知识转移策略。

当探针训练完成后，教师模型第$j$层探针$P_T^{(j)}$即可作为观察该层知识的窗口。给定任意输入$x$，将其输出的$N$维概率向量记作$\mathbf{p}_T^{(j)}(x)\in\mathbb{R}^N$，表示基于教师模型第$j$层表示对$x$的类别预测分布。同理，学生模型第$k$层探针$P_S^{(k)}$的输出记作$\mathbf{p}_S^{(k)}(x) \in\mathbb{R}^N$。这些概率向量反映了不同层次上模型对各类别的信心，据此刻画教师与学生在整体中间层输出上的相似度。为便于比较不同层的探针输出，定义教师-学生匹配得分函数$f_{T,S}(x)$如下：
\begin{equation}
	\label{eq:f-match}
	f_{T,S}(x) = \exp\left[\frac{1}{(N_T-1)(N_S-1)} \sum_{j=1}^{N_T-1}\sum_{k=1}^{N_S-1} \mathbf{p}_T^{(j)}(x) \cdot \mathbf{p}_S^{(k)}(x)\right] .
\end{equation}
其中“$\cdot$”表示两个概率向量的点积，相当于基于类别分布的相似度度量。对教师模型各层探针输出与学生模型各层探针输出分别两两计算点积，并取平均，再取指数。之所以取指数，是为了保证匹配得分为正值且差异适中，便于后续做比值运算。可见，如果教师和学生模型各层探针输出的类别分布非常相似，那么每对$(j,k)$的点积都会较高，平均后指数结果$f_{T,S}(x)$将取得较大值；相反，如果学生模型的中间表示与教师相差较大，各层探针输出分布差异明显，则$f_{T,S}(x)$趋近于较小的正数。通过$f_{T,S}(x)$，可以进一步设计一种对比学习思路：令给定输入$x$作为正样本，将与$x$类别不同的其它输入作为负样本，迫使学生模型提高正样本的匹配得分、降低与负样本之间的匹配得分，以此方式来对齐学生模型与教师模型的中间层知识分布。

具体而言，对于每个标注样本$(x_i, y_i)\in X$，构造一个负样本集合$\mathcal{N}(x_i)$，其中包含若干与$x_i$类别不同的训练样本（即标签$y \neq y_i$）。负样本可以随机选取，也可以包括训练集中所有类别不为$y_i$的样本。在少样本条件下，$|\mathcal{N}(x_i)|$不必太大即可取得良好效果。本章实验中，采用简单的随机采样策略获取每个正样本的负样本集合。现在，定义有监督对比蒸馏损失为：
\begin{equation}
	\label{eq:cpd-loss-sup}
	\mathcal{L}_{\text{CPD}}(X) = -\frac{1}{|X|}\hspace{6pt} \sum_{\mathclap{(x_i, y_i)\in X}} \log \frac{f_{T,S}(x_i)}{\sum_{x' \in \mathcal{N}(x_i)} f_{T,S}(x')} .
\end{equation}
可见，$\mathcal{L}_{\text{CPD}}(X)$的形式与信息检索中的对比学习损失类似：将与$x_i$同类的教师-学生匹配作为正对，把$x_i$与若干异类样本的匹配作为负对，通过最大化$f_{T,S}(x_i)$与负对比之和的比值，来提高正对的匹配度。这样优化后，学生模型在自己的各层探针上的行为将更接近教师模型，即对于同类样本输出相似的分布、对于异类样本则产生可区分的差异。换言之，学生模型学习到了教师模型隐藏层中更高阶的判别信息，而不仅仅是底层特征的数值匹配。

类似地，也利用无标注数据$\tilde{X}$计算对比损失，从而将预训练教师$\Theta_{T'}$的中间层知识引入学生模型。对于每个无标注样本$x_j'\in \tilde{X}$，由于缺乏真实标签，无法直接按类别选择负样本集$\mathcal{N}(x_j')$。一种折衷办法是：先用预训练教师模型$\Theta_{T'}$对$x_j'$进行一次零样本预测，取其最可能的类别$\hat{y} = \arg\max_{y} p_{T'}(y\mid x_j')$作为$x_j'$的一个伪标签。然后，从$\tilde{X}$中随机抽取若干样本，这些样本经$\Theta_{T'}$预测的类别与$\hat{y}$不同，作为$x_j'$的负例集合$\mathcal{N}(x_j')$。虽然这种伪标签可能存在一定误差，但在大规模无监督数据中，完全相同的模型输出类别一般不会占绝大多数，因此随机采样不同类别输出的样本作为负例仍具有区分信息。这样，定义无监督对比蒸馏损失为：
\begin{equation}
	\label{eq:cpd-loss-unsup}
	\mathcal{L}_{\text{CPD}}(\tilde{X}) = -\frac{1}{|\tilde{X}|} \sum_{x_j' \in \tilde{X}} \log \frac{f_{T',S}(x_j')}{\displaystyle\sum_{x' \in \mathcal{N}(x_j')} f_{T',S}(x')} ,
\end{equation}
其中$f_{T',S}(x)$的定义与\eqref{eq:f-match}相同，只不过将教师模型换为$\Theta_{T'}$。直观上，$\mathcal{L}_{\text{CPD}}(\tilde{X})$促使学生模型在无标注数据上也能复现预训练教师模型各层对不同输入的区分模式，让学生的中间表示蕴含更多通用语言知识。这对学生模型捕获预训练教师的精细特征、弥补其容量不足具有积极作用。

\mysubsection{损失函数汇总与优化目标}
\label{sec:ch3-3-3-loss-overview-and-objectives}
结合上一小节的讨论，已分别得到学生模型的监督分类损失$\mathcal{L}_{\text{MLM}}(X)$（\eqref{eq:mlm-loss}）、有监督蒸馏损失$\mathcal{L}_{\text{KD}}(X)$（\eqref{eq:kd-loss-supervised}）、无监督蒸馏损失$\mathcal{L}_{\text{KD}}(\tilde{X})$（\eqref{eq:kd-loss-unsupervised}）、有监督对比蒸馏损失$\mathcal{L}_{\text{CPD}}(X)$（\eqref{eq:cpd-loss-sup}）和无监督对比蒸馏损失$\mathcal{L}_{\text{CPD}}(\tilde{X})$（\eqref{eq:cpd-loss-unsup}）。Prompt-Distiller算法的整体训练目标就是将上述各项损失线性加权组合后最小化。记$\lambda_1,\lambda_2$为权衡系数，则总损失函数$\mathcal{L}_{\text{total}}$定义如下：
\begin{equation}
	\label{eq:total-loss}
	\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{MLM}}(X) + \lambda_1 \Big( \mathcal{L}_{\text{KD}}(X) + \mathcal{L}_{\text{KD}}(\tilde{X}) \Big) + \lambda_2 \Big( \mathcal{L}_{\text{CPD}}(X) + \mathcal{L}_{\text{CPD}}(\tilde{X}) \Big).
\end{equation}

其中$\lambda_1$控制双教师输出蒸馏损失在总损失中的占比，$\lambda_2$控制对比蒸馏损失的占比。通过调整这两个超参数，可以在不同任务和数据规模下平衡监督学习信号与蒸馏信号、输出层知识与中间层知识，从而得到最佳的综合效果。整个Prompt-Distiller训练过程可描述为：在初始化学生模型参数$\Theta_S$为与教师模型结构对应的预训练权重（例如教师为RoBERTa-large则学生为某种减小层数和宽度的BERT结构，初始化自BERT的预训练参数），并完成上述探针训练后，使用随机梯度下降（如Adam优化器）在总损失$\mathcal{L}_{\text{total}}$下更新$\Theta_S$，直到验证集性能不再提升为止。在训练中，教师模型参数$\Theta_T$和$\Theta_{T'}$保持冻结不变。最终，得到的$\Theta_S^*$即为蒸馏后的学生模型。在推理部署时，仅需保留学生模型$\Theta_S^*$，而无需教师模型的参与，从而达到模型压缩和加速的目的。

需要强调的是，本章方法的关键创新在于充分利用多层次的知识信号：既包括来自输出层的软标签知识，也包括来自中间层的对比信息，且同时考虑了任务特定和通用两方面的知识来源。这种多层次知识迁移的思想正呼应了本论文的主题。在上一章提出的总体框架基础上，Prompt-Distiller实现了从预训练知识层（通过$\Theta_{T'}$）、微调任务知识层（通过$\Theta_T$）以及表示知识层（通过中间层探针）全面地向学生模型传递信息。下一节将进一步给出Prompt-Distiller算法的实现步骤伪代码，使上述流程更加清晰可复现。

\mysection{Prompt-Distiller算法实现}
\label{sec:ch3-4-prompt-distiller-implementation}
% 为了便于理解和复现，本节将Prompt-Distiller方法的训练流程整理为算法步骤。如算法\ref{alg:prompt-distiller}所示，整个训练可分为以下主要阶段：

% \begin{algorithm}[t]
% 	\small
% 	\caption{Prompt-Distiller 少样本知识蒸馏算法}
% 	\label{alg:prompt-distiller}
% 	\KwIn{$T$（教师, 大型PLM），$T'$（预训练教师），$S$（学生, 小型PLM），有标注$X$，无标注$\tilde X$，温度$\alpha$，权重$\lambda_1,\lambda_2$，负采样数$n$，批大小$B$，最大学习轮次$N_{\mathrm{epoch}}$}
% 	\KwOut{$\Theta_S^*$}

% 	\BlankLine

% 	\tcc{提示微调教师}

% 	$\Theta_T \leftarrow \arg\min_{\Theta}\ \mathcal{L}_{\text{prompt}}(T_\Theta; X)$\; %（如PET）

% 	\BlankLine

% 	\tcc{探针训练（冻结 $\Theta_T,\Theta_S$）}
% 	对 $j = 1\ldots N_T-1$ 与 $k = 1\ldots N_S-1$，最小化
% 	$\sum_{(x,y)\in X} \mathrm{CE}\!\big(P_T^{(j)}(h_T^{(j)}(x)),y\big)
% 		+ \mathrm{CE}\!\big(P_S^{(k)}(h_S^{(k)}(x)),y\big)$；保存 $\{P_T^{(j)}\},\{P_S^{(k)}\}$\;

% 	\BlankLine

% 	\tcc{蒸馏训练（解冻 $\Theta_S$，冻结 $\Theta_T,\Theta_{T'}$）}

% 	\For{$e=1$ \KwTo $N_{\mathrm{epoch}}$}{
% 		将 $X,\tilde X$ 划分为若干对小批 $(B_X,B_{\tilde X})$（每批大小 $B$）\;
% 		\For{每个批 $(B_X,B_{\tilde X})$}{
% 			\tcp{监督/无监督KD与对比蒸馏}
% 			$\mathcal{L}_{\text{MLM}} \gets \mathcal{L}_{\text{MLM}}(B_X)$（\eqref{eq:mlm-loss}）\;
% 			$\mathcal{L}_{\text{KD}}^{\text{sup}} \gets \mathcal{L}_{\text{KD}}(B_X;\ T,\alpha)$（\eqref{eq:kd-loss-supervised}）\;
% 			$\mathcal{L}_{\text{KD}}^{\text{unsup}} \gets \mathcal{L}_{\text{KD}}(B_{\tilde X};\ T',\alpha)$（\eqref{eq:kd-loss-unsupervised}）\;

% 			为 $B_X$ 中每个正样本 $(x_i,y_i)$ 构造 $|\mathcal{N}(x_i)|\!=\!n$ 的负集（方法见第~\ref{sec:ch3-3-2-contrastive-middle-layer-transfer} 节），得
% 			$\mathcal{L}_{\text{CPD}}^{\text{sup}} \gets \mathcal{L}_{\text{CPD}}(B_X,\{\mathcal{N}(x_i)\})$（\eqref{eq:cpd-loss-sup}）\;

% 			为 $B_{\tilde X}$ 中每个样本 $x_j'$ 基于 $T'$ 预测构造 $|\mathcal{N}(x_j')|\!=\!n$ 的负集，得
% 			$\mathcal{L}_{\text{CPD}}^{\text{unsup}} \gets \mathcal{L}_{\text{CPD}}(B_{\tilde X},\{\mathcal{N}(x_j')\})$（\eqref{eq:cpd-loss-unsup}）\;

% 			\tcp{加权汇总与更新}
% 			$\displaystyle
% 				\mathcal{L}_{\text{batch}}
% 				\!=\!
% 				\mathcal{L}_{\text{MLM}}
% 				+ \lambda_1\!\left(\mathcal{L}_{\text{KD}}^{\text{sup}}+\mathcal{L}_{\text{KD}}^{\text{unsup}}\right)
% 				+ \lambda_2\!\left(\mathcal{L}_{\text{CPD}}^{\text{sup}}+\mathcal{L}_{\text{CPD}}^{\text{unsup}}\right)$\;

% 			$\Theta_S \leftarrow \Theta_S - \eta\,\nabla_{\Theta_S}\mathcal{L}_{\text{batch}}$\;
% 		}
% 		若验证集满足提前终止准则则 \textbf{break}\;
% 	}
% 	\Return 训练后的学生参数 $\Theta_S^*$\;
% \end{algorithm}

\textbf{步骤1：提示微调教师模型。} 在开始知识蒸馏之前，需要保证教师模型已具备较好的下游任务性能。为此，首先对教师模型$T$进行提示微调。在算法\ref{alg:prompt-distiller}中，假定教师模型已经过充分的预训练（例如RoBERTa-large模型），然后利用少样本集$X$按照提示学习方法进行微调，使其参数更新为$\Theta_T$。这一步骤可以调用现有的提示微调实现，如PET、LM-BFF等。在微调过程中，可能需要验证集调参以获得最佳效果。微调完成后，$\Theta_T$将固定下来，不再改变。微调所得模型在$X$上的高精度为后续蒸馏提供了前提。需要注意，如果教师模型本身在$X$上过拟合明显或者性能不佳，蒸馏给学生模型的知识也会受到影响，因此教师模型应充分利用已有的提示技术提升表现。

\textbf{步骤2：训练探针观察中间层。} 教师模型和学生模型初始化（学生为小模型，参数 $\Theta_S$ 可直接采用对应架构的预训练权重初始化）后，分别为其中间层训练探针分类器 $P_T^{(j)}$ 和 $P_S^{(k)}$。该步骤的目的是获取教师模型和学生模型在各自不同层上的行为快照。由于教师模型容量较大，通常即使未经下游任务微调，其低层也蕴含一定可用于判别任务的信息；学生模型由于较小，初始预训练特征较弱，但仍然为其配备探针以记录初始状态下各层对任务的预测能力。在训练探针时，将模型参数固定，仅优化探针的参数，以确保探针捕捉的是当前模型层给定的特征分布下所能达到的任务性能。探针的训练可以使用简单的梯度下降方法，由于 $X$ 很小，可快速完成。对于教师模型，探针 $P_T^{(j)}$ 试图学习函数 $\tilde{y}=P_T^{(j)}(h_T^{(j)})$，其中 $h_T^{(j)}$ 是教师模型第 $j$ 层输出的 [MASK] 向量。损失为 $\text{CE}(\tilde{y}, y)$ 并对探针参数求导。同理训练学生模型的探针 $P_S^{(k)}$。最终，获得两组探针参数，但需要强调的是这些探针仅用于计算对比损失，并不参与学生模型最终的部署。

\textbf{步骤3：初始化蒸馏训练。} 探针训练完毕后，即可着手知识蒸馏的主过程。首先解冻学生模型 $\Theta_S$ 的参数，允许其在后续迭代中更新。与此同时，教师模型 $\Theta_T$ 和 $\Theta_{T'}$ 在整个蒸馏过程中保持冻结，充当固定的知识提供者。设定一个训练轮数上限或提前停止条件（例如验证集精度连续若干次迭代未提升）。之后进入批量迭代训练。

\textbf{步骤4：逐批次优化学生模型。} 在每一训练轮次，对 $X$ 和 $\tilde{X}$ 分别进行批处理采样（例如 $B=4$ 或 8），并对每一个批次同时计算来自监督数据和无监督数据的各类损失，累加为总损失 $\mathcal{L}_{\text{batch}}$，然后对学生模型执行梯度更新。值得注意的是，在同一批次内结合计算了 $X$ 和 $\tilde{X}$ 上的损失，这种做法相当于每个参数更新步同时考虑标注和非标注样本的贡献，使学生模型的梯度信息更加丰富稳定。如果实际内存或实现限制，也可以采用交替更新的方式（例如一次迭代仅使用 $X$ 或仅使用 $\tilde{X}$ 计算损失，交替进行），但需要适当调整学习率以平衡两部分的影响。首先计算学生模型在监督批次 $B_X$ 上的分类损失 $\mathcal{L}_{\text{MLM}}(B_X)$，保障基本的监督学习方向；然后计算提示微调教师提供的蒸馏损失 $\mathcal{L}_{\text{KD}}(B_X)$，指导学生靠拢教师输出；接着计算预训练教师提供的蒸馏损失 $\mathcal{L}_{\text{KD}}(B_{\tilde{X}})$，将无监督知识融入学生；最后计算有、无监督的对比损失 $\mathcal{L}_{\text{CPD}}(B_X)$ 和 $\mathcal{L}_{\text{CPD}}(B_{\tilde{X}})$，以对齐中间层的分布。负样本集 $\mathcal{N}(x)$ 的选取按照第 \ref{sec:ch3-3-2-contrastive-middle-layer-transfer} 节描述进行，其中负样本数的超参数由 $n$ 控制（例如 $n=1$ 表示每个正样本取 1 个负样本）。在将各损失加权求和时，可根据验证集或先验经验设置 $\lambda_1,\lambda_2$，通常 $\lambda_1,\lambda_2$ 取值在 $0.1\sim1$ 范围内。将 $\mathcal{L}_{\text{batch}}$ 关于 $\Theta_S$ 的梯度用于更新学生模型，即完成一次训练步。随着训练的进行，学生模型会逐步从教师模型汲取知识，验证集性能也应随之提高。训练过程中若检测到验证性能连续下降，可提前停止以防止过拟合。

\textbf{步骤5：输出学生模型。} 当训练结束后，得到更新后的学生模型参数 $\Theta_S^*$。这即是知识蒸馏后的最终模型，可用于后续章节的系统部署。由于学生模型通常比教师模型小一个量级以上，其推理速度和资源占用显著降低，而性能与教师模型相近。本章稍后将通过实验证明 Prompt-Distiller 训练得到的学生模型在多个任务上均取得了令人满意的结果。

通过上述步骤，Prompt-Distiller 算法将一个提示微调的庞大教师模型成功压缩为小型模型，并充分利用了双教师输出和中间对比信号来提高蒸馏效果。接下来，将在不同的基准数据集上对本章方法进行验证与分析。

\mysection{实验与结果分析}
\label{sec:ch3-5-experiments-and-analysis}
为评估本章提出的 Prompt-Distiller 算法在少样本知识蒸馏任务上的有效性，本节设计多个实验。从数据选取、参数设置、对比方法、整体性能、消融分析、参数敏感性到具体案例，将全面展示 Prompt-Distiller 的表现与优势。

\mysubsection{实验设置}
\label{sec:ch3-5-1-experimental-setup}

\mysubsubsection{数据集与任务}

选择了自然语言处理领域具有代表性的8个数据集进行评估，涵盖了文本分类和问答等不同类型任务，具体如下：
\begin{itemize}[leftmargin=3\ccwd]
	\item 自然语言推理任务：MNLI、MNLI-mm（MNLI的跨域测试集）、SNLI、RTE\cite{broadcoveragechallenge_williams_2018,largeannotatedcorpus_bowman_2015,semeval2013task_dzikovska_2013}。这类任务要求模型判断两个句子（前提和假设）之间的逻辑关系（如蕴含、矛盾、中立）。其中MNLI和SNLI为大规模数据集，而RTE较小。将采用其中子集构建少样本训练集。
	\item 情感分析任务：MR、SST-2\cite{seeingstarsexploiting_pang_2005,recursivedeepmodels_socher_2013}。要求判断给定文本（影评）的情感极性（正面或负面）。MR为影评二分类数据集，SST-2为经过标注的影评句子数据集。
	\item 用户意图问答任务：MPQA\cite{mpqa3.0entity/event_deng_2015}。为问答对的二分类，判断回答是否为肯定。
	\item 问句类型分类任务：TREC\cite{learningquestionclassifiers_li_2005}。将问句按所问信息类型分类，如人物、地点、数量等 6类问题。
\end{itemize}
上述数据集选择了不同领域和任务类型，以验证Prompt-Distiller的通用性。为了构造少样本场景，从每个数据集中按类别均匀采样$K=16$条训练样本，以及相同数量的验证样本，模拟16-shot学习环境。测试集则使用官方提供的标准测试集（若无官方测试集，则使用原验证集作为测试）。对于无标注数据$\tilde{X}$，从原始训练集中剔除少样本训练集和验证集后，随机抽取$10 \times |X|$条数据作为$\tilde{X}$（即$n=10$）。这些无标签语句在内容上与任务接近，可以为蒸馏提供丰富的语言现象。所有实验均重复5次，每次随机采样不同的少样本训练集，结果取平均值，以减少偶然性带来的波动。

\mysubsubsection{教师与学生模型配置}

教师模型选用RoBERTa-large预训练模型\cite{robertarobustlyoptimized_liu_2019}，其Transformer编码层数为24，隐层维度768，总参数约3.55亿。在少样本提示微调阶段，对教师模型应用PET方法\cite{exploitingclozequestions_schick_2021}。PET需要为每个任务设计提示模板和对应的标签词映射。为公平起见，直接使用Schick等人在PET论文中提供的公开模板。例如，对于 MNLI 任务，PET使用了若干自然语言句式（如“\textit{[Premise]? [Hypothesis]? Yes/Maybe/No.}”等）将文本对转换为带 [MASK] 的句子，并将“Yes”、“Maybe”、“No”三词分别映射为“蕴含”、“中立”、“矛盾”三类。类似地，其他任务模板在PET论文附录中均有列出。利用这些模板在16-shot数据上微调RoBERTa-large，获得提示微调教师模型$\Theta_T$。实际微调时，由于样本极少，采用PET所建议的技巧：初始化一个PET模型时包含若干个模式，每个模式都对应一个模板，用各模板分别微调出多个模型，然后将这些模型预测结果进行投票融合，可在一定程度上提升稳定性。不过，为了蒸馏过程简洁，亦可只使用单一最佳模板的模型作为教师。验证中观察到，使用多个模板融合提升有限，因此结果中报呈单一模板教师模型的性能。

学生模型选取缩小版的BERT模型。默认情况下，采用BERT-small作为学生，它有4个Transformer编码层，隐层维度512，参数约为2.9千万，仅为教师模型参数量的8.2\%。为了测试Prompt-Distiller在不同学生模型容量下的有效性，还尝试了BERT-base（12层，隐层768，约1.1亿参数）和BERT-tiny（2层，隐层128，约1.5千万参数）作为学生模型。这些小模型的预训练初始化均来自Turc等人提供的预训练权重\cite{wellreadstudents_turc_2019}。学生模型在知识蒸馏开始前尚未见过当前任务的数据。蒸馏过程中，教师模型始终冻结，学生模型的参数通过学习损失函数\eqref{eq:total-loss}不断更新。

\mysubsubsection{对比方法与参数设置}

为体现Prompt-Distiller的优势，设计了多种对比实验：

\textbf{Student FT/Student PT（下界）}：直接对学生模型进行少样本训练或提示微调，不使用任何蒸馏。FT 指传统微调方式（添加一个随机初始化的线性分类层训练学生模型）；PT 指提示微调方式（类似教师模型但在学生模型上执行）。这两个结果反映了学生模型在无教师指导下仅凭少量数据能达到的性能，是下界。

\textbf{Teacher FT/Teacher PT（上界）}：教师模型以相应方式训练所得性能。由于教师模型容量大、学习充分，其在少样本下的性能远高于学生模型，可视作所追求的上界。Teacher PT 即提示学习教师（RoBERTa-large PET 模型）的性能，Teacher FT 则指相同教师模型采用微调法训练的性能。

\textbf{Vanilla KD}、\textbf{BERT-PKD}、\textbf{TinyBERT}：这三种是经典的知识蒸馏方法，用于与 Prompt-Distiller 进行比较\cite{distillingknowledgeneural_hinton_2015,patientknowledgedistillation_sun_2019,tinybertdistillingbert_jiao_2020}。其中 Vanilla KD 仅使用教师输出 logits 指导学生；BERT-PKD 在蒸馏时对齐部分中间层 hidden state；TinyBERT 则结合中间层和输出层的多重损失。需要说明的是，这些方法原本假定有充分标注数据，在少样本条件下直接对其应用并可能调低了部分超参数（如 TinyBERT 中预训练蒸馏阶段数据，仅用原训练集的剩余无标签部分）。尽管如此，这些方法在如此极端的数据稀少环境下的表现依然代表了现有技术的应用上限。

\textbf{Prompt-KD}：针对提示学习的蒸馏基线。Prompt-KD与Prompt-Distiller的区别在于：Prompt-KD仅使用提示微调教师$\Theta_T$的输出分布指导学生，且不包括对比学习和无监督数据蒸馏（相当于$\lambda_1=1,\lambda_2=0$且只使用$X$）。该方法用于验证双教师和对比学习策略的贡献。

以上方法将在相同的数据划分和学生模型初始化下运行，以保证可比性。所有实验采用的优化器均为 AdamW，学习率 $1\times 10^{-5}$，批次大小 4，训练轮数视验证集性能动态决定，一般不超过 100 轮。Prompt-Distiller 中 $\lambda_1,\lambda_2$ 设置为 0.2 左右，温度系数 $\alpha$ 设为 10，负样本选择每正例 1 个。对比方法的参数亦在验证集上进行了调优，以确保其发挥最大性能。实验在 Tesla V100 GPU 上进行，教师模型微调和学生模型蒸馏各耗时不到 1 小时。

\mysubsection{整体性能比较}
\label{sec:ch3-5-2-overall-performance-comparison}
表\ref{tab:overall-results}展示了Prompt-Distiller与各基线方法在不同数据集上的性能比较，评价指标为测试集分类准确率（\%）。为了便于分析，将结果按训练范式（Fine-tuning或Prompt-tuning）分类列出，并标注了教师模型和学生模型的不使用KD时的上下界表现。

\begin{sidewaystable}
	\centering
	\renewcommand{\arraystretch}{1.3}
	\bicaption[Prompt-Distiller少样本性能对比]{Prompt-Distiller与各基线方法在8个少样本数据集上的准确率比较（\%）。粗体表示最佳结果，斜体表示教师模型的上界性能。其中 FT=Fine-Tuning，PT=Prompt-Tuning。}[Prompt-Distiller accuracy comparison]{Accuracy comparison (\%) between Prompt-Distiller and baseline methods on eight few-shot datasets. Bold denotes the best result and italics denote the teacher upper bound. FT=Fine-Tuning, PT=Prompt-Tuning.}
	\label{tab:overall-results}
	\small\begin{tabular}{c|l|*{8}{c}|c}
		\toprule[1pt]
		\textbf{训练} & \textbf{模型/方法}                                          & \textbf{MNLI} & \textbf{MNLI-mm} & \textbf{MPQA} & \textbf{MR}   & \textbf{RTE}  & \textbf{SNLI} & \textbf{SST-2} & \textbf{TREC} & \textbf{平均}   \\
		\midrule[0.5pt]
		\multirow{5}{*}{FT}
		            & 教师FT                                                    & \textit{44.3} & \textit{46.3}    & \textit{66.3} & \textit{75.0} & \textit{50.5} & \textit{48.5} & \textit{79.7}  & \textit{85.3} & \textit{61.9} \\
		            & 学生FT                                                    & 34.8          & 35.1             & 63.9          & 53.8          & 51.3          & 41.4          & 63.5           & 69.0          & 51.6          \\
		\cline{2-11}
		            & Vanilla KD~\cite{distillingknowledgeneural_hinton_2015} & 34.9          & 35.6             & 64.5          & 52.5          & 50.2          & 40.6          & 63.5           & 68.5          & 51.3          \\
		            & BERT-PKD~\cite{patientknowledgedistillation_sun_2019}   & 35.4          & 36.1             & 56.2          & 46.8          & 53.1          & 43.7          & 62.5           & 59.3          & 49.1          \\
		            & TinyBERT~\cite{tinybertdistillingbert_jiao_2020}        & 35.6          & 36.4             & 64.7          & 48.8          & 53.3          & 42.8          & 63.5           & 63.2          & 51.0          \\
		\midrule[0.5pt]
		\multirow{4}{*}{PT}
		            & 教师PT（上界）                                                & \textit{67.0} & \textit{69.0}    & \textit{84.5} & \textit{87.0} & \textit{71.3} & \textit{77.3} & \textit{93.1}  & \textit{85.8} & \textit{79.3} \\
		            & 学生PT（下界）                                                & 35.4          & 36.2             & 64.0          & 55.8          & 51.8          & 38.9          & 62.5           & 70.5          & 51.8          \\
		\cline{2-11}
		            & Prompt-KD                                               & 35.5          & 36.0             & 63.5          & 53.8          & 53.7          & 39.3          & 61.9           & 69.9          & 51.6          \\
		\cline{2-11}
		            & \textbf{Prompt-Distiller}                               & \textbf{43.3} & \textbf{45.0}    & \textbf{75.5} & \textbf{71.9} & \textbf{55.6} & \textbf{48.0} & \textbf{78.4}  & \textbf{71.3} & \textbf{61.1} \\
		\bottomrule[1pt]
	\end{tabular}
\end{sidewaystable}

从表\ref{tab:overall-results}可以看出，Prompt-Distiller在所有数据集上均取得了学生模型的最佳性能，并且相比其他知识蒸馏方法有显著优势。具体分析如下：

首先，对比“学生PT（下界）”和“教师PT（上界）”两行可见，小模型在不蒸馏时的性能远低于大模型。在16-shot条件下，RoBERTa-large提示微调后的平均准确率达到79.3\%，而BERT-small提示微调仅为51.8\%。这说明仅靠极少量数据直接训练小模型，其潜力难以发挥，存在大量可提升空间。传统微调下学生模型的平均准确率更低（51.6\%），这与已有研究一致：提示学习能更有效利用少数据。

其次，在Fine-tuning范式下，三种KD基线（Vanilla KD, BERT-PKD, TinyBERT）的效果并不理想，有的甚至不如不蒸馏的学生FT表现。例如，Vanilla KD平均准确率51.3\%，略低于学生FT的51.6\%；BERT-PKD更是降低到了49.1\%。这是因为在极少数据下，教师FT模型本身泛化能力较差，其知识未必能提升学生；同时附加的蒸馏损失可能让学生模型过拟合教师在训练集上的特殊模式，从而损害泛化。TinyBERT相对较稳健，平均51.0\%，但仍未超越学生FT。这一结果表明：传统KD在few-shot场景下的直接应用效果有限。

再次，在Prompt-tuning范式下，关注Prompt-KD和Prompt-Distiller的比较。Prompt-KD方法基本与学生PT持平（平均51.6\% vs 51.8\%），在大部分数据集上并未带来提升。这说明单纯蒸馏提示微调教师的输出logits，由于缺少数据支撑，学生模型并未学到更多有用东西，反而可能只是对教师输出做了简单拟合，难以拓展其泛化能力。

相比之下，Prompt-Distiller取得了平均61.1\%的准确率，较学生PT提高了约9.3个百分点，较Prompt-KD也提高了近9.5个百分点。这是一个巨大的跃升，表明本章提出的双教师+对比学习策略在few-shot知识蒸馏中发挥了显著作用。从各任务看，Prompt-Distiller在电影评论情感分类（MR, SST-2）上提升尤为明显，准确率从学生PT的55.8\%, 62.5\%提高到71.9\%, 78.4\%，增幅分别达16.1和15.9个百分点，接近教师模型性能。推测，这是因为情感分类任务上预训练模型的通用知识（如褒义贬义词汇的分布等）对小模型帮助很大，Prompt-Distiller通过无监督蒸馏充分挖掘了这部分知识。而在自然语言推理任务上（MNLI, SNLI, RTE），提升也较显著，MNLI提升约8个百分点，SNLI提升近9.1个百分点。哪怕是教师模型微调效果一般的RTE任务（教师上界仅71.3\%），学生模型在Prompt-Distiller下也超出了非蒸馏时的51.8\%，达到55.6\%，说明双教师策略让学生学到了一些教师未完全掌握的知识，从而避免了过拟合于教师的误判模式。唯一提升较小的是TREC问句分类（70.5\%到71.3\%），这是因为学生PT已达到70\%以上的高水平，小模型对该任务比较擅长（6分类，相对简单），故蒸馏空间有限。尽管如此，Prompt-Distiller仍取得最好结果。

综合来看，Prompt-Distiller在各任务上均显著超越了其他蒸馏方案，是唯一在few-shot环境下能有效提升学生模型性能的方法。这证明了本章提出的方法在应对少样本模型压缩挑战时的有效性。下面将通过消融实验进一步探究Prompt-Distiller各组成模块的贡献。

\mysubsection{消融实验分析}
\label{sec:ch3-5-3-ablation-analysis}
为了量化各模块对Prompt-Distiller性能的影响，设计了消融实验，逐一移除算法中的关键部分，观察学生模型精度的变化。表\ref{tab:ablation-results}给出了在三个具有代表性数据集上的结果，包括SST-2（情感分析）、MPQA（问答）、MNLI（推理）。

\begin{table}[htbp]
	\centering
	\bicaption[Prompt-Distiller消融结果]{Prompt-Distiller各组成部分的消融实验结果（准确率\%）。无对比学习表示$\lambda_2=0$且不使用探针蒸馏，无未标注数据表示$\tilde{X}$不参与且$\lambda_1$对应项为0，二者均无则同时移除两者，仅剩 $\mathcal{L}_{\text{MLM}}(X)$ 和 $\mathcal{L}_{\text{KD}}(X)$。}[Ablation results]{Ablation results (accuracy \%) for the components of Prompt-Distiller. Removing contrastive learning sets $\lambda_2=0$ and disables probe distillation; removing unlabeled data excludes $\tilde{X}$ and zeroes the $\lambda_1$ term; removing both keeps only $\mathcal{L}_{\text{MLM}}(X)$ and $\mathcal{L}_{\text{KD}}(X)$.}
	\label{tab:ablation-results}
	\small\begin{tabular}{l|ccc}
		\toprule[1pt]
		\textbf{模型配置}         & \textbf{SST-2} & \textbf{MPQA} & \textbf{MNLI} \\
		\midrule[0.5pt]
		Prompt-Distiller 完整模型 & \textbf{78.4}  & \textbf{75.5} & \textbf{43.3} \\
		\midrule[0.5pt]
		无对比学习                 & 75.3           & 73.3          & 42.4          \\
		无未标注数据                & 63.1           & 64.3          & 39.2          \\
		二者均无                  & 62.5           & 64.0          & 35.4          \\
		\bottomrule[1pt]
	\end{tabular}
\end{table}

由表\ref{tab:ablation-results}可见：
1）移除对比学习模块（即$\mathcal{L}_{\text{CPD}}(X)$和$\mathcal{L}_{\text{CPD}}(\tilde{X})$均不使用）时，模型性能有所下降。例如SST-2由78.4\%降至75.3\%，MPQA由75.5\%降至73.3\%，MNLI由43.3\%降至42.4\%。尽管下降幅度不算极端（相对降幅分别为4\%、2.9\%、2.1\%），但趋势表明对比学习确实对性能有正向贡献。尤其在SST-2上，其效果较明显，这可能是因为情感分析中一些中间特征（如语气词、程度副词）对判别很重要，对比蒸馏帮助学生更好地捕捉这些细微特征。

2）移除无标注数据蒸馏模块（即不使用$\mathcal{L}_{\text{KD}}(\tilde{X})$和$\mathcal{L}_{\text{CPD}}(\tilde{X})$，仅靠$X$进行蒸馏）时，性能大幅下降。SST-2降为63.1\%，MPQA降为64.3\%，MNLI降为39.2\%。与完整模型相比，SST-2和MPQA分别下降了约15个百分点和11个百分点。这证明无标注数据提供的预训练知识对蒸馏效果至关重要。在少样本环境中，如果没有$\tilde{X}$的辅助，学生模型几乎无法超越仅监督训练的结果（对照表\ref{tab:overall-results}中学生PT，SST-2为62.5\%，MPQA为64.0\%，与此处无未标注数据配置非常接近）。可见，$\tilde{X}$填补了知识来源的空缺，使学生模型能力有质的飞跃。

3）当同时去掉对比学习和无监督数据（只保留$\mathcal{L}_{\text{MLM}}(X)$和$\mathcal{L}_{\text{KD}}(X)$）时，学生模型效果跌至最低：SST-2为62.5\%，MPQA为64.0\%，MNLI仅35.4\%。这些结果几乎与Prompt-KD（甚至弱于Prompt-KD）的水平相当，印证了Prompt-KD中学生模型未获提升的现象。这进一步验证了双教师和对比蒸馏策略是Prompt-Distiller成功的关键：缺一不可。双教师提供了知识量的扩充，而对比学习提供了知识质量的提升，共同造就了远超基线的效果。

综上，消融实验清晰地表明：引入无标注数据的双教师蒸馏对性能提升贡献最大，其次探针对比学习亦提供了可观的附加收益。两者结合使Prompt-Distiller充分挖掘了教师模型的多层次知识。

\mysubsection{样本规模和模型规模分析}
\label{sec:ch3-5-4-sample-and-model-scale-analysis}
为了进一步了解Prompt-Distiller的适用范围，考察了训练样本规模和学生模型容量对蒸馏效果的影响。

\mysubsubsection{不同训练样本规模分析}

在SST-2和MNLI两个数据集上分别尝试了$K=16, 32, 64, 128, 256, 512$的few-shot设定，以观察当标注数据逐渐增多时，Prompt-Distiller相对于直接提示微调的优势变化。如图\ref{fig:datasize}所示，在SST-2上，随着$K$从16增至512，Prompt-Distiller和学生PT的性能差距从约16\%逐步缩小至约5\%。在MNLI上，该差距也从约8\%缩小至约3\%。这表明，Prompt-Distiller对少样本场景提升最为显著；当标注数据较丰富时，小模型自身已能学到足够知识，蒸馏的边际收益变小。但即便在$K=512$的情况下，Prompt-Distiller仍保持了领先，这说明双教师+对比学习并不会在数据增多时失效，它对不同规模的数据具有鲁棒性。当$K$极小时（如16），Prompt-Distiller几乎成为了学生模型性能提升的唯一来源；当$K$较大时，它依然能提供有益的指导，使学生模型更接近教师模型的水平。由此可见，Prompt-Distiller适用于各种few-shot乃至中等规模数据场景，具有良好的泛化性。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.7\textwidth]{plot.pdf}
	\bicaption[不同训练数据量下知识蒸馏的性能对比]{在不同训练样本规模 $K$ 下，Prompt-Distiller 与无蒸馏学生模型在 SST-2 和 MNLI 上的性能对比。随着 $K$ 增大，两者差距逐渐缩小，但 Prompt-Distiller 始终保持优势。}[Performance of Knowledge Distillation under Varying Training Data Sizes]{Performance comparison between Prompt-Distiller and a student model trained without distillation under varying training set sizes $K$, illustrated on SST-2 and MNLI. As $K$ increases, the performance gap narrows, yet Prompt-Distiller consistently outperforms the baseline.}
	\label{fig:datasize}
\end{figure}

\mysubsubsection{不同学生模型规模分析}

接下来，将学生模型从默认的BERT-small换成BERT-base和BERT-tiny，来验证Prompt-Distiller对于更大或更小学生模型的适用性。表\ref{tab:model-scale}列出了SST-2、MPQA、MNLI上三种学生模型的蒸馏前后性能。可见：

对于BERT-base学生模型（参数1.1亿，接近教师模型的1/3），其不蒸馏时的few-shot提示微调准确率在三任务上分别为66.1\%、67.2\%、40.4\%，远高于BERT-small的下界（这符合直觉：模型大一些则容易拟合少量数据）。经过Prompt-Distiller蒸馏后，BERT-base性能进一步提升到87.8\%、84.0\%、51.5\%，几乎已追平教师模型在MNLI上的水平（51.5\% vs 教师FT 52\%左右）并超越教师模型在SST-2、MPQA上的上界。这说明Prompt-Distiller同样能使较大容量的学生模型受益，其学习到的教师知识足以弥补BERT-base与RoBERTa-large结构差异带来的性能落差。
对于极小的BERT-tiny模型（参数1500万，仅教师的4.2\%），不蒸馏时表现较差（SST-2仅56.8\%，MNLI仅33.6\%），几乎不能胜任复杂任务。蒸馏后，它们分别提高到62.4\%、34.8\%，虽然绝对增幅有限，但方向依然是积极的。在MNLI上BERT-tiny蒸馏只提高了1.2个百分点，因为模型容量实在过小，难以承载大量知识。但在SST-2上提升了5.6个百分点，说明即使极小模型，Prompt-Distiller也能赋予其一定的进步空间。实际上，对于BERT-tiny这样容量的模型，更适合用于简单任务或作为特定系统模块，但从实验可知本章方法对其仍有作用。
对于默认的 BERT-small，前文已详述，蒸馏效果最佳，平均涨幅 9 个百分点以上。

综合而言，Prompt-Distiller适用于不同尺寸的学生模型：对于容量较大的学生（如base），蒸馏使其如虎添翼，在few-shot下达到甚至超过教师的水平；对于极小模型，蒸馏虽受限于容量，收益较小但仍有提升；对于中等小模型，收益最显著。这说明双教师和对比学习策略并非针对某一特定规模模型，而是一种普适的知识迁移增强手段。

\begin{table}[htbp]
	\centering
	\bicaption[不同学生模型容量对比]{不同学生模型容量下Prompt-Distiller的蒸馏效果比较。每格中“$A/B$”分别表示不蒸馏时学生模型准确率$A$及蒸馏后的准确率$B$（\%）。}[Student model capacity comparison]{Comparison of Prompt-Distiller's distillation effects under different student model capacities. Each cell “$A/B$” reports the student accuracy without distillation ($A$) and after distillation ($B$) in \%.}
	\label{tab:model-scale}
	\small\begin{tabular}{l|ccc}
		\toprule[1pt]
		\textbf{学生模型}       & \textbf{SST-2} & \textbf{MPQA} & \textbf{MNLI} \\
		\midrule[0.5pt]
		BERT-base (12层110M) & 66.1 / 87.8    & 67.2 / 84.0   & 40.4 / 51.5   \\
		BERT-small (4层29M)  & 62.5 / 78.4    & 63.9 / 75.5   & 34.8 / 43.3   \\
		BERT-tiny (2层15M)   & 56.8 / 62.4    & 60.6 / 67.1   & 33.6 / 34.8   \\
		\bottomrule[1pt]
	\end{tabular}
\end{table}

\mysubsection{案例研究}
\label{sec:ch3-5-5-case-study}
最后，通过一个具体案例直观展示 Prompt-Distiller 如何融合双教师知识来提升学生模型的输出能力。表\ref{tab:case-study} 列出了在 SST-2 情感分析任务上，一个测试样本经不同模型处理后的 [MASK] 预测结果。该样本是一个影评句对：\textit{“it all adds up to good fun. it was [MASK].”}（大意：“这一切加在一起很好玩。这部电影真是 [MASK]。”）正确的情感标签是正面，对应的标签词选取 great。

\begin{table}[htbp]
	\centering
	\bicaption[少样本提示蒸馏的案例分析]{影评掩码句“it was [MASK].”在不同模型中输出概率最高的若干预测词对比：提示微调教师模型与预训练教师模型均将具有积极语义的“great”列为高概率预测词；经 Prompt-Distiller 蒸馏后，学生模型亦成功将“great”作为最高概率词输出，表明蒸馏过程有效保留了教师模型的语义偏好。}[Case Study on Few-shot Prompt Distillation]{Top predicted tokens for the masked film review sentence “it was [MASK].” across different models: both the prompt-fine-tuned teacher and the pre-trained teacher assign high probability to the positively connoted word “great”; after distillation via Prompt-Distiller, the student model also correctly ranks “great” as the highest-probability prediction, indicating effective transfer of semantic preference from the teacher.}
	\label{tab:case-study}
	\small\begin{tabular}{l|p{6cm}}
		\toprule[1pt]
		\textbf{模型}                   & \multicolumn{1}{c}{\textbf{Top-3候选词}}       \\
		\midrule[0.5pt]
		提示微调教师 RoBERTa-large          & \texttt{\small great, brilliant, good, ...} \\
		原始预训练教师 RoBERTa-large         & \texttt{\small fun, great, good, ...}       \\
		Prompt-Distiller学生 BERT-small & \texttt{\small great, shaken, ...}          \\
		\bottomrule[1pt]
	\end{tabular}
\end{table}

从表\ref{tab:case-study}可见：提示微调后的RoBERTa-large教师模型在该提示`it was [MASK].''处的预测中，把表示棒极了的词great排在第一位，紧随其后的是“brilliant”（精彩的）、good（好的）等正面词汇。原始未微调的RoBERTa-large预训练模型虽然不知道当前任务是情感分类，但由于庞大的语料记忆，它在`it was [MASK].''后最有可能填入的词也是带有积极含义的fun（有趣）、great、good等。这说明预训练模型本身具有一定的常识：当一句话说“很好玩，这真是\_\_\_”，大概率是正面评价。学生模型BERT-small在未经蒸馏前，很可能无法正确预测这个[MASK]词（观察到未蒸馏学生往往给出无关或中性词）。但经过Prompt-Distiller训练后，学生模型的最高概率预测词同样变成了great，第二高为shaken（这里是干扰项，可能由于学生容量小出现），其他备选也更贴近正面倾向。这个例子生动地表明：通过Prompt-Distiller，学生模型成功地从提示微调教师那里学到了任务相关的倾向（即选择正面词汇），也从预训练教师那里继承了对常见表达的判断（如将great视为高度相关的填空）。两方面知识的融合，使学生模型给出了正确的输出，并且较好地模拟了大模型的行为。这与设计Prompt-Distiller的初衷一致，即学生模型不仅在硬指标上逼近教师，在行为上也越来越像教师，对任务的理解更加全面准确。

\mysection{本章小结}
\label{sec:ch3-6-chapter-summary}
本章围绕少样本提示学习模型的压缩问题，提出了 Prompt-Distiller 算法，并通过理论推导和实验证明了其有效性。以预训练语言模型的提示微调为背景，引入双教师机制和探针对比学习，实现了在极少标注数据条件下的小模型高效蒸馏。通过提示微调教师的任务特定知识与预训练教师的通用知识相结合，学生模型克服了训练数据匮乏的限制；而基于对比学习的中间层知识迁移则进一步提升了学生对教师深层判别模式的掌握。从多个任务的实验结果看，Prompt-Distiller 显著提高了学生模型的精度，在部分任务上达到或超过教师模型的表现，验证了多层次知识迁移策略的作用。同时，分析各组件的贡献表明，无标注数据提供的预训练知识是提升关键，探针对比学习亦增强了蒸馏效果；数据规模和模型规模实验展示了 Prompt-Distiller 的通用适用性。最后，案例直观说明了学生模型如何通过蒸馏获得更优表现。

本章的研究为少样本条件下深度模型的高效构建提供了新的思路，即利用大模型蕴含的不同层次知识指导小模型训练，从而弥补后者在数据和容量上的不足。这种方法与前章提出的多层次知识迁移框架一脉相承，证明了多层次知识在模型压缩领域的价值。本章的研究结果为下一章的系统实现提供了理论依据：将在下一章探讨如何将Prompt-Distiller蒸馏得到的小模型应用于实际的在线服务系统，并结合其他优化手段，实现深度模型在工业场景下的高效部署与推理。

\end{document}
