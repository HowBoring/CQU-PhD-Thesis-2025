# 第1章 引言

## 1.1 研究背景与动机

近年来，以**深度学习**为核心的人工智能技术取得了飞跃发展。其中，大规模**预训练模型**（如BERT、GPT系列）引领了自然语言处理等领域的范式变革，通过在海量数据上预训练模型再微调下游任务，大幅提升了各项任务性能。特别是**大型语言模型（LLM）**的兴起表明，模型规模和数据规模的扩张可以显著增强模型能力。以GPT-3为例，其参数规模高达1750亿，比前代GPT-2扩大两个数量级，并展示出少样本学习等新兴能力。研究指出，语言模型性能与模型参数规模、训练数据规模和计算量呈**幂律关系**持续提升。在过去几年中，SOTA模型的参数量正以每年至少10倍的速度增长，远超硬件性能提升速度。这一趋势导致模型训练成本飞速攀升——据估计，训练GPT-3单次迭代就需耗费约355 GPU年，费用超过\$460万美元。同时，模型参数存储和计算对显存和分布式计算提出了前所未有的挑战，模型并行化等新技术因此成为必要手段。

然而，**追求规模效应**的同时也暴露出**效率瓶颈**。首先，巨型模型的**算力成本**和**能耗**极为高昂，大模型开发仅有极少数科研机构和企业可承担。例如，中国北京智源研究院发布的“悟道2.0”多模态模型参数规模达1.75万亿，为GPT-3的10倍，采用了稀疏Mixture-of-Experts（MoE）架构以降低训练开销；即便如此，其训练仍消耗了海量的数据和算力资源。其次，模型对**训练数据**的依赖程度令人关注。大模型往往需要爬取、清洗数百GB乃至TB级的语料方能支撑预训练；而下游任务的有标签数据更为稀缺昂贵，许多场景难以获取足够的数据来微调如此庞大的模型。为缓解标注数据不足的问题，学术界提出了**少样本学习**、**Prompt学习**等方案，但这些方法通常仍依赖预训练模型的强大表征能力。

再次，即使有了预训练的大模型，将其**迁移适配到特定任务或环境**也面临困难。传统微调需要对模型全部参数进行梯度更新，但当模型参数量达到百亿级时，对每个新任务都进行全参数微调在计算和存储上都不可行。例如，针对一个1750亿参数的模型进行微调，不仅需要专用的多GPU集群支撑计算，其微调后的模型副本也难以部署。为提高适配效率，近年来出现了各种**参数高效微调（PEFT）**方法，例如冻结大部分预训练模型参数，仅在每层加入少量可训练参数的**LoRA方法**、在模型嵌入层追加可调向量的Prompt Tuning和Prefix Tuning等。LoRA将需要训练的参数数量减少了四个数量级，显著降低了显存占用和计算开销，在RoBERTa、GPT-3等上表现出与全模型微调相当的效果。同时，**知识蒸馏**等模型压缩技术作为一种经典的**知识迁移**手段，日益受到关注。蒸馏通过让小模型（学生）学习大模型（教师）的输出分布，实现模型压缩和加速，在移动设备等受限环境中具有重要意义。然而，小模型往往在模型容量和泛化性能上存在不足，蒸馏过程也需要精心设计（如教师选择、损失函数等）才能最大程度保留大模型知识。

此外，**模型架构设计的复杂性**也成为效率瓶颈之一。深度模型的性能高度依赖于网络架构，但手工设计高性能网络既需要专家经验又耗费大量试错成本。**神经架构搜索（NAS）**方法通过自动化搜索模型结构取得了一定成功，但经典NAS往往需要海量算力支撑，其搜索过程（如进化算法或强化学习控制器训练）可能耗费数千GPU天。为提高NAS效率，近年来提出了**可迁移的NAS（Transferable NAS, TNAS）**，即利用先前搜索得到的知识来加速新任务上的架构搜索。然而现有TNAS大多局限于**同构搜索空间**（即假定新任务与旧任务在相同的网络搜索范围内）。当面对不同领域或不同搜索空间的新任务时，如何**迁移架构设计经验**仍缺乏有效手段，这导致跨领域NAS仍需从零开始，开销巨大。

最后，**模型能力的迁移融合**需求日益凸显。在实际应用中，一个预训练模型通常经过微调形成多个**面向特定任务的专用模型**，每个模型各司其职。然而这些微调模型之间**缺乏协同**，无法共享彼此学到的知识。传统上，如果希望一个模型同时解决多任务，往往需要进行多任务联合训练或增量训练，但这在大模型场景下成本同样高昂，且可能出现遗忘效应。为此，最近兴起了**模型融合（Model Merging）**技术，尝试在**无需额外训练**的情况下，通过直接融合多个已微调模型的参数来构造一个**多任务统一模型**。该思路在早期被称为“**Task Arithmetic**”（任务算术），即通过对模型参数向量进行算术运算来合成功能叠加的模型。然而，简单的参数平均或线性组合往往导致**显著的性能退化**，因为不同模型参数更新之间存在冲突和冗余。为解决这一问题，研究者提出了一系列改进策略。例如，Yadav等提出的**TIES-Merging**方法通过重置小幅更新参数、对齐符号冲突等步骤来减轻多模型融合时的干扰；Yu等提出的**DARE**方法则通过“Drop-And-Rescale”策略消除不同模型更新的不兼容部分，实现能力吸收。另外，Akiba等研究了利用**进化算法自动优化融合配方**，通过遗传搜索寻找最优的模型融合方式。尽管这些方法在一定程度上改善了多模型融合效果，但当融合的模型数量和任务范围继续扩大时，融合策略的搜索空间呈几何级膨胀，优化难度剧增，依然缺乏高效通用的解决方案。

综上所述，**深度模型开发与适配**面临“**高成本、高复杂度**”的严峻挑战：模型训练和微调的计算代价日益攀升，数据获取与标注成本巨大，模型架构设计与优化复杂耗时，多模型协同融合尚无成熟范式。这些背景和问题催生出强烈的研究动机——如何**降低深度模型开发与适配的成本**，让更广泛的任务和场景都能负担和受益于大模型的强大能力；如何**高效地迁移已有知识和能力**，避免重复训练并减小对海量数据的依赖；如何**优化迁移过程**以最大化迁移效果、避免负迁移和冲突。在这样的共同需求驱动下，本文将探讨新的理论和技术范式来应对上述挑战。

## 1.2 核心科学问题

面对深度模型开发与应用中的种种效率瓶颈，**本文关注的核心科学问题**可以概括为：\*\*如何有效降低深度模型的开发与适配成本，通过知识迁移和优化手段来提升模型构建的效率和性能？\*\*具体而言，本研究试图解决以下关键问题：

* **（1）如何高效地复用已有模型的知识来服务新的任务？** 深度学习模型在源领域往往学得丰富的表征和能力，我们希望在目标任务中充分利用这些已有知识，而非从零开始训练。但不同任务/领域之间存在差异，直接迁移可能导致负迁移。需要研究**迁移学习**中“迁移什么、如何迁移、何时迁移”等基本问题，包括如何选择和提取源模型中对目标任务有用的知识（如特征、参数、结构）、如何设计高效的迁移策略和算法来避免源知识与目标任务冲突，以及在什么条件下迁移能带来正向增益。

* **（2）如何降低大模型适配特定任务的资源开销？** 传统的全模型微调在大模型场景下不可持续，需要探索参数高效的适配方法。例如只调优少量参数（如LoRA、Adapters）、借助少量示例的提示调优、或利用蒸馏将知识迁移到小模型以部署推理。核心问题在于如何在**大幅减少训练开销**的同时，最大限度保持原模型对新任务的性能。这里涉及到优化目标的权衡：既要减少计算量和参数改动，又要确保模型在目标任务上的准确率不显著下降。

* **（3）如何实现在**不同模型和架构之间**迁移**，以加速新模型的构建？\*\* 不同任务可能需要不同的模型结构，但从头设计和训练每个新架构代价巨大。如果能将之前任务中优化得到的**架构设计经验**迁移过来，将显著减少搜索时间。因此需要研究**架构层面的迁移学习**：包括如何表示和保存神经网络架构的知识，如何在新任务中高效利用这些知识初始化或引导架构搜索，如何跨不同搜索空间进行迁移等。这一问题延伸出对**异构迁移**（跨领域、跨架构）的探索，具有很强的挑战性。

* **（4）如何将多个模型的**能力融合**到一个模型中？** 当面对综合性任务或资源受限环境时，希望能有一个模型拥有多个已训练模型的技能，而不是维护多个专用模型。这涉及**能力迁移与整合**的问题：如何在不损失各模型特长的前提下，将它们融合为一个统一模型。需要解决参数冲突、能力干扰等技术难点，并寻找高效的融合优化方法来**避免暴力搜索**。这一问题连接到模型融合、终身学习、连续学习等研究方向，具有重要的理论和应用价值。

概括而言，本论文要回答的核心问题是：**能否提出一种统一的方法论，将模型知识迁移与优化过程有机结合，在内容、结构和系统层面全面提升深度模型开发与适配的效率？** 这要求我们在理论上定义新的范式，在方法上提供可行的算法框架，并通过实验证明该范式在不同场景下的有效性。

## 1.3 本文核心思想——迁移优化范式

针对上述科学问题，本文提出并定义一种统一的方法论——**“迁移优化”**（Transfer Optimization）范式。简而言之，迁移优化指的是：**在深度模型的开发与适配过程中，以明确的优化目标为导向，将源模型/任务的知识高效迁移到目标模型/任务中，从而以最小代价获得最大性能增益的方法论**。

这一范式包含“**迁移**”和“**优化**”两个关键词的有机结合。一方面，“迁移”强调利用已有的模型、数据或经验（记为迁移源\$S\$）来帮助新的模型或任务（记为目标\$T\$），体现为知识层面的重用；另一方面，“优化”则意味着我们并非简单地直接套用源知识，而是围绕某个性能指标或损失函数（记为优化目标\$F(\cdot)\$）来**主动调整迁移过程**，通过算法优化使迁移效果最优。

具体来说，我们将迁移优化范式应用于**深度模型开发与适配的不同层面**，呈现出方法论的统一性：

* **模型知识迁移（内容优化）**：这是迁移优化在*知识内容*层面的体现。典型情形如**知识蒸馏**，我们以大模型的预测分布或中间表示为知识\$K\$，通过设计特定的损失函数\$F\$来优化学生模型对这些知识的拟合，从而在保持性能的情况下压缩模型。又如**微调优化**，我们冻结部分参数，仅优化少量新参数，使模型在新任务上的损失\$F\$最小化。这些过程都符合迁移优化的范式：从源模型提取有价值的知识，并通过优化算法高效传递给目标模型。本文的工作1（详见第3章）正是关注于此，通过对比学习优化目标，实现在少样本条件下大模型对小模型的知识迁移。

* **模型架构迁移（结构优化）**：这是迁移优化在*模型结构*层面的应用。传统NAS需要从头搜索架构，而迁移优化范式下，我们希望将**源任务中优良架构的设计经验**迁移过来。例如，可将已验证有效的网络模块、拓扑等作为先验嵌入到新任务的搜索过程中，并设定适当的优化目标\$F\$引导搜索，使得在更小搜索成本下找到高性能架构。这种方法论上的统一在**可迁移架构搜索**中得到体现。本文的工作2（详见第4章）提出了跨搜索空间的迁移NAS方法，通过学习公共的架构表示和演化优化算法，实现不同任务之间架构知识的迁移，被视为迁移优化在结构设计上的成功实践。

* **模型能力迁移（系统级优化）**：这是迁移优化在*系统整合*层面的体现。它关注将多个模型的能力迁移并融合到一个模型中，涉及复杂的多源迁移情景。迁移优化范式认为，可以通过**将多模型融合问题转化为一个优化问题**，设定合理的目标函数（例如融合后模型在各任务上的综合损失），并利用优化算法在参数空间中搜索最优的融合方案。这一思想贯穿于模型融合最新研究中，例如上文提到的TIES-Merging通过优化参数符号一致性来减小干扰，Akiba等的进化融合通过遗传算法优化融合配比。在本文工作3（详见第5章）中，我们同样遵循迁移优化范式，将大模型多任务融合表述为多个子问题的优化组合，借助演化策略逐步迁移各子问题的解以优化原问题，实现了系统级能力的高效整合。

通过以上三方面，我们正式定义**迁移优化**为一个普适的方法论范式：给定迁移源\$S\$（可以是预训练模型、已有架构、已训好的模型集合等）和目标\$T\$（可以是新模型、新任务域或融合后的模型），确定待迁移的知识元素\$K\$（如参数、特征、结构单元等）以及任务域\$D\$（如数据分布、任务集合），设计适当的优化目标函数\$F(\cdot)\$，并通过优化算法求解，使得知识从\$S\$向\$T\$的传递效果达到最优。在这个范式下，不同迁移场景中的众多方法都可视作特例：无论是知识蒸馏、参数高效微调，还是架构搜索迁移、模型融合等，都可以抽象为上述形式。这种统一性有助于我们从更高的视角审视深度学习中的迁移问题，并开发出兼具通用性和高效性的解决方案。值得注意的是，在进化计算领域也出现了将知识迁移融入优化过程的研究，被称为“进化迁移优化”。本文的迁移优化范式在思想上与其一脉相承，但我们聚焦于深度模型领域的迁移与优化融合，涵盖模型训练、架构设计和模型融合等多个层面，具有更广泛的应用背景和挑战。

## 1.4 本文研究内容与主要贡献

围绕迁移优化这一核心思想，本文开展了系统的研究工作。作者在攻读博士学位期间已完成三项互相关联的研究，分别针对深度模型开发与适配的不同层面提出了创新方法，共同构成了迁移优化范式在**内容优化、结构优化和系统级能力整合**三方面的分层次体现。下面对这三项研究内容及其主要贡献作简要介绍。

**（1）小模型高效学习大模型知识的方法：Prompt-Distiller**。本研究针对大规模预训练语言模型微调过程中**标注数据不足**和**模型部署困难**这两大痛点，提出了一种基于对比学习的**小样本知识蒸馏**方法。主要思想是利用预训练大模型作为教师，在只有极少数标注样本（few-shot）的目标任务中，通过精巧设计的**Prompt模板**引导模型输出，以**双重对比学习**构造蒸馏目标，最大化学生模型对教师模型知识的吸收。与传统知识蒸馏不同，我们的框架同时考虑了**正样本对比**（拉近学生对正确答案的表示与教师的对应表示）和**难负样本对比**（推开学生对错误选项的表示），从而在极小数据下稳定了蒸馏过程。实验在多个NLP基准任务上表明，该方法无需大型模型完整微调，只用少量示例就能使一个小模型达到接近教师模型的性能，但推理开销大幅降低。这项工作率先将**对比学习与Prompt微调**引入少样本蒸馏领域，为大模型知识高效传递提供了新思路。【发表于ICASSP 2023】

**（2）跨域迁移的神经架构搜索方法：Evolutionary-TNAS**。本研究面向**神经架构搜索**的高成本问题，提出了一种利用表征学习进行**跨搜索空间的迁移NAS**方法。我们的核心贡献在于设计了一个统一的**神经架构表示编码器**：将网络的算子序列和拓扑结构编码成定长序列向量，经由基于Transformer的变分自编码器学习得到紧凑的架构隐空间表示。基于这一表示，我们构建了不同搜索空间之间的**映射机制**，使源空间中优良架构对应的隐编码能够映射到目标空间，从而提供初始解猜测。随后，我们开发了一套**进化迁移优化算法**：在进化搜索过程中，利用源域的优秀个体种群对目标域种群进行初始化和引导（通过隐空间映射实现），实现跨域的架构经验迁移。在NAS-Bench-101、NAS-Bench-201以及DARTS等公共搜索空间上的实验表明，与从头搜索相比，我们的方法在目标任务中以更少的评价次数找到了更高精度的架构，实现了**搜索效率和性能的同步提升**。值得强调的是，这是首次在**异构架构搜索空间**间实现有效迁移的NAS方法，证明了迁移优化范式在架构设计领域的可行性。【发表于IEEE TEVC，2025】

**（3）大模型多任务能力无训练融合的方法：EMFTO**。本研究针对**大型语言模型的多任务自适应**难题，提出了一种基于进化的**多形态迁移优化融合**方法。我们关注于**模型融合（Model Merging）**这一新兴思路，即将多个针对不同任务微调后的大型模型直接在参数空间融合，生成一个具备多任务能力的新模型。现有工作如Task Arithmetic提供了基本框架，但随着融合模型数量增加，参数冲突和冗余导致性能急剧下降。我们在分析了参数**符号冲突**和**重要性差异**等干扰因素基础上，提出将原始的多模型多任务融合问题分解为一系列更小的**子问题**：每个子问题只涉及部分模型和部分任务的融合。通过为不同子问题设置多种融合形态，我们获得了一系列子优化解及其中蕴含的**模型-任务关系知识**。接下来，我们设计了**进化式的融合优化算法**，将子问题的经验逐步迁移回原始问题：在进化过程中以子问题解为指导高效探索融合策略空间，从而显著降低了优化难度。实验以多个开源大型语言模型为基础，融合多项下游任务，结果表明我们的方案相比现有融合方法在综合性能上有明显提升。在无需追加训练的情况下，新模型成功吸收了各专家模型的知识，实现了\*\*“一次融合，多任务共享”**的能力扩展。此工作将迁移优化理念拓展到**系统级能力整合\*\*层面，为今后大模型的弹性部署和升级提供了有益参考。【本工作相关成果已投稿，处于评审中】

综上，本文围绕迁移优化范式，在模型内容压缩、架构设计、能力融合三个层次各有创新贡献。三个研究方向各自独立又一脉相承：工作1解决小模型高效继承大模型知识，实现**知识层面的迁移优化**；工作2解决新模型架构高效搜寻，实现**结构层面的迁移优化**；工作3解决多模型能力统一集成，实现**系统层面的迁移优化**。它们共同构建了**迁移优化**这一新范式的理论与实践基础。本文的主要创新点在于：提出了迁移优化的统一框架，将不同迁移问题纳入同一优化驱动下解析；在该框架下分别设计了针对内容、结构、系统的具体算法，并通过实验证明了其有效性和优越性。三项工作逐级推进，体现了从理论概念到方法落地的逻辑主线，充分展示了迁移优化范式在深度学习领域的巨大潜力。

## 1.5 论文结构安排与符号约定

**论文结构：** 本文余下章节的组织安排如下：

* **第2章（背景技术与相关工作）**：回顾深度模型高效开发与适配相关的研究背景，包括预训练模型与微调范式的发展、迁移学习理论基础，知识蒸馏和模型压缩技术，神经架构搜索与架构迁移方法，大模型参数高效微调策略，以及大型模型融合与多任务学习等前沿工作，为后续章节奠定理论基础。

* **第3章**：介绍迁移优化范式在**内容优化**层面的实现，即小样本知识蒸馏方法Prompt-Distiller。详细阐述该方法的算法流程、对比学习蒸馏目标的设计，以及在Few-Shot学习场景下的实验结果和分析。

* **第4章**：介绍迁移优化范式在**结构优化**层面的实现，即跨搜索空间的迁移NAS方法Evolutionary-TNAS。给出神经架构表示学习的模型设计、跨域映射与迁移进化算法，并报告在不同NAS基准上的性能提升情况。

* **第5章**：介绍迁移优化范式在**系统级能力融合**层面的实现，即大模型多任务演化融合方法EMFTO。描述将多模型融合问题分解的策略、进化迁移优化的求解过程，以及实验中新模型在多任务上的综合表现，对比现有模型融合方法的优势。

* **第6章（总结与展望）**：对全文进行总结，凝练本文的研究结论，讨论迁移优化范式的局限与可拓展之处，并展望未来值得深入研究的方向。

**符号约定：** 为了在各章节中保持一致，本文使用如下主要符号（括号中为英文释义）：

* \$S\$（Source）：迁移源，表示提供知识的源模型或源任务；
* \$T\$（Target）：目标，表示承接迁移知识的目标模型或目标任务；
* \$K\$（Knowledge）：知识，表示从迁移源中提取并用于迁移的内容，例如模型参数、特征表示、架构单元等；
* \$D\$（Domain）：任务域，表示问题所属的领域或数据分布，如源域/目标域；
* \$F(\cdot)\$（Objective Function）：优化目标函数，表示在迁移优化过程中需要优化的评价函数（如损失函数或性能指标）。

上述符号在后续章节中反复出现，以指代对应的概念。在不引起混淆的情况下，本文中多次出现的术语我们也沿用其英文缩写形式（如“LLM”指大型语言模型、“NAS”指神经架构搜索等）。读者在阅读时可随时参照本节提供的符号表，以便准确理解相关论述。本文通过以上章节结构和符号约定，逐步展开对迁移优化技术的全面研究与探讨。各章内容既相对独立又前后呼应，围绕统一的主题展开，力求逻辑清晰、层次分明地呈现本论文的研究工作。

**参考文献：**

1. Chuan Li. *OpenAI’s GPT-3 Language Model: A Technical Overview*. Lambda Labs Blog, 2020.

2. Tom B. Brown, *et al.* “Language Models are Few-Shot Learners.” *Advances in Neural Information Processing Systems*, vol. 33, 2020, pp. 1877–1901.

3. Xuetan Liu, *et al.* “Pre-trained Models for Natural Language Processing: A Survey.” *Science China Technological Sciences*, vol. 63, no. 10, 2020, pp. 1872–1897.

4. Wikipedia. “Wu Dao (Artificial Intelligence Model).” *Wikipedia*, 2023.

5. Edward J. Hu, *et al.* “LoRA: Low-Rank Adaptation of Large Language Models.” *Proceedings of the 2022 International Conference on Learning Representations (ICLR)*, 2022.

6. Jianping Gou, *et al.* “Knowledge Distillation: A Survey.” *International Journal of Computer Vision*, vol. 129, no. 6, 2021, pp. 1789–1819.

7. Boyu Hou, *et al.* “Evolutionary Transfer Neural Architecture Search Across Spaces via Representation Learning.” *IEEE Transactions on Evolutionary Computation*, 2025.

8. Prateek Yadav, *et al.* “TIES-Merging: Resolving Interference When Merging Models.” *Advances in Neural Information Processing Systems*, 2023.

9. Takuya Akiba, *et al.* “Evolutionary Optimization of Model Merging Recipes.” *Nature Machine Intelligence*, vol. 7, 2025.

10. Sinno Jialin Pan, and Qiang Yang. “A Survey on Transfer Learning.” *IEEE Transactions on Knowledge and Data Engineering*, vol. 22, no. 10, 2010, pp. 1345–1359.

11. Yew-Soon Ong, *et al.* “Evolutionary Transfer Optimization: A New Frontier in Evolutionary Computation.” *IEEE Computational Intelligence Magazine*, vol. 16, no. 1, 2021, pp. 54–62.

12. Boyu Hou, *et al.* “Prompt-Distiller: Few-Shot Knowledge Distillation for Prompt-Based Language Learners with Dual Contrastive Learning.” *Proceedings of ICASSP 2023*, pp. 8702–8706, 2023.

13. Boyu Hou, *et al.* “Evolutionary Transfer Neural Architecture Search Across Spaces via Representation Learning.” *IEEE Trans. on Evolutionary Computation*, DOI: 10.1109/TEVC.2025.XXXXX, 2025.

14. *(待出版)* **Boyu Hou**, *et al.* “Evolutionary Multi-Form Transfer Optimization for Large Language Model Merging.” 2025. (Manuscript under review).
